{"function_name": "DataParallelPRIMERewardModel._forward_micro_batch", "overview": "Computes token-level reward scores for implicit process reward model", "function_signature": "def _forward_micro_batch(self, micro_batch, prompt_length):", "input_parameters": "- `micro_batch`: Micro-batch data {input_ids, attention_mask, position_ids, acc}\n- `prompt_length`: Prompt sequence length", "detailed_description": "First selects computation path based on whether padding removal optimization is enabled. For reward model and reference model, performs forward pass to compute output logits, then extracts log probabilities for corresponding tokens.\nImplicit rewards are obtained by computing log probability differences between the two models on response sequences.\nProcess reward generation supports two modes: direct mode and GAE mode.\nIn token granularity, each position receives corresponding reward value; in whole granularity, all rewards accumulate to the last valid token position.", "output": "- `token_level_scores`: Token-level reward scores (torch.Tensor, shape: [batch_size, seq_len])\n- `q_values`: Raw log probability differences (torch.Tensor, shape: [batch_size, seq_len])", "func_implementation": "", "test_code_path": "code/test_code/test_forward_micro_batch.py", "implementation_location": "code/recipe/prime/prime_dp_rm.py:line 51-228", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `DataParallelPRIMERewardModel._forward_micro_batch`.\n\n**Function Overview:**\nComputes token-level reward scores for implicit process reward model\n\n**Function Signature:**\n```python\ndef _forward_micro_batch(self, micro_batch, prompt_length):\n```\n\n**Input Parameters:**\n- `micro_batch`: Micro-batch data {input_ids, attention_mask, position_ids, acc}\n- `prompt_length`: Prompt sequence length\n\n**Detailed Description:**\nFirst selects computation path based on whether padding removal optimization is enabled. For reward model and reference model, performs forward pass to compute output logits, then extracts log probabilities for corresponding tokens.\nImplicit rewards are obtained by computing log probability differences between the two models on response sequences.\nProcess reward generation supports two modes: direct mode and GAE mode.\nIn token granularity, each position receives corresponding reward value; in whole granularity, all rewards accumulate to the last valid token position.\n\n**Expected Output:**\n- `token_level_scores`: Token-level reward scores (torch.Tensor, shape: [batch_size, seq_len])\n- `q_values`: Raw log probability differences (torch.Tensor, shape: [batch_size, seq_len])\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}]}
{"function_name": "compute_ce_dpo_loss_rm", "overview": "Computes cross-entropy style DPO loss", "function_signature": "def compute_ce_dpo_loss_rm(token_level_scores, acc, response_mask, beta):", "input_parameters": "- `token_level_scores`: Token-level rewards\n- `acc`: Binary accuracy labels\n- `response_mask`: Response mask\n- `beta`: Temperature parameter", "detailed_description": "Computes sequence-level scores, converts them to probabilities, and finally computes cross-entropy loss.", "output": "Scalar loss value", "func_implementation": "", "test_code_path": "code/test_code/test_ce_dpo_loss.py", "implementation_location": "code/recipe/prime/prime_core_algos.py:line 82-85", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_ce_dpo_loss_rm`.\n\n**Function Overview:**\nComputes cross-entropy style DPO loss\n\n**Function Signature:**\n```python\ndef compute_ce_dpo_loss_rm(token_level_scores, acc, response_mask, beta):\n```\n\n**Input Parameters:**\n- `token_level_scores`: Token-level rewards\n- `acc`: Binary accuracy labels\n- `response_mask`: Response mask\n- `beta`: Temperature parameter\n\n**Detailed Description:**\nComputes sequence-level scores, converts them to probabilities, and finally computes cross-entropy loss.\n\n**Expected Output:**\nScalar loss value\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}]}
{"function_name": "compute_detach_dpo_loss_rm", "overview": "Computes detached DPO loss, supporting BoN (Best-of-N) mode", "function_signature": "def compute_detach_dpo_loss_rm(token_level_scores, acc, Q_bc, acc_bc, response_mask, beta, bon_mode=\"none\"):", "input_parameters": "- Base: `token_level_scores`, `acc`, `response_mask`, `beta`\n- Contrast: `Q_bc` (batch Q-value matrix), `acc_bc` (batch accuracy matrix)\n- `bon_mode`: Weight mode {\"none\", \"bon_rm\", \"bon_acc\"}", "detailed_description": "Computes detached DPO loss based on contrastive samples, selecting samples with opposite accuracy labels within batch as contrastive baseline, and supports assigning higher training weights to high-quality samples through BoN weight mechanism.\nBoN weights include three modes: none mode, bon_rm mode, and bon_acc mode.", "output": "Weighted DPO loss value", "func_implementation": "", "test_code_path": "code/test_code/test_detach_dpo_loss.py", "implementation_location": "code/recipe/prime/prime_core_algos.py:line 88-116", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_detach_dpo_loss_rm`.\n\n**Function Overview:**\nComputes detached DPO loss, supporting BoN (Best-of-N) mode\n\n**Function Signature:**\n```python\ndef compute_detach_dpo_loss_rm(token_level_scores, acc, Q_bc, acc_bc, response_mask, beta, bon_mode=\"none\"):\n```\n\n**Input Parameters:**\n- Base: `token_level_scores`, `acc`, `response_mask`, `beta`\n- Contrast: `Q_bc` (batch Q-value matrix), `acc_bc` (batch accuracy matrix)\n- `bon_mode`: Weight mode {\"none\", \"bon_rm\", \"bon_acc\"}\n\n**Detailed Description:**\nComputes detached DPO loss based on contrastive samples, selecting samples with opposite accuracy labels within batch as contrastive baseline, and supports assigning higher training weights to high-quality samples through BoN weight mechanism.\nBoN weights include three modes: none mode, bon_rm mode, and bon_acc mode.\n\n**Expected Output:**\nWeighted DPO loss value\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}]}
{"function_name": "compute_dpo_accuracy", "overview": "Computes pairwise comparison DPO accuracy", "function_signature": "def compute_dpo_accuracy(token_level_scores, acc, response_mask, n_samples):", "input_parameters": "- `token_level_scores`: Token-level reward scores\n- `acc`: Accuracy labels\n- `response_mask`: Response mask\n- `n_samples`: Sample count per group", "detailed_description": "Uses pairwise comparison to evaluate reward model accuracy. Aggregates token-level scores to sequence-level scores, constructs sample pairs for comparison. Judges directional consistency by computing score differences and accuracy differences.\nUses weighted average to compute accuracy, with higher weights for sample pairs with larger accuracy differences. Returns 0.5 when all samples have the same accuracy.", "output": "Weighted pairwise comparison accuracy", "func_implementation": "", "test_code_path": "code/test_code/test_dpo_accuracy.py", "implementation_location": "code/recipe/prime/prime_core_algos.py:line 119-143", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_dpo_accuracy`.\n\n**Function Overview:**\nComputes pairwise comparison DPO accuracy\n\n**Function Signature:**\n```python\ndef compute_dpo_accuracy(token_level_scores, acc, response_mask, n_samples):\n```\n\n**Input Parameters:**\n- `token_level_scores`: Token-level reward scores\n- `acc`: Accuracy labels\n- `response_mask`: Response mask\n- `n_samples`: Sample count per group\n\n**Detailed Description:**\nUses pairwise comparison to evaluate reward model accuracy. Aggregates token-level scores to sequence-level scores, constructs sample pairs for comparison. Judges directional consistency by computing score differences and accuracy differences.\nUses weighted average to compute accuracy, with higher weights for sample pairs with larger accuracy differences. Returns 0.5 when all samples have the same accuracy.\n\n**Expected Output:**\nWeighted pairwise comparison accuracy\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}]}
{"function_name": "compute_dpo_abs_accuracy", "overview": "Computes absolute accuracy metric", "function_signature": "def compute_dpo_abs_accuracy(token_level_scores, acc, response_mask, n_samples):", "input_parameters": "- `token_level_scores`: Token-level reward scores\n- `acc`: Accuracy labels\n- `response_mask`: Response mask", "detailed_description": "Converts aggregated token-level scores to predicted labels, compares with true labels to compute absolute accuracy.", "output": "Absolute classification accuracy", "func_implementation": "", "test_code_path": "code/test_code/test_dpo_abs_accuracy.py", "implementation_location": "code/recipe/prime/prime_core_algos.py:line 146-147", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_dpo_abs_accuracy`.\n\n**Function Overview:**\nComputes absolute accuracy metric\n\n**Function Signature:**\n```python\ndef compute_dpo_abs_accuracy(token_level_scores, acc, response_mask, n_samples):\n```\n\n**Input Parameters:**\n- `token_level_scores`: Token-level reward scores\n- `acc`: Accuracy labels\n- `response_mask`: Response mask\n\n**Detailed Description:**\nConverts aggregated token-level scores to predicted labels, compares with true labels to compute absolute accuracy.\n\n**Expected Output:**\nAbsolute classification accuracy\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}]}
{"function_name": "compute_rloo_advantage_return", "overview": "Implements RLOO advantage estimation, supporting multiple reward source fusion", "function_signature": "def compute_rloo_advantage_return(data: verl.DataProto, response_mask: torch.Tensor, n_samples, config):", "input_parameters": "- `data`: Data protocol object containing DataProto.batch fields\n- `data.batch[\"rm_scores\"]`: Implicit reward scores\n- `data.batch[\"acc\"]`: Binary accuracy labels\n- `data.batch[\"prompts\"]`: Input prompt sequences\n- `data.batch[\"attention_mask\"]`: Attention mask\n- `response_mask`: Response mask indicating valid token positions\n- `n_samples`: Sample count per group, used for RLOO grouping computation\n- `config`: Reward weight configuration, hierarchical configuration object\n- `config.algorithm.reward_dpo_coef`: DPO reward weight coefficient, default 5.0\n- `config.algorithm.reward_gt_coef`: Accuracy reward weight coefficient, default 5.0", "detailed_description": "Function supports advantage computation with multi-source reward fusion. Reward sources include reward model scores and accuracy rewards.\nGroups multiple responses generated from the same prompt by n_samples intervals.\nUses RLOO to compute advantage function for each reward source and fuses multiple reward sources, computes cumulative return values after reward fusion and applies whitening.", "output": "- `advantages`: Whitened advantage function (torch.Tensor, shape: [batch_size, seq_len])\n- `returns`: Cumulative return values (torch.Tensor, shape: [batch_size, seq_len])", "func_implementation": "", "test_code_path": "code/test_code/test_rloo_advantage_return.py", "implementation_location": "code/recipe/prime/prime_core_algos.py:line 21-79", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_rloo_advantage_return`.\n\n**Function Overview:**\nImplements RLOO advantage estimation, supporting multiple reward source fusion\n\n**Function Signature:**\n```python\ndef compute_rloo_advantage_return(data: verl.DataProto, response_mask: torch.Tensor, n_samples, config):\n```\n\n**Input Parameters:**\n- `data`: Data protocol object containing DataProto.batch fields\n- `data.batch[\"rm_scores\"]`: Implicit reward scores\n- `data.batch[\"acc\"]`: Binary accuracy labels\n- `data.batch[\"prompts\"]`: Input prompt sequences\n- `data.batch[\"attention_mask\"]`: Attention mask\n- `response_mask`: Response mask indicating valid token positions\n- `n_samples`: Sample count per group, used for RLOO grouping computation\n- `config`: Reward weight configuration, hierarchical configuration object\n- `config.algorithm.reward_dpo_coef`: DPO reward weight coefficient, default 5.0\n- `config.algorithm.reward_gt_coef`: Accuracy reward weight coefficient, default 5.0\n\n**Detailed Description:**\nFunction supports advantage computation with multi-source reward fusion. Reward sources include reward model scores and accuracy rewards.\nGroups multiple responses generated from the same prompt by n_samples intervals.\nUses RLOO to compute advantage function for each reward source and fuses multiple reward sources, computes cumulative return values after reward fusion and applies whitening.\n\n**Expected Output:**\n- `advantages`: Whitened advantage function (torch.Tensor, shape: [batch_size, seq_len])\n- `returns`: Cumulative return values (torch.Tensor, shape: [batch_size, seq_len])\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}]}
{"function_name": "RayPRIMETrainer.filter_and_downsample", "overview": "Implements intelligent sample filtering and priority downsampling for PRIME algorithm", "function_signature": "def filter_and_downsample(self, scores, batch: DataProto):", "input_parameters": "- `scores`: Verification score list, binary accuracy labels\n- `batch`: Data batch object\n- `config`: Filtering parameter configuration", "detailed_description": "Filters high-quality samples from oversampled candidate responses. Reconstructs verification scores into 2D matrix, performs dual filtering: accuracy filtering and length filtering.\nAccuracy filtering retains sample groups with average accuracy within configured range; length filtering removes sample groups exceeding maximum length threshold.\nSample groups meeting filtering conditions are retained with priority, ultimately retaining 1/oversample_factor proportion of samples. If all groups fail to meet filtering conditions, algorithm still retains corresponding proportion of samples to ensure training continues.", "output": "Filtered high-quality data batch", "func_implementation": "", "test_code_path": "code/test_code/test_filter_and_downsample.py", "implementation_location": "code/recipe/prime/prime_ray_trainer.py:line 540-572", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `RayPRIMETrainer.filter_and_downsample`.\n\n**Function Overview:**\nImplements intelligent sample filtering and priority downsampling for PRIME algorithm\n\n**Function Signature:**\n```python\ndef filter_and_downsample(self, scores, batch: DataProto):\n```\n\n**Input Parameters:**\n- `scores`: Verification score list, binary accuracy labels\n- `batch`: Data batch object\n- `config`: Filtering parameter configuration\n\n**Detailed Description:**\nFilters high-quality samples from oversampled candidate responses. Reconstructs verification scores into 2D matrix, performs dual filtering: accuracy filtering and length filtering.\nAccuracy filtering retains sample groups with average accuracy within configured range; length filtering removes sample groups exceeding maximum length threshold.\nSample groups meeting filtering conditions are retained with priority, ultimately retaining 1/oversample_factor proportion of samples. If all groups fail to meet filtering conditions, algorithm still retains corresponding proportion of samples to ensure training continues.\n\n**Expected Output:**\nFiltered high-quality data batch\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}]}
