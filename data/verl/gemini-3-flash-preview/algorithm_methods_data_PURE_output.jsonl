{"function_name": "ProcessRewardModelWorker._forward_micro_batch", "overview": "Computes token-level scores from the Process Reward Model (PRM) and performs approximate weighting for min-form credit assignment based on configuration.", "function_signature": "def _forward_micro_batch(self, micro_batch):", "input_parameters": "- `micro_batch`: Micro-batch data containing `input_ids`, `attention_mask`, `position_ids`, `responses`, `reward_mask`, etc.\n- `response_length`: Response sequence length (inferred from batch tensors)\n- `temperature`: Temperature for approximate min-form (when `reward_model.credit_assignment` is a float)", "detailed_description": "Performs forward pass on the Process Reward Model (PRM), computing token-level scores via softmax to obtain the positive-negative class probability difference. Uses masking to retain only valid scores from the response portion.\nIf depadding or sequence parallelism optimization is enabled, padding must be removed before computation and restored afterward.\nCredit assignment weighting executes different strategies based on configuration mode: min-form, strict min-form, or gamma-decay. Lower-scoring tokens should receive higher weights.", "output": "- `rm_score`: Token-level process reward scores (shape `[batch_size, response_length]`)", "func_implementation": "", "test_code_path": "code/test_code/test_forward_micro_batch.py", "implementation_location": "code/verl/workers/fsdp_workers.py:line 1462-1539", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `ProcessRewardModelWorker._forward_micro_batch`.\n\n**Function Overview:**\nComputes token-level scores from the Process Reward Model (PRM) and performs approximate weighting for min-form credit assignment based on configuration.\n\n**Function Signature:**\n```python\ndef _forward_micro_batch(self, micro_batch):\n```\n\n**Input Parameters:**\n- `micro_batch`: Micro-batch data containing `input_ids`, `attention_mask`, `position_ids`, `responses`, `reward_mask`, etc.\n- `response_length`: Response sequence length (inferred from batch tensors)\n- `temperature`: Temperature for approximate min-form (when `reward_model.credit_assignment` is a float)\n\n**Detailed Description:**\nPerforms forward pass on the Process Reward Model (PRM), computing token-level scores via softmax to obtain the positive-negative class probability difference. Uses masking to retain only valid scores from the response portion.\nIf depadding or sequence parallelism optimization is enabled, padding must be removed before computation and restored afterward.\nCredit assignment weighting executes different strategies based on configuration mode: min-form, strict min-form, or gamma-decay. Lower-scoring tokens should receive higher weights.\n\n**Expected Output:**\n- `rm_score`: Token-level process reward scores (shape `[batch_size, response_length]`)\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport torch\nimport torch.nn.functional as F\nfrom verl.utils.torch_utils import get_all_reduce_mean\n\ndef _forward_micro_batch(self, micro_batch):\n    \"\"\"\n    Computes token-level scores from the Process Reward Model (PRM) and performs \n    approximate weighting for credit assignment.\n    \"\"\"\n    # 1. Prepare inputs and extract metadata\n    input_ids = micro_batch['input_ids']\n    attention_mask = micro_batch['attention_mask']\n    position_ids = micro_batch['position_ids']\n    \n    # reward_mask identifies which tokens in the sequence are part of the response/steps to be scored\n    reward_mask = micro_batch['reward_mask'] \n    \n    # In verl, the response is usually the suffix of the input_ids\n    # We calculate response_length to slice the output scores later\n    seq_len = input_ids.shape[1]\n    response_length = micro_batch['responses'].shape[1]\n    \n    # 2. Forward pass through the PRM\n    # Handle potential depadding/sequence parallelism if enabled in the model config\n    if self.config.model.get('use_remove_padding', False):\n        # Implementation detail: verl often uses flash_attn style packing\n        # This is a simplified representation of the logic\n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            use_cache=False\n        )\n    else:\n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            use_cache=False\n        )\n\n    # 3. Compute token-level scores\n    # PRM typically outputs logits for [Negative, Positive] classes at each token\n    logits = outputs.logits  # Shape: [batch_size, seq_len, 2]\n    \n    # Apply softmax to get probabilities and take the difference or the positive class prob\n    # Standard PRM approach: score = P(positive) or P(positive) - P(negative)\n    probs = F.softmax(logits, dim=-1)\n    token_scores = probs[:, :, 1] # Probability of the 'positive' class\n    \n    # 4. Slice to keep only the response part and apply reward_mask\n    # Assuming response is at the end of the sequence\n    rm_score = token_scores[:, -response_length:]\n    rm_mask = reward_mask[:, -response_length:]\n    \n    # Zero out scores for padding/non-reward tokens\n    rm_score = rm_score * rm_mask\n\n    # 5. Credit Assignment Weighting\n    # Based on configuration: min-form, strict-min, or gamma-decay\n    ca_strategy = self.config.reward_model.get('credit_assignment', None)\n    \n    if ca_strategy is not None:\n        if isinstance(ca_strategy, float):\n            # Approximate min-form weighting using Softmin-like approach\n            # Lower scores (potential bottlenecks) get higher weights\n            temperature = self.config.reward_model.get('temperature', 1.0)\n            \n            # Compute weights: exp(-score/temp) / sum(exp(-score/temp))\n            # We mask the scores to avoid influence from padding\n            masked_scores = rm_score.masked_fill(~rm_mask.bool(), float('inf'))\n            weights = F.softmin(masked_scores / temperature, dim=-1)\n            \n            # Apply weighting to the scores\n            rm_score = rm_score * weights * response_length\n            \n        elif ca_strategy == 'strict_min':\n            # Only the minimum score in the sequence gets the credit\n            masked_scores = rm_score.masked_fill(~rm_mask.bool(), float('inf'))\n            min_vals, _ = torch.min(masked_scores, dim=-1, keepdim=True)\n            rm_score = torch.where(rm_score == min_vals, rm_score, torch.zeros_like(rm_score))\n            \n        elif ca_strategy == 'gamma_decay':\n            # Apply a temporal decay factor (gamma) to the scores\n            gamma = self.config.reward_model.get('gamma', 0.99)\n            indices = torch.arange(response_length, device=rm_score.device).unsqueeze(0)\n            decay = torch.pow(gamma, (response_length - 1 - indices))\n            rm_score = rm_score * decay\n\n    return rm_score\n```"], "usage": {"prompt_tokens": 480, "completion_tokens": 1074, "total_tokens": 1554, "per_completion": [{"prompt_tokens": 480, "completion_tokens": 1074, "total_tokens": 1554}]}}
{"function_name": "ProcessRewardModelWorker.compute_rm_score", "overview": "Batch-constructs PRM inputs (splitting by steps and appending newlines after each step), performs inference with (optional) variable-length sharding/backfilling, and aggregates to obtain batch-level token-level `rm_scores`.", "function_signature": "def compute_rm_score(self, data:DataProto):", "input_parameters": "- `data`: `DataProto` containing `responses`, `reward_mask`, etc.;", "detailed_description": "Splits response sequences by steps to construct input format for the process reward model.\nIn dynamic batch mode, reorganizes data into multiple mini-batches based on total token count limit and records original order; in fixed batch mode, evenly splits by sample count.\nCalls forward function for each mini-batch to compute token-level scores, concatenating results from all batches. In dynamic batch mode, restores results to original order based on recorded indices.", "output": "- `DataProto`: `batch['rm_scores']` with shape `[batch_size, response_length]`", "func_implementation": "", "test_code_path": "code/test_code/test_compute_rm_score.py", "implementation_location": "code/verl/workers/fsdp_workers.py:line 1541-1594", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `ProcessRewardModelWorker.compute_rm_score`.\n\n**Function Overview:**\nBatch-constructs PRM inputs (splitting by steps and appending newlines after each step), performs inference with (optional) variable-length sharding/backfilling, and aggregates to obtain batch-level token-level `rm_scores`.\n\n**Function Signature:**\n```python\ndef compute_rm_score(self, data:DataProto):\n```\n\n**Input Parameters:**\n- `data`: `DataProto` containing `responses`, `reward_mask`, etc.;\n\n**Detailed Description:**\nSplits response sequences by steps to construct input format for the process reward model.\nIn dynamic batch mode, reorganizes data into multiple mini-batches based on total token count limit and records original order; in fixed batch mode, evenly splits by sample count.\nCalls forward function for each mini-batch to compute token-level scores, concatenating results from all batches. In dynamic batch mode, restores results to original order based on recorded indices.\n\n**Expected Output:**\n- `DataProto`: `batch['rm_scores']` with shape `[batch_size, response_length]`\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport torch\nfrom verl import DataProto\n\ndef compute_rm_score(self, data: DataProto):\n    \"\"\"\n    Batch-constructs PRM inputs, performs inference with optional dynamic sharding,\n    and aggregates results to obtain token-level rm_scores.\n    \n    Args:\n        data (DataProto): Contains 'responses' and 'reward_mask'.\n        \n    Returns:\n        DataProto: Updated data with 'rm_scores' in the batch.\n    \"\"\"\n    responses = data.batch['responses'] # Shape: [batch_size, seq_len]\n    reward_mask = data.batch['reward_mask'] # Shape: [batch_size, seq_len]\n    \n    batch_size, seq_len = responses.shape\n    device = responses.device\n\n    # 1. Construct PRM inputs by splitting steps\n    # Note: PRMs often expect specific formatting (e.g., appending newlines or step separators)\n    # Here we assume the model's tokenizer/processor handles the specific formatting \n    # based on the raw response tokens.\n    input_ids = responses\n    attention_mask = data.batch.get('attention_mask', torch.ones_like(responses))\n\n    # 2. Handle Batching Logic (Dynamic vs Fixed)\n    # We determine the indices for processing to handle potential sharding/reordering\n    indices = list(range(batch_size))\n    \n    # Calculate total tokens per sample for dynamic batching\n    seq_lengths = attention_mask.sum(dim=-1).cpu().tolist()\n    \n    all_rm_scores = []\n    \n    # Determine mini-batches\n    if self.config.get('dynamic_batch_mode', False):\n        # Sort by length to optimize padding in dynamic batching\n        sorted_indices = sorted(indices, key=lambda i: seq_lengths[i], reverse=True)\n        max_tokens = self.config.get('max_tokens_per_batch', 4096)\n        \n        mini_batches = []\n        current_batch = []\n        current_tokens = 0\n        \n        for idx in sorted_indices:\n            if current_tokens + seq_lengths[idx] > max_tokens and current_batch:\n                mini_batches.append(current_batch)\n                current_batch = []\n                current_tokens = 0\n            current_batch.append(idx)\n            current_tokens += seq_lengths[idx]\n        if current_batch:\n            mini_batches.append(current_batch)\n    else:\n        # Fixed batch size mode\n        micro_batch_size = self.config.get('micro_batch_size', batch_size)\n        mini_batches = [indices[i:i + micro_batch_size] for i in range(0, batch_size, micro_batch_size)]\n\n    # 3. Inference Loop\n    # Store results in a dictionary to map back to original order\n    results_map = {}\n\n    for m_batch_indices in mini_batches:\n        m_input_ids = input_ids[m_batch_indices]\n        m_attention_mask = attention_mask[m_batch_indices]\n        \n        # Forward pass to get token-level rewards\n        # The model is expected to return scores of shape [m_batch_size, seq_len]\n        with torch.no_grad():\n            # self.model is the underlying PRM\n            m_scores = self.model(input_ids=m_input_ids, attention_mask=m_attention_mask)\n            \n        # Store results\n        for i, original_idx in enumerate(m_batch_indices):\n            results_map[original_idx] = m_scores[i]\n\n    # 4. Aggregate and Restore Order\n    # Reconstruct the full rm_scores tensor in the original batch order\n    rm_scores = torch.stack([results_map[i] for i in range(batch_size)], dim=0)\n\n    # 5. Apply reward mask\n    # Ensure scores are only provided where the reward_mask is active (e.g., at step ends)\n    # Depending on the PRM, it might already be sparse; here we align it with the data's mask\n    rm_scores = rm_scores * reward_mask.to(rm_scores.dtype)\n\n    # Update DataProto\n    data.batch['rm_scores'] = rm_scores\n    \n    return data\n```"], "usage": {"prompt_tokens": 434, "completion_tokens": 1018, "total_tokens": 1452, "per_completion": [{"prompt_tokens": 434, "completion_tokens": 1018, "total_tokens": 1452}]}}
{"function_name": "compute_return", "overview": "Computes token-level return sequences based on `method`", "function_signature": "def compute_return(token_level_rewards, eos_mask, method='sum', gamma=1.0):", "input_parameters": "- `token_level_rewards`: `[batch_size, response_length]`\n- `eos_mask`: `[batch_size, response_length]`\n- `method`: `'sum'|'min'`\n- `gamma`: Discount factor (applicable in `'sum'` mode)", "detailed_description": "Computes token-level return sequences based on method, using masking to ensure only valid positions are processed.\n\"method\" includes \"sum\" and \"min\".", "output": "- `returns`: `[batch_size, response_length]`", "func_implementation": "", "test_code_path": "code/test_code/test_compute_return.py", "implementation_location": "code/verl/trainer/ppo/core_algos.py:line 70-88", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_return`.\n\n**Function Overview:**\nComputes token-level return sequences based on `method`\n\n**Function Signature:**\n```python\ndef compute_return(token_level_rewards, eos_mask, method='sum', gamma=1.0):\n```\n\n**Input Parameters:**\n- `token_level_rewards`: `[batch_size, response_length]`\n- `eos_mask`: `[batch_size, response_length]`\n- `method`: `'sum'|'min'`\n- `gamma`: Discount factor (applicable in `'sum'` mode)\n\n**Detailed Description:**\nComputes token-level return sequences based on method, using masking to ensure only valid positions are processed.\n\"method\" includes \"sum\" and \"min\".\n\n**Expected Output:**\n- `returns`: `[batch_size, response_length]`\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport torch\n\ndef compute_return(token_level_rewards, eos_mask, method='sum', gamma=1.0):\n    \"\"\"\n    Computes token-level return sequences based on the specified method.\n\n    Args:\n        token_level_rewards (torch.Tensor): [batch_size, response_length]\n        eos_mask (torch.Tensor): [batch_size, response_length], 1 for valid tokens, 0 for padding.\n        method (str): 'sum' for discounted cumulative return, 'min' for minimum future reward.\n        gamma (float): Discount factor for the 'sum' method.\n\n    Returns:\n        torch.Tensor: [batch_size, response_length]\n    \"\"\"\n    batch_size, seq_len = token_level_rewards.shape\n    returns = torch.zeros_like(token_level_rewards)\n\n    if method == 'sum':\n        # Compute discounted cumulative return from right to left\n        # G_t = r_t + gamma * G_{t+1}\n        running_return = torch.zeros(batch_size, device=token_level_rewards.device)\n        for t in reversed(range(seq_len)):\n            # Update running return only for valid tokens (eos_mask == 1)\n            # If mask is 0, the reward is 0 and the previous running_return is ignored\n            running_return = token_level_rewards[:, t] + gamma * running_return * eos_mask[:, t]\n            returns[:, t] = running_return\n            \n    elif method == 'min':\n        # Compute the minimum future reward from right to left\n        # Often used in specific RL formulations to penalize worst-case outcomes\n        running_min = torch.full((batch_size,), float('inf'), device=token_level_rewards.device)\n        for t in reversed(range(seq_len)):\n            # We only consider rewards where eos_mask is 1\n            # For masked positions, we don't update the running_min in a way that affects valid tokens\n            current_reward = token_level_rewards[:, t]\n            \n            # If current token is valid, update min. If not, we can reset or ignore.\n            # Logic: min(current_reward, running_min) if mask is 1\n            running_min = torch.where(\n                eos_mask[:, t] == 1,\n                torch.min(current_reward, running_min),\n                torch.full_like(running_min, float('inf')) # Reset for padding\n            )\n            returns[:, t] = running_min\n            \n        # Replace inf with 0 for clean output in masked areas\n        returns = torch.where(returns == float('inf'), torch.zeros_like(returns), returns)\n\n    else:\n        raise ValueError(f\"Unknown method: {method}. Expected 'sum' or 'min'.\")\n\n    # Ensure returns are zeroed out where eos_mask is 0\n    returns = returns * eos_mask\n    \n    return returns\n```"], "usage": {"prompt_tokens": 376, "completion_tokens": 689, "total_tokens": 1065, "per_completion": [{"prompt_tokens": 376, "completion_tokens": 689, "total_tokens": 1065}]}}
