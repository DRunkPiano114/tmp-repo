{"function_name": "compute_grpo_outcome_advantage_split", "overview": "Computes group GRPO (Group Relative Policy Optimization) advantage estimation, with the key innovation of using only on-policy samples to compute within-group baseline.", "function_signature": "def compute_grpo_outcome_advantage_split(token_level_rewards: torch.Tensor,\neos_mask: torch.Tensor,\nindex: torch.Tensor,\non_policy_mask: torch.Tensor,\nepsilon: float = 1e-6,\nuse_std: bool = True):", "input_parameters": "- `token_level_rewards`: Token-level reward tensor with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `eos_mask`: Valid token mask with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `index`: Sample grouping indices, n responses from the same prompt share the same index. **Note: This parameter should be a Python list or indexable sequence (e.g., `[0, 0, 1, 1]`), not `torch.Tensor`, as index values will be used as dictionary keys**\n- `on_policy_mask`: Boolean mask with shape `[batch_size]`, type `torch.Tensor`, marking which samples are on-policy generated\n- `epsilon`: Numerical stability constant, default 1e-6\n- `use_std`: Whether to use standard deviation for normalization, default True", "detailed_description": "This function needs to sum token-level reward tensors along the time dimension for each response sequence to obtain sequence-level total scores.\nThe main implementation performs grouped normalization (including both on-policy and off-policy) for multiple responses from the same prompt, computing relative advantage values. During normalization, only samples marked as on-policy are selected to compute the group's mean and standard deviation. If a group has only one on-policy sample, set mean to 0 and standard deviation to 1; if there are multiple samples, compute statistics normally. For cases where standard deviation is 0, replace it with 1 to avoid division by zero errors.\nIf standard deviation normalization is enabled, compute (score - mean) / (std + epsilon); otherwise only subtract the mean. Finally, expand sequence-level advantage values to token-level advantage tensors.", "output": "- `advantages`: Token-level advantage tensor with shape `[batch_size, response_length]`\n- `returns`: Token-level return tensor with shape `[batch_size, response_length]`, same as advantages", "func_implementation": "", "test_code_path": "code/test_code/test_compute_grpo_outcome_advantage_split.py", "implementation_location": "code/luffy/verl/verl/mix_src/mix_core_alg.py:line 12-67", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_grpo_outcome_advantage_split`.\n\n**Function Overview:**\nComputes group GRPO (Group Relative Policy Optimization) advantage estimation, with the key innovation of using only on-policy samples to compute within-group baseline.\n\n**Function Signature:**\n```python\ndef compute_grpo_outcome_advantage_split(token_level_rewards: torch.Tensor,\neos_mask: torch.Tensor,\nindex: torch.Tensor,\non_policy_mask: torch.Tensor,\nepsilon: float = 1e-6,\nuse_std: bool = True):\n```\n\n**Input Parameters:**\n- `token_level_rewards`: Token-level reward tensor with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `eos_mask`: Valid token mask with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `index`: Sample grouping indices, n responses from the same prompt share the same index. **Note: This parameter should be a Python list or indexable sequence (e.g., `[0, 0, 1, 1]`), not `torch.Tensor`, as index values will be used as dictionary keys**\n- `on_policy_mask`: Boolean mask with shape `[batch_size]`, type `torch.Tensor`, marking which samples are on-policy generated\n- `epsilon`: Numerical stability constant, default 1e-6\n- `use_std`: Whether to use standard deviation for normalization, default True\n\n**Detailed Description:**\nThis function needs to sum token-level reward tensors along the time dimension for each response sequence to obtain sequence-level total scores.\nThe main implementation performs grouped normalization (including both on-policy and off-policy) for multiple responses from the same prompt, computing relative advantage values. During normalization, only samples marked as on-policy are selected to compute the group's mean and standard deviation. If a group has only one on-policy sample, set mean to 0 and standard deviation to 1; if there are multiple samples, compute statistics normally. For cases where standard deviation is 0, replace it with 1 to avoid division by zero errors.\nIf standard deviation normalization is enabled, compute (score - mean) / (std + epsilon); otherwise only subtract the mean. Finally, expand sequence-level advantage values to token-level advantage tensors.\n\n**Expected Output:**\n- `advantages`: Token-level advantage tensor with shape `[batch_size, response_length]`\n- `returns`: Token-level return tensor with shape `[batch_size, response_length]`, same as advantages\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport torch\nfrom typing import List, Union\n\ndef compute_grpo_outcome_advantage_split(token_level_rewards: torch.Tensor,\n                                        eos_mask: torch.Tensor,\n                                        index: Union[List, torch.Tensor],\n                                        on_policy_mask: torch.Tensor,\n                                        epsilon: float = 1e-6,\n                                        use_std: bool = True):\n    \"\"\"\n    Computes GRPO advantage estimation using only on-policy samples for group statistics.\n    \n    Args:\n        token_level_rewards: [batch_size, response_length]\n        eos_mask: [batch_size, response_length]\n        index: List or sequence of length batch_size indicating group membership\n        on_policy_mask: [batch_size] boolean tensor\n        epsilon: Numerical stability constant\n        use_std: Whether to divide by standard deviation\n        \n    Returns:\n        advantages: [batch_size, response_length]\n        returns: [batch_size, response_length]\n    \"\"\"\n    # 1. Compute sequence-level total rewards (scores)\n    # Sum rewards along the time dimension for each response\n    sequence_scores = (token_level_rewards * eos_mask).sum(dim=-1) # [batch_size]\n    \n    batch_size = sequence_scores.size(0)\n    advantages = torch.zeros_like(sequence_scores)\n    \n    # 2. Group samples by index\n    # We use a dictionary to map group indices to the list of sample indices belonging to that group\n    group_to_indices = {}\n    for i, group_idx in enumerate(index):\n        if group_idx not in group_to_indices:\n            group_to_indices[group_idx] = []\n        group_to_indices[group_idx].append(i)\n        \n    # 3. Compute statistics per group using only on-policy samples\n    for group_idx, indices in group_to_indices.items():\n        indices_tensor = torch.tensor(indices, device=sequence_scores.device)\n        \n        # Filter indices that are on-policy within this group\n        group_on_policy_mask = on_policy_mask[indices_tensor]\n        on_policy_indices = indices_tensor[group_on_policy_mask]\n        \n        # Get scores for all samples in the group and specifically on-policy samples\n        group_scores = sequence_scores[indices_tensor]\n        on_policy_scores = sequence_scores[on_policy_indices]\n        \n        # Compute mean and std based on on-policy samples\n        if on_policy_scores.numel() <= 1:\n            # If 0 or 1 on-policy sample, use default baseline\n            mean = 0.0 if on_policy_scores.numel() == 0 else on_policy_scores.item()\n            std = 1.0\n        else:\n            mean = on_policy_scores.mean()\n            std = on_policy_scores.std()\n            \n        # Handle zero variance case\n        if std == 0:\n            std = 1.0\n            \n        # 4. Compute relative advantage for all samples in the group\n        if use_std:\n            group_advantages = (group_scores - mean) / (std + epsilon)\n        else:\n            group_advantages = group_scores - mean\n            \n        # Assign computed advantages back to the main tensor\n        advantages[indices_tensor] = group_advantages\n\n    # 5. Expand sequence-level advantages to token-level\n    # [batch_size] -> [batch_size, 1] -> [batch_size, response_length]\n    advantages = advantages.unsqueeze(1) * eos_mask\n    \n    # In GRPO outcome-based reward, returns are typically equivalent to the computed advantages\n    returns = advantages.clone()\n    \n    return advantages, returns\n```"], "usage": {"prompt_tokens": 754, "completion_tokens": 902, "total_tokens": 1656, "per_completion": [{"prompt_tokens": 754, "completion_tokens": 902, "total_tokens": 1656}]}, "results": [false], "pass_ratios": [0.8333333333333334]}
{"function_name": "compute_token_on_off_policy_loss", "overview": "Computes mixed policy loss, combining on-policy PPO clip loss and off-policy importance sampling loss, supporting multiple policy shaping methods to emphasize low-probability but critical actions.", "function_signature": "def compute_token_on_off_policy_loss(\nold_log_prob,\nlog_prob,\nadvantages,\neos_mask,\ncliprange,\nclip_upper_bound,\nprefix_mask,\noff_cliprange,\noff_normalize=False,\noff_abs_cliprange=None,\noff_max_clip=None,\noff_min_clip=None,\nall_max_clip=None,\noff_policy_reshape=\"no_reshape\",\noff_policy_reshape_weight=1.0,\noff_policy_reshape_pow_exp=0.5,\non_policy_reshape=\"no_reshape\",\non_policy_reshape_weight=1.0,\non_policy_reshape_pow_exp=0.5,\ntarget_probs=None,\nloss_remove_token_mean=False,\nloss_remove_clip=False,\n):", "input_parameters": "- `old_log_prob`: Old policy log probabilities with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `log_prob`: Current policy log probabilities with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `advantages`: Advantage estimates with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `eos_mask`: Valid token mask with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `cliprange`: PPO clip range, default 0.2\n- `clip_upper_bound`: PPO clip upper bound, default 100.0\n- `prefix_mask`: Mask marking off-policy tokens with shape `[batch_size, response_length]`, **type must be `torch.Tensor` with dtype `torch.bool`, as the code uses bitwise NOT operator `~prefix_mask` to obtain on-policy token mask**\n- `off_cliprange`: off-policy clip range (not used in current implementation)\n- `off_normalize`: Whether to normalize off-policy ratio, default False\n- `off_abs_cliprange`: off-policy absolute value clip range (not used in current implementation)\n- `off_max_clip`: off-policy ratio upper limit, None means no limit\n- `off_min_clip`: off-policy ratio lower limit, None means no limit\n- `all_max_clip`: Global probability upper limit, None means no limit\n- `off_policy_reshape`: off-policy policy shaping method name, default 'no_reshape'\n- `off_policy_reshape_weight`: Policy shaping weight parameter, default 1.0\n- `off_policy_reshape_pow_exp`: Power transformation exponent, default 0.5\n- `on_policy_reshape`: on-policy policy shaping method name, default 'no_reshape'\n- `on_policy_reshape_weight`: on-policy policy shaping weight, default 1.0\n- `on_policy_reshape_pow_exp`: on-policy power transformation exponent, default 0.5\n- `target_probs`: Optional target probabilities (for explicit behavior policy), default None\n- `loss_remove_token_mean`: Whether to remove token-level averaging, default False\n- `loss_remove_clip`: Whether to remove clip operation, default False", "detailed_description": "This function computes mixed policy gradient loss, with the core idea of simultaneously handling both on-policy and off-policy data. For on-policy data, it uses standard PPO algorithm to compute importance sampling ratio and perform clipping; for off-policy data, it directly uses current policy probabilities (or ratio relative to target probabilities) to compute loss, without relying on old policy.\nThe function distinguishes which tokens are off-policy and which are on-policy through prefix_mask, then combines the two losses with weighting. Additionally, the function should support multiple probability reshaping strategies and clipping mechanisms.", "output": "Returns dictionary containing the following key-value pairs:\n- `pg_loss`: Total policy gradient loss (scalar)\n- `off_pg_loss`: off-policy loss component (scalar)\n- `on_pg_loss`: on-policy loss component (scalar)\n- `off_pg_clipfrac`: off-policy clip fraction (scalar, currently 0 in implementation)\n- `on_pg_clipfrac`: on-policy clip fraction (scalar)\n- `ppo_kl`: PPO KL divergence (scalar)\n- `off_policy_prob`: Average probability of off-policy samples (scalar)\n- `on_policy_prob`: Average probability of on-policy samples (scalar)\n- `off_ratio_mean`: off-policy average ratio (scalar)\n- `off_ratio_max_clip_frac`: Fraction reaching upper clip limit (scalar)\n- `off_ratio_min_clip_frac`: Fraction reaching lower clip limit (scalar)", "func_implementation": "", "test_code_path": "code/test_code/test_compute_token_on_off_policy_loss.py", "implementation_location": "code/luffy/verl/verl/mix_src/mix_core_alg.py:line 69-251", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_token_on_off_policy_loss`.\n\n**Function Overview:**\nComputes mixed policy loss, combining on-policy PPO clip loss and off-policy importance sampling loss, supporting multiple policy shaping methods to emphasize low-probability but critical actions.\n\n**Function Signature:**\n```python\ndef compute_token_on_off_policy_loss(\nold_log_prob,\nlog_prob,\nadvantages,\neos_mask,\ncliprange,\nclip_upper_bound,\nprefix_mask,\noff_cliprange,\noff_normalize=False,\noff_abs_cliprange=None,\noff_max_clip=None,\noff_min_clip=None,\nall_max_clip=None,\noff_policy_reshape=\"no_reshape\",\noff_policy_reshape_weight=1.0,\noff_policy_reshape_pow_exp=0.5,\non_policy_reshape=\"no_reshape\",\non_policy_reshape_weight=1.0,\non_policy_reshape_pow_exp=0.5,\ntarget_probs=None,\nloss_remove_token_mean=False,\nloss_remove_clip=False,\n):\n```\n\n**Input Parameters:**\n- `old_log_prob`: Old policy log probabilities with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `log_prob`: Current policy log probabilities with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `advantages`: Advantage estimates with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `eos_mask`: Valid token mask with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `cliprange`: PPO clip range, default 0.2\n- `clip_upper_bound`: PPO clip upper bound, default 100.0\n- `prefix_mask`: Mask marking off-policy tokens with shape `[batch_size, response_length]`, **type must be `torch.Tensor` with dtype `torch.bool`, as the code uses bitwise NOT operator `~prefix_mask` to obtain on-policy token mask**\n- `off_cliprange`: off-policy clip range (not used in current implementation)\n- `off_normalize`: Whether to normalize off-policy ratio, default False\n- `off_abs_cliprange`: off-policy absolute value clip range (not used in current implementation)\n- `off_max_clip`: off-policy ratio upper limit, None means no limit\n- `off_min_clip`: off-policy ratio lower limit, None means no limit\n- `all_max_clip`: Global probability upper limit, None means no limit\n- `off_policy_reshape`: off-policy policy shaping method name, default 'no_reshape'\n- `off_policy_reshape_weight`: Policy shaping weight parameter, default 1.0\n- `off_policy_reshape_pow_exp`: Power transformation exponent, default 0.5\n- `on_policy_reshape`: on-policy policy shaping method name, default 'no_reshape'\n- `on_policy_reshape_weight`: on-policy policy shaping weight, default 1.0\n- `on_policy_reshape_pow_exp`: on-policy power transformation exponent, default 0.5\n- `target_probs`: Optional target probabilities (for explicit behavior policy), default None\n- `loss_remove_token_mean`: Whether to remove token-level averaging, default False\n- `loss_remove_clip`: Whether to remove clip operation, default False\n\n**Detailed Description:**\nThis function computes mixed policy gradient loss, with the core idea of simultaneously handling both on-policy and off-policy data. For on-policy data, it uses standard PPO algorithm to compute importance sampling ratio and perform clipping; for off-policy data, it directly uses current policy probabilities (or ratio relative to target probabilities) to compute loss, without relying on old policy.\nThe function distinguishes which tokens are off-policy and which are on-policy through prefix_mask, then combines the two losses with weighting. Additionally, the function should support multiple probability reshaping strategies and clipping mechanisms.\n\n**Expected Output:**\nReturns dictionary containing the following key-value pairs:\n- `pg_loss`: Total policy gradient loss (scalar)\n- `off_pg_loss`: off-policy loss component (scalar)\n- `on_pg_loss`: on-policy loss component (scalar)\n- `off_pg_clipfrac`: off-policy clip fraction (scalar, currently 0 in implementation)\n- `on_pg_clipfrac`: on-policy clip fraction (scalar)\n- `ppo_kl`: PPO KL divergence (scalar)\n- `off_policy_prob`: Average probability of off-policy samples (scalar)\n- `on_policy_prob`: Average probability of on-policy samples (scalar)\n- `off_ratio_mean`: off-policy average ratio (scalar)\n- `off_ratio_max_clip_frac`: Fraction reaching upper clip limit (scalar)\n- `off_ratio_min_clip_frac`: Fraction reaching lower clip limit (scalar)\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport torch\n\ndef compute_token_on_off_policy_loss(\n    old_log_prob,\n    log_prob,\n    advantages,\n    eos_mask,\n    cliprange,\n    clip_upper_bound,\n    prefix_mask,\n    off_cliprange,\n    off_normalize=False,\n    off_abs_cliprange=None,\n    off_max_clip=None,\n    off_min_clip=None,\n    all_max_clip=None,\n    off_policy_reshape=\"no_reshape\",\n    off_policy_reshape_weight=1.0,\n    off_policy_reshape_pow_exp=0.5,\n    on_policy_reshape=\"no_reshape\",\n    on_policy_reshape_weight=1.0,\n    on_policy_reshape_pow_exp=0.5,\n    target_probs=None,\n    loss_remove_token_mean=False,\n    loss_remove_clip=False,\n):\n    \"\"\"\n    Computes mixed policy loss combining on-policy PPO clip loss and off-policy importance sampling loss.\n    \"\"\"\n    # 1. Calculate basic ratios and probabilities\n    # ratio = pi(a|s) / pi_old(a|s)\n    ratio = torch.exp(log_prob - old_log_prob)\n    curr_prob = torch.exp(log_prob)\n    \n    # 2. Handle On-Policy Loss (PPO Clipping)\n    # On-policy tokens are where prefix_mask is False\n    on_policy_mask = (~prefix_mask) & eos_mask\n    \n    surr1 = ratio * advantages\n    surr2 = torch.clamp(ratio, 1.0 - cliprange, 1.0 + cliprange) * advantages\n    \n    if loss_remove_clip:\n        on_loss_raw = -surr1\n    else:\n        on_loss_raw = -torch.min(surr1, surr2)\n    \n    # Apply on-policy reshaping if needed\n    if on_policy_reshape == \"pow\":\n        on_loss_raw = on_loss_raw * (curr_prob ** on_policy_reshape_pow_exp) * on_policy_reshape_weight\n        \n    on_pg_loss = (on_loss_raw * on_policy_mask).sum()\n    \n    # Calculate on-policy metrics\n    on_token_count = on_policy_mask.sum().clamp(min=1.0)\n    on_pg_clipfrac = (torch.gt(surr1, surr2).float() * on_policy_mask).sum() / on_token_count\n    on_policy_prob = (curr_prob * on_policy_mask).sum() / on_token_count\n\n    # 3. Handle Off-Policy Loss\n    # Off-policy tokens are where prefix_mask is True\n    off_policy_mask = prefix_mask & eos_mask\n    \n    # Calculate off-policy ratio (relative to target_probs if provided, else absolute prob)\n    if target_probs is not None:\n        off_ratio = curr_prob / (target_probs + 1e-8)\n    else:\n        off_ratio = curr_prob\n        \n    # Apply clipping to off-policy ratios\n    off_ratio_max_clip_frac = torch.tensor(0.0, device=log_prob.device)\n    off_ratio_min_clip_frac = torch.tensor(0.0, device=log_prob.device)\n    \n    off_ratio_clipped = off_ratio\n    if off_max_clip is not None:\n        off_ratio_max_clip_frac = (torch.gt(off_ratio, off_max_clip).float() * off_policy_mask).sum() / off_policy_mask.sum().clamp(min=1.0)\n        off_ratio_clipped = torch.clamp(off_ratio_clipped, max=off_max_clip)\n    if off_min_clip is not None:\n        off_ratio_min_clip_frac = (torch.lt(off_ratio, off_min_clip).float() * off_policy_mask).sum() / off_policy_mask.sum().clamp(min=1.0)\n        off_ratio_clipped = torch.clamp(off_ratio_clipped, min=off_min_clip)\n        \n    off_loss_raw = -(off_ratio_clipped * advantages)\n    \n    # Apply off-policy reshaping\n    if off_policy_reshape == \"pow\":\n        off_loss_raw = off_loss_raw * (curr_prob ** off_policy_reshape_pow_exp) * off_policy_reshape_weight\n        \n    off_pg_loss = (off_loss_raw * off_policy_mask).sum()\n    \n    # Calculate off-policy metrics\n    off_token_count = off_policy_mask.sum().clamp(min=1.0)\n    off_policy_prob = (curr_prob * off_policy_mask).sum() / off_token_count\n    off_ratio_mean = (off_ratio * off_policy_mask).sum() / off_token_count\n\n    # 4. Aggregate Losses\n    total_tokens = eos_mask.sum().clamp(min=1.0)\n    \n    if loss_remove_token_mean:\n        # Sum across tokens, mean across batch (handled by caller usually, but here we return sum)\n        pg_loss = on_pg_loss + off_pg_loss\n    else:\n        pg_loss = (on_pg_loss + off_pg_loss) / total_tokens\n        on_pg_loss = on_pg_loss / total_tokens\n        off_pg_loss = off_pg_loss / total_tokens\n\n    # 5. Calculate KL and other global metrics\n    # Approximate KL: 0.5 * (log_prob - old_log_prob)^2\n    log_ratio = log_prob - old_log_prob\n    ppo_kl = (0.5 * (log_ratio**2) * eos_mask).sum() / total_tokens\n\n    return {\n        \"pg_loss\": pg_loss,\n        \"off_pg_loss\": off_pg_loss,\n        \"on_pg_loss\": on_pg_loss,\n        \"off_pg_clipfrac\": torch.tensor(0.0, device=log_prob.device), # Placeholder as per requirement\n        \"on_pg_clipfrac\": on_pg_clipfrac,\n        \"ppo_kl\": ppo_kl,\n        \"off_policy_prob\": off_policy_prob,\n        \"on_policy_prob\": on_policy_prob,\n        \"off_ratio_mean\": off_ratio_mean,\n        \"off_ratio_max_clip_frac\": off_ratio_max_clip_frac,\n        \"off_ratio_min_clip_frac\": off_ratio_min_clip_frac,\n    }\n```"], "usage": {"prompt_tokens": 1338, "completion_tokens": 1636, "total_tokens": 2974, "per_completion": [{"prompt_tokens": 1338, "completion_tokens": 1636, "total_tokens": 2974}]}, "results": [false], "pass_ratios": [0.0]}
{"function_name": "compute_sft_pure_loss", "overview": "Computes pure SFT (Supervised Fine-Tuning) loss, i.e., negative log likelihood loss, used for supervised learning on off-policy samples in multi-task learning.", "function_signature": "", "input_parameters": "- `log_prob`: Model-predicted log probabilities with shape `[batch_size, response_length]`\n- `eos_mask`: Valid token mask with shape `[batch_size, response_length]`", "detailed_description": "Computes negative log probability for each token as loss value. Uses eos_mask to mask the loss, computing loss only at valid token positions. Averages the masked loss to obtain batch-level SFT loss.", "output": "- `sft_loss`: Scalar representing average SFT loss", "func_implementation": "", "test_code_path": "code/test_code/test_compute_sft_pure_loss.py", "implementation_location": "code/luffy/verl/verl/mix_src/mix_core_alg.py:line 7-10", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_sft_pure_loss`.\n\n**Function Overview:**\nComputes pure SFT (Supervised Fine-Tuning) loss, i.e., negative log likelihood loss, used for supervised learning on off-policy samples in multi-task learning.\n\n**Function Signature:**\n```python\n\n```\n\n**Input Parameters:**\n- `log_prob`: Model-predicted log probabilities with shape `[batch_size, response_length]`\n- `eos_mask`: Valid token mask with shape `[batch_size, response_length]`\n\n**Detailed Description:**\nComputes negative log probability for each token as loss value. Uses eos_mask to mask the loss, computing loss only at valid token positions. Averages the masked loss to obtain batch-level SFT loss.\n\n**Expected Output:**\n- `sft_loss`: Scalar representing average SFT loss\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport torch\n\ndef compute_sft_pure_loss(log_prob, eos_mask):\n    \"\"\"\n    Computes pure SFT (Supervised Fine-Tuning) loss, i.e., negative log likelihood loss,\n    used for supervised learning on off-policy samples in multi-task learning.\n\n    Args:\n        log_prob (torch.Tensor): Model-predicted log probabilities with shape [batch_size, response_length].\n                                 These are typically the log probabilities of the ground truth tokens.\n        eos_mask (torch.Tensor): Valid token mask with shape [batch_size, response_length].\n                                 1 for valid tokens (including EOS), 0 for padding.\n\n    Returns:\n        torch.Tensor: Scalar representing the average SFT loss.\n    \"\"\"\n    # The SFT loss is the negative log likelihood: -log(P(x))\n    # log_prob contains log(P(x)) for each token position.\n    loss = -log_prob\n\n    # Apply the eos_mask to ensure we only consider valid tokens (ignore padding)\n    # We multiply the loss by the mask to zero out invalid positions.\n    masked_loss = loss * eos_mask\n\n    # Sum the loss across all valid tokens and divide by the total number of valid tokens\n    # to get the average loss.\n    # We use a small epsilon in the denominator to avoid division by zero if the mask is all zeros.\n    total_loss = torch.sum(masked_loss)\n    total_valid_tokens = torch.sum(eos_mask)\n    \n    sft_loss = total_loss / (total_valid_tokens + 1e-8)\n\n    return sft_loss\n```"], "usage": {"prompt_tokens": 374, "completion_tokens": 384, "total_tokens": 758, "per_completion": [{"prompt_tokens": 374, "completion_tokens": 384, "total_tokens": 758}]}, "results": [false], "pass_ratios": [0.9375]}
