{"function_name": "DataParallelPRIMERewardModel._forward_micro_batch", "overview": "Computes token-level reward scores for implicit process reward model", "function_signature": "def _forward_micro_batch(self, micro_batch, prompt_length):", "input_parameters": "- `micro_batch`: Micro-batch data {input_ids, attention_mask, position_ids, acc}\n- `prompt_length`: Prompt sequence length", "detailed_description": "First selects computation path based on whether padding removal optimization is enabled. For reward model and reference model, performs forward pass to compute output logits, then extracts log probabilities for corresponding tokens.\nImplicit rewards are obtained by computing log probability differences between the two models on response sequences.\nProcess reward generation supports two modes: direct mode and GAE mode.\nIn token granularity, each position receives corresponding reward value; in whole granularity, all rewards accumulate to the last valid token position.", "output": "- `token_level_scores`: Token-level reward scores (torch.Tensor, shape: [batch_size, seq_len])\n- `q_values`: Raw log probability differences (torch.Tensor, shape: [batch_size, seq_len])", "func_implementation": "", "test_code_path": "code/test_code/test_forward_micro_batch.py", "implementation_location": "code/recipe/prime/prime_dp_rm.py:line 51-228", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `DataParallelPRIMERewardModel._forward_micro_batch`.\n\n**Function Overview:**\nComputes token-level reward scores for implicit process reward model\n\n**Function Signature:**\n```python\ndef _forward_micro_batch(self, micro_batch, prompt_length):\n```\n\n**Input Parameters:**\n- `micro_batch`: Micro-batch data {input_ids, attention_mask, position_ids, acc}\n- `prompt_length`: Prompt sequence length\n\n**Detailed Description:**\nFirst selects computation path based on whether padding removal optimization is enabled. For reward model and reference model, performs forward pass to compute output logits, then extracts log probabilities for corresponding tokens.\nImplicit rewards are obtained by computing log probability differences between the two models on response sequences.\nProcess reward generation supports two modes: direct mode and GAE mode.\nIn token granularity, each position receives corresponding reward value; in whole granularity, all rewards accumulate to the last valid token position.\n\n**Expected Output:**\n- `token_level_scores`: Token-level reward scores (torch.Tensor, shape: [batch_size, seq_len])\n- `q_values`: Raw log probability differences (torch.Tensor, shape: [batch_size, seq_len])\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport torch\nimport torch.nn.functional as F\n\ndef _forward_micro_batch(self, micro_batch, prompt_length):\n    \"\"\"\n    Computes token-level reward scores for implicit process reward model (PRIME).\n    \n    Args:\n        micro_batch: Dictionary containing input_ids, attention_mask, position_ids, etc.\n        prompt_length: The length of the prompt sequence.\n        \n    Returns:\n        token_level_scores: Token-level reward scores [batch_size, seq_len]\n        q_values: Raw log probability differences [batch_size, seq_len]\n    \"\"\"\n    input_ids = micro_batch['input_ids']\n    attention_mask = micro_batch['attention_mask']\n    position_ids = micro_batch.get('position_ids', None)\n\n    # 1. Select computation path based on padding removal optimization\n    # verl often uses flash_attn or custom kernels that handle packed sequences\n    use_remove_padding = getattr(self.config, 'use_remove_padding', False)\n\n    def get_logps(model, ids, mask, pos_ids):\n        if use_remove_padding:\n            # Optimized path for packed sequences\n            outputs = model(input_ids=ids, attention_mask=mask, position_ids=pos_ids, use_cache=False)\n        else:\n            # Standard path\n            outputs = model(input_ids=ids, attention_mask=mask, position_ids=pos_ids, use_cache=False)\n        \n        logits = outputs.logits\n        # Shift logits and labels to compute log_probs for the tokens actually generated\n        # logits: [batch, seq, vocab] -> [batch, seq-1, vocab]\n        # labels: [batch, seq] -> [batch, seq-1]\n        shift_logits = logits[:, :-1, :].contiguous()\n        shift_labels = ids[:, 1:].contiguous()\n        \n        log_probs = F.log_softmax(shift_logits, dim=-1)\n        per_token_logps = torch.gather(log_probs, dim=2, index=shift_labels.unsqueeze(2)).squeeze(2)\n        \n        # Pad back to original sequence length to align with input_ids\n        return F.pad(per_token_logps, (1, 0), value=0.0)\n\n    # 2. Compute log probabilities for Reward Model and Reference Model\n    # self.model is the current policy/reward model, self.ref_model is the frozen reference\n    rm_logps = get_logps(self.model, input_ids, attention_mask, position_ids)\n    ref_logps = get_logps(self.ref_model, input_ids, attention_mask, position_ids)\n\n    # 3. Implicit rewards: log probability differences on response sequences\n    # q_values represent the implicit reward signal (r = log(P_rm) - log(P_ref))\n    q_values = rm_logps - ref_logps\n\n    # Mask out the prompt part and padding\n    # Only compute rewards for the response part (indices >= prompt_length)\n    sequence_mask = attention_mask.clone()\n    sequence_mask[:, :prompt_length] = 0\n    q_values = q_values * sequence_mask\n\n    # 4. Process reward generation modes\n    # granularity can be 'token' (reward at every step) or 'whole' (accumulated at the end)\n    granularity = getattr(self.config, 'granularity', 'token')\n    \n    if granularity == 'token':\n        # In token granularity, each position receives its corresponding reward value\n        token_level_scores = q_values.clone()\n    elif granularity == 'whole':\n        # In whole granularity, all rewards accumulate to the last valid token position\n        token_level_scores = torch.zeros_like(q_values)\n        # Sum rewards across the sequence dimension for each batch element\n        total_reward = q_values.sum(dim=1)\n        # Find the last valid token index using the attention mask\n        last_token_indices = attention_mask.sum(dim=1) - 1\n        token_level_scores.scatter_(1, last_token_indices.unsqueeze(1), total_reward.unsqueeze(1))\n    else:\n        raise ValueError(f\"Unsupported granularity: {granularity}\")\n\n    # Note: GAE (Generalized Advantage Estimation) logic is typically handled in the \n    # trainer/actor-critic logic using these scores, but the raw scores are returned here.\n    \n    return token_level_scores, q_values\n```"], "usage": {"prompt_tokens": 455, "completion_tokens": 1066, "total_tokens": 1521, "per_completion": [{"prompt_tokens": 455, "completion_tokens": 1066, "total_tokens": 1521}]}}
{"function_name": "compute_ce_dpo_loss_rm", "overview": "Computes cross-entropy style DPO loss", "function_signature": "def compute_ce_dpo_loss_rm(token_level_scores, acc, response_mask, beta):", "input_parameters": "- `token_level_scores`: Token-level rewards\n- `acc`: Binary accuracy labels\n- `response_mask`: Response mask\n- `beta`: Temperature parameter", "detailed_description": "Computes sequence-level scores, converts them to probabilities, and finally computes cross-entropy loss.", "output": "Scalar loss value", "func_implementation": "", "test_code_path": "code/test_code/test_ce_dpo_loss.py", "implementation_location": "code/recipe/prime/prime_core_algos.py:line 82-85", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_ce_dpo_loss_rm`.\n\n**Function Overview:**\nComputes cross-entropy style DPO loss\n\n**Function Signature:**\n```python\ndef compute_ce_dpo_loss_rm(token_level_scores, acc, response_mask, beta):\n```\n\n**Input Parameters:**\n- `token_level_scores`: Token-level rewards\n- `acc`: Binary accuracy labels\n- `response_mask`: Response mask\n- `beta`: Temperature parameter\n\n**Detailed Description:**\nComputes sequence-level scores, converts them to probabilities, and finally computes cross-entropy loss.\n\n**Expected Output:**\nScalar loss value\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport torch\n\ndef compute_ce_dpo_loss_rm(token_level_scores, acc, response_mask, beta):\n    \"\"\"\n    Computes cross-entropy style DPO loss.\n    \n    This function calculates sequence-level scores from token-level rewards,\n    converts them to probabilities using a sigmoid function (scaled by beta),\n    and computes the cross-entropy loss against binary accuracy labels.\n\n    Args:\n        token_level_scores (torch.Tensor): Token-level rewards/scores.\n        acc (torch.Tensor): Binary accuracy labels (1 for chosen/correct, 0 for rejected/incorrect).\n        response_mask (torch.Tensor): Mask indicating which tokens belong to the response.\n        beta (float): Temperature parameter to scale the sequence-level scores.\n\n    Returns:\n        torch.Tensor: Scalar loss value.\n    \"\"\"\n    # 1. Compute sequence-level scores by summing token-level scores within the response mask\n    # We multiply by the mask to zero out non-response tokens\n    sequence_scores = torch.sum(token_level_scores * response_mask, dim=-1)\n    \n    # 2. Scale the sequence scores by the temperature parameter beta\n    # In DPO-style formulations, the implicit reward is often scaled by beta\n    scaled_scores = sequence_scores / beta\n    \n    # 3. Convert scores to probabilities using the sigmoid function\n    # prob = sigmoid(scaled_scores)\n    probs = torch.sigmoid(scaled_scores)\n    \n    # 4. Compute Cross-Entropy Loss\n    # Since acc is binary (0 or 1), we use the Binary Cross Entropy formula:\n    # loss = -[acc * log(prob) + (1 - acc) * log(1 - prob)]\n    # We use a small epsilon or torch.clamp to avoid log(0) if necessary, \n    # but standard functional BCE is more numerically stable.\n    loss = torch.nn.functional.binary_cross_entropy(\n        probs, \n        acc.to(probs.dtype), \n        reduction='mean'\n    )\n    \n    return loss\n```"], "usage": {"prompt_tokens": 331, "completion_tokens": 482, "total_tokens": 813, "per_completion": [{"prompt_tokens": 331, "completion_tokens": 482, "total_tokens": 813}]}}
{"function_name": "compute_detach_dpo_loss_rm", "overview": "Computes detached DPO loss, supporting BoN (Best-of-N) mode", "function_signature": "def compute_detach_dpo_loss_rm(token_level_scores, acc, Q_bc, acc_bc, response_mask, beta, bon_mode=\"none\"):", "input_parameters": "- Base: `token_level_scores`, `acc`, `response_mask`, `beta`\n- Contrast: `Q_bc` (batch Q-value matrix), `acc_bc` (batch accuracy matrix)\n- `bon_mode`: Weight mode {\"none\", \"bon_rm\", \"bon_acc\"}", "detailed_description": "Computes detached DPO loss based on contrastive samples, selecting samples with opposite accuracy labels within batch as contrastive baseline, and supports assigning higher training weights to high-quality samples through BoN weight mechanism.\nBoN weights include three modes: none mode, bon_rm mode, and bon_acc mode.", "output": "Weighted DPO loss value", "func_implementation": "", "test_code_path": "code/test_code/test_detach_dpo_loss.py", "implementation_location": "code/recipe/prime/prime_core_algos.py:line 88-116", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_detach_dpo_loss_rm`.\n\n**Function Overview:**\nComputes detached DPO loss, supporting BoN (Best-of-N) mode\n\n**Function Signature:**\n```python\ndef compute_detach_dpo_loss_rm(token_level_scores, acc, Q_bc, acc_bc, response_mask, beta, bon_mode=\"none\"):\n```\n\n**Input Parameters:**\n- Base: `token_level_scores`, `acc`, `response_mask`, `beta`\n- Contrast: `Q_bc` (batch Q-value matrix), `acc_bc` (batch accuracy matrix)\n- `bon_mode`: Weight mode {\"none\", \"bon_rm\", \"bon_acc\"}\n\n**Detailed Description:**\nComputes detached DPO loss based on contrastive samples, selecting samples with opposite accuracy labels within batch as contrastive baseline, and supports assigning higher training weights to high-quality samples through BoN weight mechanism.\nBoN weights include three modes: none mode, bon_rm mode, and bon_acc mode.\n\n**Expected Output:**\nWeighted DPO loss value\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport torch\nimport torch.nn.functional as F\n\ndef compute_detach_dpo_loss_rm(token_level_scores, acc, Q_bc, acc_bc, response_mask, beta, bon_mode=\"none\"):\n    \"\"\"\n    Computes detached DPO loss based on contrastive samples within a batch.\n    \n    Args:\n        token_level_scores (torch.Tensor): Log probabilities or scores from the current policy [B, L].\n        acc (torch.Tensor): Accuracy/Reward labels for the current samples [B].\n        Q_bc (torch.Tensor): Log probabilities or scores for the contrastive batch [B_bc, L].\n        acc_bc (torch.Tensor): Accuracy/Reward labels for the contrastive batch [B_bc].\n        response_mask (torch.Tensor): Mask for the response tokens [B, L].\n        beta (float): KL divergence coefficient (temperature).\n        bon_mode (str): Weighting mode (\"none\", \"bon_rm\", \"bon_acc\").\n        \n    Returns:\n        torch.Tensor: Weighted DPO loss value.\n    \"\"\"\n    # 1. Compute sequence-level scores (sum of token-level scores within response mask)\n    # Shape: [B]\n    Q = (token_level_scores * response_mask).sum(dim=-1)\n    \n    # 2. Compute sequence-level scores for the contrastive batch\n    # Note: Assuming response_mask for Q_bc is handled or Q_bc is already masked/summed.\n    # If Q_bc is [B_bc, L], we assume it's already processed or we use a similar mask.\n    # For standard detached DPO in verl, Q_bc is often the pre-summed log-probs.\n    if Q_bc.dim() > 1:\n        Q_bc_sum = Q_bc.sum(dim=-1)\n    else:\n        Q_bc_sum = Q_bc\n\n    # 3. Identify contrastive pairs (samples with opposite accuracy labels)\n    # acc: [B, 1], acc_bc: [1, B_bc]\n    acc = acc.view(-1, 1)\n    acc_bc = acc_bc.view(1, -1)\n    \n    # Mask where current sample is correct (1) and contrastive is wrong (0), or vice versa\n    # pos_mask: current is 1, contrast is 0\n    pos_mask = (acc == 1) & (acc_bc == 0)\n    # neg_mask: current is 0, contrast is 1\n    neg_mask = (acc == 0) & (acc_bc == 1)\n    \n    # 4. Compute DPO Logits\n    # diff: [B, B_bc]\n    # We want to maximize (Q_pos - Q_neg), which is equivalent to minimizing -log(sigmoid(beta * (Q_pos - Q_neg)))\n    diff = Q.view(-1, 1) - Q_bc_sum.view(1, -1)\n    \n    # 5. Apply BoN (Best-of-N) Weighting\n    # This scales the loss based on the quality of the samples\n    if bon_mode == \"bon_rm\":\n        # Weight based on reward model scores (if available in Q)\n        weight = torch.exp(Q.view(-1, 1) / beta)\n    elif bon_mode == \"bon_acc\":\n        # Weight based on accuracy (prioritizing correct samples)\n        weight = acc.float().view(-1, 1)\n    else:\n        weight = torch.ones_like(diff)\n\n    # 6. Calculate Loss\n    # For pos_mask: current is positive, bc is negative. Loss = -log_sigmoid(beta * (Q - Q_bc))\n    # For neg_mask: current is negative, bc is positive. Loss = -log_sigmoid(beta * (Q_bc - Q))\n    loss_pos = -F.logsigmoid(beta * diff) * pos_mask.float() * weight\n    loss_neg = -F.logsigmoid(beta * (-diff)) * neg_mask.float() * weight\n    \n    # Combine and normalize\n    total_mask = pos_mask.float() + neg_mask.float()\n    combined_loss = loss_pos + loss_neg\n    \n    if total_mask.sum() > 0:\n        final_loss = combined_loss.sum() / total_mask.sum()\n    else:\n        final_loss = combined_loss.sum() * 0.0 # Avoid NaN if no contrastive pairs found\n        \n    return final_loss\n```"], "usage": {"prompt_tokens": 433, "completion_tokens": 1079, "total_tokens": 1512, "per_completion": [{"prompt_tokens": 433, "completion_tokens": 1079, "total_tokens": 1512}]}}
{"function_name": "compute_dpo_accuracy", "overview": "Computes pairwise comparison DPO accuracy", "function_signature": "def compute_dpo_accuracy(token_level_scores, acc, response_mask, n_samples):", "input_parameters": "- `token_level_scores`: Token-level reward scores\n- `acc`: Accuracy labels\n- `response_mask`: Response mask\n- `n_samples`: Sample count per group", "detailed_description": "Uses pairwise comparison to evaluate reward model accuracy. Aggregates token-level scores to sequence-level scores, constructs sample pairs for comparison. Judges directional consistency by computing score differences and accuracy differences.\nUses weighted average to compute accuracy, with higher weights for sample pairs with larger accuracy differences. Returns 0.5 when all samples have the same accuracy.", "output": "Weighted pairwise comparison accuracy", "func_implementation": "", "test_code_path": "code/test_code/test_dpo_accuracy.py", "implementation_location": "code/recipe/prime/prime_core_algos.py:line 119-143", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_dpo_accuracy`.\n\n**Function Overview:**\nComputes pairwise comparison DPO accuracy\n\n**Function Signature:**\n```python\ndef compute_dpo_accuracy(token_level_scores, acc, response_mask, n_samples):\n```\n\n**Input Parameters:**\n- `token_level_scores`: Token-level reward scores\n- `acc`: Accuracy labels\n- `response_mask`: Response mask\n- `n_samples`: Sample count per group\n\n**Detailed Description:**\nUses pairwise comparison to evaluate reward model accuracy. Aggregates token-level scores to sequence-level scores, constructs sample pairs for comparison. Judges directional consistency by computing score differences and accuracy differences.\nUses weighted average to compute accuracy, with higher weights for sample pairs with larger accuracy differences. Returns 0.5 when all samples have the same accuracy.\n\n**Expected Output:**\nWeighted pairwise comparison accuracy\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport torch\n\ndef compute_dpo_accuracy(token_level_scores, acc, response_mask, n_samples):\n    \"\"\"\n    Computes pairwise comparison DPO accuracy.\n\n    Args:\n        token_level_scores: Token-level reward scores (Batch, Seq_len)\n        acc: Accuracy labels for each sequence (Batch,)\n        response_mask: Mask for response tokens (Batch, Seq_len)\n        n_samples: Number of samples per group (integer)\n\n    Returns:\n        Weighted pairwise comparison accuracy (float)\n    \"\"\"\n    # 1. Aggregate token-level scores to sequence-level scores\n    # We sum the scores for tokens within the response mask\n    seq_scores = torch.sum(token_level_scores * response_mask, dim=-1)\n\n    # 2. Reshape to group samples (Batch / n_samples, n_samples)\n    # Each row represents a group of responses for the same prompt\n    seq_scores = seq_scores.view(-1, n_samples)\n    acc = acc.view(-1, n_samples)\n\n    # 3. Construct pairwise comparisons within each group\n    # We use broadcasting to get all pairs (i, j)\n    # diff_scores[group, i, j] = score_i - score_j\n    diff_scores = seq_scores.unsqueeze(2) - seq_scores.unsqueeze(1)\n    # diff_acc[group, i, j] = acc_i - acc_j\n    diff_acc = acc.unsqueeze(2) - acc.unsqueeze(1)\n\n    # 4. Filter pairs where accuracy is different\n    # If acc_i == acc_j, the pair doesn't provide a preference signal\n    mask = diff_acc > 0\n    \n    # If no pairs have different accuracies, return 0.5 (random baseline)\n    if not torch.any(mask):\n        return torch.tensor(0.5, device=token_level_scores.device)\n\n    # 5. Compute directional consistency\n    # A \"correct\" prediction is when the score difference has the same sign as the accuracy difference\n    # Since we filtered for diff_acc > 0, we just check if diff_scores > 0\n    correct_predictions = (diff_scores > 0).float()\n\n    # 6. Weighted average calculation\n    # Higher weights for sample pairs with larger accuracy differences\n    weights = diff_acc.float()\n    \n    # Apply mask to only consider valid pairs (acc_i > acc_j)\n    weighted_correct = (correct_predictions * weights * mask.float()).sum()\n    total_weights = (weights * mask.float()).sum()\n\n    # Final weighted accuracy\n    dpo_acc = weighted_correct / total_weights\n\n    return dpo_acc\n```"], "usage": {"prompt_tokens": 379, "completion_tokens": 638, "total_tokens": 1017, "per_completion": [{"prompt_tokens": 379, "completion_tokens": 638, "total_tokens": 1017}]}}
{"function_name": "compute_dpo_abs_accuracy", "overview": "Computes absolute accuracy metric", "function_signature": "def compute_dpo_abs_accuracy(token_level_scores, acc, response_mask, n_samples):", "input_parameters": "- `token_level_scores`: Token-level reward scores\n- `acc`: Accuracy labels\n- `response_mask`: Response mask", "detailed_description": "Converts aggregated token-level scores to predicted labels, compares with true labels to compute absolute accuracy.", "output": "Absolute classification accuracy", "func_implementation": "", "test_code_path": "code/test_code/test_dpo_abs_accuracy.py", "implementation_location": "code/recipe/prime/prime_core_algos.py:line 146-147", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_dpo_abs_accuracy`.\n\n**Function Overview:**\nComputes absolute accuracy metric\n\n**Function Signature:**\n```python\ndef compute_dpo_abs_accuracy(token_level_scores, acc, response_mask, n_samples):\n```\n\n**Input Parameters:**\n- `token_level_scores`: Token-level reward scores\n- `acc`: Accuracy labels\n- `response_mask`: Response mask\n\n**Detailed Description:**\nConverts aggregated token-level scores to predicted labels, compares with true labels to compute absolute accuracy.\n\n**Expected Output:**\nAbsolute classification accuracy\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport torch\n\ndef compute_dpo_abs_accuracy(token_level_scores, acc, response_mask, n_samples):\n    \"\"\"\n    Computes absolute accuracy metric for DPO (Direct Preference Optimization).\n    \n    This function aggregates token-level scores into sequence-level scores,\n    converts them into binary predictions (positive if score > 0), and \n    compares them against the ground truth accuracy labels.\n\n    Args:\n        token_level_scores (torch.Tensor): Token-level reward scores, \n                                           shape [batch_size, seq_len].\n        acc (torch.Tensor): Ground truth accuracy labels (0 or 1), \n                            shape [batch_size].\n        response_mask (torch.Tensor): Mask indicating response tokens, \n                                      shape [batch_size, seq_len].\n        n_samples (int): Number of samples (usually batch size).\n\n    Returns:\n        float: The absolute classification accuracy.\n    \"\"\"\n    # 1. Aggregate token-level scores to sequence-level scores\n    # We sum the scores for tokens within the response mask\n    # shape: [batch_size]\n    seq_scores = torch.sum(token_level_scores * response_mask, dim=-1)\n\n    # 2. Convert aggregated scores to predicted labels\n    # In DPO/Reward modeling contexts, a positive score typically indicates \n    # a \"chosen\" or \"correct\" response.\n    # shape: [batch_size]\n    predictions = (seq_scores > 0).long()\n\n    # 3. Compare predictions with true labels (acc)\n    # acc is expected to be 1 for correct/chosen and 0 for incorrect/rejected\n    correct_predictions = (predictions == acc).float()\n\n    # 4. Compute the mean accuracy\n    abs_accuracy = torch.sum(correct_predictions) / n_samples\n\n    return abs_accuracy\n```"], "usage": {"prompt_tokens": 318, "completion_tokens": 434, "total_tokens": 752, "per_completion": [{"prompt_tokens": 318, "completion_tokens": 434, "total_tokens": 752}]}}
{"function_name": "compute_rloo_advantage_return", "overview": "Implements RLOO advantage estimation, supporting multiple reward source fusion", "function_signature": "def compute_rloo_advantage_return(data: verl.DataProto, response_mask: torch.Tensor, n_samples, config):", "input_parameters": "- `data`: Data protocol object containing DataProto.batch fields\n- `data.batch[\"rm_scores\"]`: Implicit reward scores\n- `data.batch[\"acc\"]`: Binary accuracy labels\n- `data.batch[\"prompts\"]`: Input prompt sequences\n- `data.batch[\"attention_mask\"]`: Attention mask\n- `response_mask`: Response mask indicating valid token positions\n- `n_samples`: Sample count per group, used for RLOO grouping computation\n- `config`: Reward weight configuration, hierarchical configuration object\n- `config.algorithm.reward_dpo_coef`: DPO reward weight coefficient, default 5.0\n- `config.algorithm.reward_gt_coef`: Accuracy reward weight coefficient, default 5.0", "detailed_description": "Function supports advantage computation with multi-source reward fusion. Reward sources include reward model scores and accuracy rewards.\nGroups multiple responses generated from the same prompt by n_samples intervals.\nUses RLOO to compute advantage function for each reward source and fuses multiple reward sources, computes cumulative return values after reward fusion and applies whitening.", "output": "- `advantages`: Whitened advantage function (torch.Tensor, shape: [batch_size, seq_len])\n- `returns`: Cumulative return values (torch.Tensor, shape: [batch_size, seq_len])", "func_implementation": "", "test_code_path": "code/test_code/test_rloo_advantage_return.py", "implementation_location": "code/recipe/prime/prime_core_algos.py:line 21-79", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_rloo_advantage_return`.\n\n**Function Overview:**\nImplements RLOO advantage estimation, supporting multiple reward source fusion\n\n**Function Signature:**\n```python\ndef compute_rloo_advantage_return(data: verl.DataProto, response_mask: torch.Tensor, n_samples, config):\n```\n\n**Input Parameters:**\n- `data`: Data protocol object containing DataProto.batch fields\n- `data.batch[\"rm_scores\"]`: Implicit reward scores\n- `data.batch[\"acc\"]`: Binary accuracy labels\n- `data.batch[\"prompts\"]`: Input prompt sequences\n- `data.batch[\"attention_mask\"]`: Attention mask\n- `response_mask`: Response mask indicating valid token positions\n- `n_samples`: Sample count per group, used for RLOO grouping computation\n- `config`: Reward weight configuration, hierarchical configuration object\n- `config.algorithm.reward_dpo_coef`: DPO reward weight coefficient, default 5.0\n- `config.algorithm.reward_gt_coef`: Accuracy reward weight coefficient, default 5.0\n\n**Detailed Description:**\nFunction supports advantage computation with multi-source reward fusion. Reward sources include reward model scores and accuracy rewards.\nGroups multiple responses generated from the same prompt by n_samples intervals.\nUses RLOO to compute advantage function for each reward source and fuses multiple reward sources, computes cumulative return values after reward fusion and applies whitening.\n\n**Expected Output:**\n- `advantages`: Whitened advantage function (torch.Tensor, shape: [batch_size, seq_len])\n- `returns`: Cumulative return values (torch.Tensor, shape: [batch_size, seq_len])\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport torch\nimport verl\n\ndef compute_rloo_advantage_return(data: verl.DataProto, response_mask: torch.Tensor, n_samples: int, config):\n    \"\"\"\n    Implements RLOO (REINFORCE Leave-One-Out) advantage estimation with multi-source reward fusion.\n    \n    Args:\n        data: verl.DataProto object containing batch data.\n        response_mask: Tensor of shape [batch_size, seq_len] indicating valid response tokens.\n        n_samples: Number of samples per prompt group.\n        config: Configuration object for reward coefficients.\n        \n    Returns:\n        advantages: Whitened advantage tensor [batch_size, seq_len].\n        returns: Cumulative return tensor [batch_size, seq_len].\n    \"\"\"\n    # 1. Extract reward components from data.batch\n    # rm_scores: Reward Model scores (often DPO-related or scalar RM)\n    # acc: Binary accuracy labels (Ground Truth)\n    rm_scores = data.batch['rm_scores'].to(torch.float32)\n    acc_scores = data.batch['acc'].to(torch.float32)\n\n    # 2. Fuse reward sources based on configuration coefficients\n    # Total Reward = coef_dpo * RM + coef_gt * Accuracy\n    reward_dpo_coef = getattr(config.algorithm, 'reward_dpo_coef', 5.0)\n    reward_gt_coef = getattr(config.algorithm, 'reward_gt_coef', 5.0)\n    \n    combined_rewards = reward_dpo_coef * rm_scores + reward_gt_coef * acc_scores\n\n    # 3. RLOO Advantage Computation\n    # RLOO computes the baseline for a sample as the average reward of the OTHER samples \n    # generated from the same prompt.\n    # Shape of combined_rewards: [batch_size]\n    batch_size = combined_rewards.shape[0]\n    assert batch_size % n_samples == 0, f\"Batch size {batch_size} must be divisible by n_samples {n_samples}\"\n\n    # Reshape to [num_prompts, n_samples]\n    reward_grouped = combined_rewards.view(-1, n_samples)\n    \n    # Compute the sum of rewards per group\n    reward_sum = torch.sum(reward_grouped, dim=1, keepdim=True)\n    \n    # RLOO baseline for sample i: (Sum_of_all_samples - reward_i) / (n_samples - 1)\n    # This is equivalent to the mean of all other samples in the group.\n    rloo_advantages = (reward_sum - reward_grouped) / (n_samples - 1)\n    \n    # The actual advantage is (reward_i - baseline_i)\n    # advantage_i = reward_i - (Sum - reward_i)/(n-1) = (n*reward_i - Sum)/(n-1)\n    advantages_flat = reward_grouped - rloo_advantages\n    advantages_flat = advantages_flat.view(-1) # Back to [batch_size]\n\n    # 4. Broadcast scalar advantages to token-level sequences\n    # In REINFORCE-style algorithms like RLOO, the sequence-level reward is typically \n    # attributed to the end of the sequence or distributed. Here we treat it as the return.\n    seq_len = response_mask.shape[1]\n    \n    # Create token-level returns and advantages\n    # We broadcast the scalar reward/advantage to all valid token positions\n    returns = advantages_flat.unsqueeze(1) * response_mask.to(torch.float32)\n    advantages = advantages_flat.unsqueeze(1) * response_mask.to(torch.float32)\n\n    # 5. Whitening (Normalization)\n    # Standardize advantages to have mean 0 and std 1 over the valid tokens in the batch\n    # This stabilizes training.\n    adv_mean = advantages[response_mask].mean()\n    adv_std = advantages[response_mask].std()\n    \n    # Avoid division by zero\n    advantages = (advantages - adv_mean) / (adv_std + 1e-8)\n    # Re-mask after whitening to ensure padding tokens remain 0\n    advantages = advantages * response_mask.to(torch.float32)\n\n    return advantages, returns\n```"], "usage": {"prompt_tokens": 554, "completion_tokens": 1008, "total_tokens": 1562, "per_completion": [{"prompt_tokens": 554, "completion_tokens": 1008, "total_tokens": 1562}]}}
{"function_name": "RayPRIMETrainer.filter_and_downsample", "overview": "Implements intelligent sample filtering and priority downsampling for PRIME algorithm", "function_signature": "def filter_and_downsample(self, scores, batch: DataProto):", "input_parameters": "- `scores`: Verification score list, binary accuracy labels\n- `batch`: Data batch object\n- `config`: Filtering parameter configuration", "detailed_description": "Filters high-quality samples from oversampled candidate responses. Reconstructs verification scores into 2D matrix, performs dual filtering: accuracy filtering and length filtering.\nAccuracy filtering retains sample groups with average accuracy within configured range; length filtering removes sample groups exceeding maximum length threshold.\nSample groups meeting filtering conditions are retained with priority, ultimately retaining 1/oversample_factor proportion of samples. If all groups fail to meet filtering conditions, algorithm still retains corresponding proportion of samples to ensure training continues.", "output": "Filtered high-quality data batch", "func_implementation": "", "test_code_path": "code/test_code/test_filter_and_downsample.py", "implementation_location": "code/recipe/prime/prime_ray_trainer.py:line 540-572", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `RayPRIMETrainer.filter_and_downsample`.\n\n**Function Overview:**\nImplements intelligent sample filtering and priority downsampling for PRIME algorithm\n\n**Function Signature:**\n```python\ndef filter_and_downsample(self, scores, batch: DataProto):\n```\n\n**Input Parameters:**\n- `scores`: Verification score list, binary accuracy labels\n- `batch`: Data batch object\n- `config`: Filtering parameter configuration\n\n**Detailed Description:**\nFilters high-quality samples from oversampled candidate responses. Reconstructs verification scores into 2D matrix, performs dual filtering: accuracy filtering and length filtering.\nAccuracy filtering retains sample groups with average accuracy within configured range; length filtering removes sample groups exceeding maximum length threshold.\nSample groups meeting filtering conditions are retained with priority, ultimately retaining 1/oversample_factor proportion of samples. If all groups fail to meet filtering conditions, algorithm still retains corresponding proportion of samples to ensure training continues.\n\n**Expected Output:**\nFiltered high-quality data batch\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport torch\nimport numpy as np\nfrom verl import DataProto\n\ndef filter_and_downsample(self, scores, batch: DataProto):\n    \"\"\"\n    Implements intelligent sample filtering and priority downsampling for the PRIME algorithm.\n    \n    Args:\n        scores (torch.Tensor or list): Verification score list (binary accuracy labels).\n        batch (DataProto): Data batch object containing responses and metadata.\n        \n    Returns:\n        DataProto: Filtered high-quality data batch.\n    \"\"\"\n    # 1. Configuration extraction\n    # oversample_factor: how many responses were generated per prompt\n    # filter_range: [min_avg_acc, max_avg_acc] for accuracy filtering\n    # max_token_len: threshold for length filtering\n    oversample_factor = self.config.algorithm.prime_oversample_factor\n    filter_range = self.config.algorithm.prime_filter_range # e.g., [0.1, 0.9]\n    max_token_len = self.config.algorithm.prime_max_token_len\n    \n    num_total_samples = len(scores)\n    num_prompts = num_total_samples // oversample_factor\n    \n    # 2. Reconstruct scores and metadata into 2D matrices [num_prompts, oversample_factor]\n    # scores shape: (num_total_samples,) -> (num_prompts, oversample_factor)\n    scores_tensor = torch.tensor(scores, device='cpu')\n    scores_matrix = scores_tensor.view(num_prompts, oversample_factor)\n    \n    # Calculate response lengths (assuming batch.batch['responses'] exists)\n    # Shape: (num_total_samples,)\n    response_lengths = batch.batch['responses'].shape[1] - (batch.batch['responses'] == 0).sum(dim=1)\n    lengths_matrix = response_lengths.view(num_prompts, oversample_factor)\n    \n    # 3. Dual Filtering Logic\n    # Calculate average accuracy per prompt group\n    avg_accuracy_per_group = scores_matrix.mean(dim=1)\n    \n    # Accuracy filtering: average accuracy within configured range\n    acc_mask = (avg_accuracy_per_group >= filter_range[0]) & (avg_accuracy_per_group <= filter_range[1])\n    \n    # Length filtering: remove groups where any response exceeds max_token_len\n    # (Alternatively, could be based on average length, but usually strict per group)\n    len_mask = (lengths_matrix.max(dim=1).values <= max_token_len)\n    \n    # Combined high-quality mask\n    high_quality_mask = acc_mask & len_mask\n    \n    # 4. Priority Downsampling\n    # We need to select 1 response per prompt group (1/oversample_factor proportion)\n    selected_indices = []\n    \n    for i in range(num_prompts):\n        group_scores = scores_matrix[i]\n        \n        # If the group is \"high quality\", we prioritize it. \n        # In PRIME, we often pick a specific sample (e.g., a random one or a specific label)\n        # Here we implement the logic to ensure we pick 1 sample per prompt.\n        \n        if high_quality_mask[i]:\n            # Logic: Pick a sample from the high quality group\n            # For PRIME, we often pick a random index within the group\n            idx_in_group = torch.randint(0, oversample_factor, (1,)).item()\n        else:\n            # If group fails filtering, we still retain a sample to ensure training continues\n            # but we might flag it or just pick a default one\n            idx_in_group = torch.randint(0, oversample_factor, (1,)).item()\n            \n        global_idx = i * oversample_factor + idx_in_group\n        selected_indices.append(global_idx)\n        \n    # 5. Reconstruct the DataProto batch with selected indices\n    # This selects the corresponding rows from all tensors in the DataProto object\n    filtered_batch = batch[selected_indices]\n    \n    return filtered_batch\n```"], "usage": {"prompt_tokens": 400, "completion_tokens": 972, "total_tokens": 1372, "per_completion": [{"prompt_tokens": 400, "completion_tokens": 972, "total_tokens": 1372}]}}
