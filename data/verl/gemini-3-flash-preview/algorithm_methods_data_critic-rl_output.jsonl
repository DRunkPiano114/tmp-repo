{"function_name": "RewardModelForCritic.build_revision_messages", "overview": "Builds prompt messages for generating revised code. Combines original problem, initial solution, and critique into chat template format.", "function_signature": "def build_revision_messages(self, init_messages, critique):", "input_parameters": "- `init_messages`: Initial dialogue history list containing problem (user role) and initial solution (assistant role)\n- `critique`: Model-generated critique text string", "detailed_description": "Function receives initial dialogue history and critique text, validates that initial dialogue history is list type and last message role is assistant, calls related functions for content cleaning (keeping only code portion) and updating, then converts processed plain text critique with no leading/trailing whitespace and revision instruction \"Please finalize your answer accordingly using the same format.\" into new message with user role appended to end of dialogue.", "output": "- Complete dialogue message list (List[Dict]) containing original problem, cleaned initial code, and revision instruction", "func_implementation": "", "test_code_path": "code/test_code/test_build_revision_messages.py", "implementation_location": "code/ctrl/rl/critic_rm.py:line 136-150", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `RewardModelForCritic.build_revision_messages`.\n\n**Function Overview:**\nBuilds prompt messages for generating revised code. Combines original problem, initial solution, and critique into chat template format.\n\n**Function Signature:**\n```python\ndef build_revision_messages(self, init_messages, critique):\n```\n\n**Input Parameters:**\n- `init_messages`: Initial dialogue history list containing problem (user role) and initial solution (assistant role)\n- `critique`: Model-generated critique text string\n\n**Detailed Description:**\nFunction receives initial dialogue history and critique text, validates that initial dialogue history is list type and last message role is assistant, calls related functions for content cleaning (keeping only code portion) and updating, then converts processed plain text critique with no leading/trailing whitespace and revision instruction \"Please finalize your answer accordingly using the same format.\" into new message with user role appended to end of dialogue.\n\n**Expected Output:**\n- Complete dialogue message list (List[Dict]) containing original problem, cleaned initial code, and revision instruction\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\ndef build_revision_messages(self, init_messages, critique):\n    \"\"\"\n    Builds prompt messages for generating revised code. \n    Combines original problem, initial solution, and critique into chat template format.\n\n    Args:\n        init_messages (List[Dict]): Initial dialogue history list containing problem (user role) \n                                    and initial solution (assistant role).\n        critique (str): Model-generated critique text string.\n\n    Returns:\n        List[Dict]: Complete dialogue message list containing original problem, \n                    cleaned initial code, and revision instruction.\n    \"\"\"\n    # 1. Validate that initial dialogue history is a list\n    if not isinstance(init_messages, list):\n        raise TypeError(\"init_messages must be a list of message dictionaries.\")\n\n    # 2. Validate that the last message role is 'assistant'\n    if not init_messages or init_messages[-1].get('role') != 'assistant':\n        raise ValueError(\"The last message in init_messages must have the role 'assistant'.\")\n\n    # 3. Content cleaning and updating\n    # Note: Assuming self.clean_code and self.update_messages are helper methods \n    # available in the RewardModelForCritic class as per verl framework patterns.\n    # We extract the code portion from the initial solution.\n    cleaned_init_messages = self.update_messages(init_messages)\n\n    # 4. Process the critique text\n    # Remove leading/trailing whitespace\n    processed_critique = critique.strip()\n    \n    # 5. Construct the revision instruction\n    revision_instruction = (\n        f\"{processed_critique}\\n\\n\"\n        \"Please finalize your answer accordingly using the same format.\"\n    )\n\n    # 6. Append the revision instruction as a new user message\n    # We create a copy to avoid mutating the input list in-place\n    revision_messages = list(cleaned_init_messages)\n    revision_messages.append({\n        \"role\": \"user\",\n        \"content\": revision_instruction\n    })\n\n    return revision_messages\n```"], "usage": {"prompt_tokens": 409, "completion_tokens": 467, "total_tokens": 876, "per_completion": [{"prompt_tokens": 409, "completion_tokens": 467, "total_tokens": 876}]}}
{"function_name": "RewardModelForCritic.get_reward_all", "overview": "Concurrently executes sandbox tests for multiple critique-revision pairs, computing verifiable rewards. Improves execution efficiency through asynchronous I/O mechanism, supporting large-scale parallel evaluation.", "function_signature": "async def get_reward_all(\nself, critiques: List[str], revisions: List[str], samples: List[Any]\n) -> List[float]:", "input_parameters": "- `critiques`: Critique text list, each element corresponding to a generated critique\n- `revisions`: Revised code list, one-to-one correspondence with critiques\n- `samples`: Test sample list containing test cases and problem metadata", "detailed_description": "Function calls underlying reward computation method for each triplet (sample, critique, revision), evaluating improvement quality of revised version relative to original sample, returning numerical reward scores. All computation tasks execute asynchronously and concurrently, maximizing processing throughput while ensuring resource control, with default maximum concurrency set to MAX_REQUESTS (256), requiring progress bar (\"Generating rewards\") for real-time completion feedback.", "output": "- Float list, each element representing reward value for corresponding critique-revision pair, range [0.0, 1.0]", "func_implementation": "", "test_code_path": "code/test_code/test_get_reward_all.py", "implementation_location": "code/ctrl/rl/critic_rm.py:line 198-209", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `RewardModelForCritic.get_reward_all`.\n\n**Function Overview:**\nConcurrently executes sandbox tests for multiple critique-revision pairs, computing verifiable rewards. Improves execution efficiency through asynchronous I/O mechanism, supporting large-scale parallel evaluation.\n\n**Function Signature:**\n```python\nasync def get_reward_all(\nself, critiques: List[str], revisions: List[str], samples: List[Any]\n) -> List[float]:\n```\n\n**Input Parameters:**\n- `critiques`: Critique text list, each element corresponding to a generated critique\n- `revisions`: Revised code list, one-to-one correspondence with critiques\n- `samples`: Test sample list containing test cases and problem metadata\n\n**Detailed Description:**\nFunction calls underlying reward computation method for each triplet (sample, critique, revision), evaluating improvement quality of revised version relative to original sample, returning numerical reward scores. All computation tasks execute asynchronously and concurrently, maximizing processing throughput while ensuring resource control, with default maximum concurrency set to MAX_REQUESTS (256), requiring progress bar (\"Generating rewards\") for real-time completion feedback.\n\n**Expected Output:**\n- Float list, each element representing reward value for corresponding critique-revision pair, range [0.0, 1.0]\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport asyncio\nfrom typing import List, Any\nfrom tqdm.asyncio import tqdm\n\nclass RewardModelForCritic:\n    \"\"\"\n    A reward model class for evaluating critique-revision pairs in verl.\n    \"\"\"\n    \n    def __init__(self):\n        # Default maximum concurrency to prevent resource exhaustion\n        self.MAX_REQUESTS = 256\n\n    async def get_reward_single(self, critique: str, revision: str, sample: Any) -> float:\n        \"\"\"\n        Underlying method to compute reward for a single triplet.\n        This should be implemented to interface with the specific sandbox or \n        verifiable reward logic (e.g., executing code against test cases).\n        \"\"\"\n        # Placeholder for actual reward logic:\n        # 1. Parse the revision (code)\n        # 2. Run against sample['test_cases']\n        # 3. Compare performance/correctness vs original\n        # 4. Return a float in [0.0, 1.0]\n        raise NotImplementedError(\"Subclasses must implement get_reward_single\")\n\n    async def get_reward_all(\n        self, critiques: List[str], revisions: List[str], samples: List[Any]\n    ) -> List[float]:\n        \"\"\"\n        Concurrently executes sandbox tests for multiple critique-revision pairs.\n        \n        Args:\n            critiques: List of generated critique strings.\n            revisions: List of revised code strings.\n            samples: List of metadata/test cases for each problem.\n            \n        Returns:\n            List[float]: Reward scores in the range [0.0, 1.0].\n        \"\"\"\n        # Ensure inputs are of equal length\n        assert len(critiques) == len(revisions) == len(samples), \"Input lists must have the same length\"\n\n        # Semaphore to control maximum concurrency (MAX_REQUESTS)\n        semaphore = asyncio.Semaphore(self.MAX_REQUESTS)\n\n        async def wrapped_get_reward(critique, revision, sample):\n            async with semaphore:\n                # Call the underlying reward computation logic\n                return await self.get_reward_single(critique, revision, sample)\n\n        # Create a list of tasks for all triplets\n        tasks = [\n            wrapped_get_reward(c, r, s) \n            for c, r, s in zip(critiques, revisions, samples)\n        ]\n\n        # Execute tasks concurrently with a progress bar\n        rewards = []\n        for f in tqdm(\n            asyncio.as_completed(tasks), \n            total=len(tasks), \n            desc=\"Generating rewards\"\n        ):\n            reward = await f\n            rewards.append(reward)\n\n        # Note: asyncio.as_completed does not guarantee order. \n        # If order must be preserved (matching input indices), use asyncio.gather:\n        \n        # To preserve order while using tqdm:\n        # rewards = await tqdm.gather(*tasks, desc=\"Generating rewards\")\n        \n        # Re-implementing with gather to ensure the output list matches the input order:\n        rewards = await tqdm.gather(*tasks, desc=\"Generating rewards\")\n\n        return rewards\n```"], "usage": {"prompt_tokens": 458, "completion_tokens": 713, "total_tokens": 1171, "per_completion": [{"prompt_tokens": 458, "completion_tokens": 713, "total_tokens": 1171}]}}
{"function_name": "sanitize", "overview": "Extracts pure code blocks from mixed text containing code and explanations, removes explanatory text, preserves markdown format code block structure.", "function_signature": "def sanitize(text):", "input_parameters": "- `text`: Mixed text string containing code blocks", "detailed_description": "Function uses regular expressions to identify and extract markdown format code blocks, matching code blocks wrapped in triple backticks through regex, recognizing cases with or without python identifier. Matching process is case-insensitive. After successful match, extracts content of first code block and reformats it to standard markdown format ````python\\n{code}\\n``` for return. If no code block found in text or code block is empty, directly returns original text.", "output": "- Formatted code block string (````python\\n{code}\\n```) or original text (when no code block)\nIn `build_revision_messages` function, used to clean model-generated initial solution, remove explanatory text, keep only code portion, ensuring conciseness of revision prompt.", "func_implementation": "", "test_code_path": "code/test_code/test_sanitize_desanitize.py", "implementation_location": "code/ctrl/rl/critic_rm.py:line 76-84", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `sanitize`.\n\n**Function Overview:**\nExtracts pure code blocks from mixed text containing code and explanations, removes explanatory text, preserves markdown format code block structure.\n\n**Function Signature:**\n```python\ndef sanitize(text):\n```\n\n**Input Parameters:**\n- `text`: Mixed text string containing code blocks\n\n**Detailed Description:**\nFunction uses regular expressions to identify and extract markdown format code blocks, matching code blocks wrapped in triple backticks through regex, recognizing cases with or without python identifier. Matching process is case-insensitive. After successful match, extracts content of first code block and reformats it to standard markdown format ````python\\n{code}\\n``` for return. If no code block found in text or code block is empty, directly returns original text.\n\n**Expected Output:**\n- Formatted code block string (````python\\n{code}\\n```) or original text (when no code block)\nIn `build_revision_messages` function, used to clean model-generated initial solution, remove explanatory text, keep only code portion, ensuring conciseness of revision prompt.\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport re\n\ndef sanitize(text):\n    \"\"\"\n    Extracts the first code block from mixed text and reformats it into a standard \n    markdown Python block. If no code block is found, returns the original text.\n    \n    Args:\n        text (str): Mixed text string containing code blocks and explanations.\n        \n    Returns:\n        str: Formatted code block (```python\\n{code}\\n```) or original text.\n    \"\"\"\n    # Regex pattern to match markdown code blocks:\n    # - ```: Matches triple backticks\n    # - (?:python|py)? : Non-capturing group for optional language identifier (case-insensitive)\n    # - \\s*: Optional whitespace\n    # - (.*?): Capturing group for the actual code content (non-greedy)\n    # - ```: Matches closing triple backticks\n    # re.DOTALL allows '.' to match newlines\n    # re.IGNORECASE handles Python/PYTHON/python identifiers\n    pattern = r\"```(?:python|py)?\\s*(.*?)```\"\n    \n    match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)\n    \n    if match:\n        # Extract the content of the first code block\n        code_content = match.group(1).strip()\n        \n        # If the code block is not empty, return it in standard format\n        if code_content:\n            return f\"```python\\n{code_content}\\n```\"\n    \n    # If no code block is found or the code block is empty, return original text\n    return text\n```"], "usage": {"prompt_tokens": 419, "completion_tokens": 361, "total_tokens": 780, "per_completion": [{"prompt_tokens": 419, "completion_tokens": 361, "total_tokens": 780}]}}
{"function_name": "desanitize", "overview": "Removes markdown code block markers (triple backticks) from code text, extracts pure code string.", "function_signature": "def desanitize(text):", "input_parameters": "- `text`: Text with or without markdown code block markers", "detailed_description": "Function uses regular expression to match code blocks wrapped in triple backticks, recognizing cases with or without python identifier. After successful match, extracts content inside code block, returns after removing leading/trailing whitespace. If no code block format found in text, directly returns original text.", "output": "- Pure code string with markdown markers removed\nIn `get_sandbox_response` function, used to extract pure code from revision text, remove markdown format markers, prepare code for sandbox execution.", "func_implementation": "", "test_code_path": "code/test_code/test_sanitize_desanitize.py", "implementation_location": "code/ctrl/rl/critic_rm.py:line 66-73", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `desanitize`.\n\n**Function Overview:**\nRemoves markdown code block markers (triple backticks) from code text, extracts pure code string.\n\n**Function Signature:**\n```python\ndef desanitize(text):\n```\n\n**Input Parameters:**\n- `text`: Text with or without markdown code block markers\n\n**Detailed Description:**\nFunction uses regular expression to match code blocks wrapped in triple backticks, recognizing cases with or without python identifier. After successful match, extracts content inside code block, returns after removing leading/trailing whitespace. If no code block format found in text, directly returns original text.\n\n**Expected Output:**\n- Pure code string with markdown markers removed\nIn `get_sandbox_response` function, used to extract pure code from revision text, remove markdown format markers, prepare code for sandbox execution.\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport re\n\ndef desanitize(text):\n    \"\"\"\n    Removes markdown code block markers (triple backticks) from code text and extracts the pure code string.\n    \n    This function is typically used in the verl framework to prepare code generated by an LLM \n    for execution in a sandbox environment.\n    \n    Args:\n        text (str): The input text which may contain markdown code blocks.\n        \n    Returns:\n        str: The extracted code string without markdown markers, or the original text if no markers are found.\n    \"\"\"\n    # Regular expression to match code blocks wrapped in triple backticks.\n    # It accounts for optional language identifiers (like 'python') immediately after the first set of backticks.\n    # The re.DOTALL flag ensures that the dot (.) matches newline characters as well.\n    pattern = r\"```(?:[a-zA-Z0-9]+)?\\n?(.*?)```\"\n    \n    match = re.search(pattern, text, re.DOTALL)\n    \n    if match:\n        # Extract the content inside the code block\n        code = match.group(1)\n        # Return the code after removing leading/trailing whitespace\n        return code.strip()\n    \n    # If no code block format is found, return the original text\n    return text\n```"], "usage": {"prompt_tokens": 360, "completion_tokens": 295, "total_tokens": 655, "per_completion": [{"prompt_tokens": 360, "completion_tokens": 295, "total_tokens": 655}]}}
{"function_name": "normalize_code", "overview": "Normalizes Python code through Abstract Syntax Tree (AST) level transformation, unifies variable naming, improves cache hit rate.", "function_signature": "def normalize_code(code_str):", "input_parameters": "- `code_str`: Python code string to be normalized", "detailed_description": "Function uniformly renames variables in Python code to standard form (v_0, v_1, ...) through AST parsing, enabling functionally identical but differently named code to be recognized as equivalent. If code has syntax errors, function returns original code without transformation.", "output": "- Normalized code string, or original code (when parsing fails)\nDuring training, model may generate multiple semantically identical but differently named code versions. For example, `x = 1; return x` and `y = 1; return y` are functionally completely equivalent. Through normalization, both are converted to `v_0 = 1; return v_0`, using same cache key, avoiding repeated sandbox execution, significantly improving training efficiency.", "func_implementation": "", "test_code_path": "code/test_code/test_normalize_code.py", "implementation_location": "code/ctrl/rl/critic_rm.py:line 35-63", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `normalize_code`.\n\n**Function Overview:**\nNormalizes Python code through Abstract Syntax Tree (AST) level transformation, unifies variable naming, improves cache hit rate.\n\n**Function Signature:**\n```python\ndef normalize_code(code_str):\n```\n\n**Input Parameters:**\n- `code_str`: Python code string to be normalized\n\n**Detailed Description:**\nFunction uniformly renames variables in Python code to standard form (v_0, v_1, ...) through AST parsing, enabling functionally identical but differently named code to be recognized as equivalent. If code has syntax errors, function returns original code without transformation.\n\n**Expected Output:**\n- Normalized code string, or original code (when parsing fails)\nDuring training, model may generate multiple semantically identical but differently named code versions. For example, `x = 1; return x` and `y = 1; return y` are functionally completely equivalent. Through normalization, both are converted to `v_0 = 1; return v_0`, using same cache key, avoiding repeated sandbox execution, significantly improving training efficiency.\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport ast\n\ndef normalize_code(code_str):\n    \"\"\"\n    Normalizes Python code by renaming variables to a standard form (v_0, v_1, ...) \n    using AST transformation. This improves cache hit rates for semantically \n    identical code snippets.\n    \n    Args:\n        code_str (str): The Python code string to be normalized.\n        \n    Returns:\n        str: The normalized code string, or the original string if parsing fails.\n    \"\"\"\n    try:\n        # Parse the code into an Abstract Syntax Tree\n        tree = ast.parse(code_str)\n    except Exception:\n        # If there's a syntax error or parsing fails, return the original code\n        return code_str\n\n    # Dictionary to map original variable names to normalized names (v_0, v_1, ...)\n    name_map = {}\n    # Counter to generate unique indices for normalized names\n    counter = 0\n\n    class NameNormalizer(ast.NodeTransformer):\n        \"\"\"\n        AST NodeTransformer that visits Name nodes and renames them.\n        \"\"\"\n        def visit_Name(self, node):\n            nonlocal counter\n            # We only want to rename user-defined variables, not built-ins or constants\n            # Note: This simple implementation renames all identifiers. \n            # In a production verl environment, you might exclude common built-ins.\n            if node.id not in name_map:\n                name_map[node.id] = f\"v_{counter}\"\n                counter += 1\n            \n            # Create a new Name node with the mapped ID\n            return ast.copy_location(\n                ast.Name(id=name_map[node.id], ctx=node.ctx), \n                node\n            )\n\n        def visit_arg(self, node):\n            \"\"\"Handles function arguments.\"\"\"\n            nonlocal counter\n            if node.arg not in name_map:\n                name_map[node.arg] = f\"v_{counter}\"\n                counter += 1\n            \n            node.arg = name_map[node.arg]\n            return self.generic_visit(node)\n\n        def visit_FunctionDef(self, node):\n            \"\"\"Handles function names.\"\"\"\n            nonlocal counter\n            if node.name not in name_map:\n                name_map[node.name] = f\"v_{counter}\"\n                counter += 1\n            node.name = name_map[node.name]\n            return self.generic_visit(node)\n\n    # Transform the tree\n    transformer = NameNormalizer()\n    normalized_tree = transformer.visit(tree)\n    \n    # Fix missing line numbers and attributes after transformation\n    ast.fix_missing_locations(normalized_tree)\n\n    try:\n        # Convert the AST back into a string\n        # ast.unparse is available in Python 3.9+\n        return ast.unparse(normalized_tree)\n    except Exception:\n        return code_str\n\n# Example Usage:\n# code1 = \"x = 1\\nreturn x\"\n# code2 = \"y = 1\\nreturn y\"\n# normalize_code(code1) == normalize_code(code2) -> True\n```"], "usage": {"prompt_tokens": 416, "completion_tokens": 729, "total_tokens": 1145, "per_completion": [{"prompt_tokens": 416, "completion_tokens": 729, "total_tokens": 1145}]}}
