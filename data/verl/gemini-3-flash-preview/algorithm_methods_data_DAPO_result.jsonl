{"function_name": "agg_loss", "overview": "Aggregates loss matrix to scalar, supporting multiple aggregation modes. DAPO recommends using `token-mean` mode, making each token's contribution to loss equal, avoiding long sequences dominating gradients.", "function_signature": "def agg_loss(loss_mat: torch.Tensor, loss_mask: torch.Tensor, loss_agg_mode: str):", "input_parameters": "- `loss_mat`: Loss matrix with shape `(batch_size, response_length)`\n- `loss_mask`: Mask tensor with shape `(batch_size, response_length)` (response part is 1, padding is 0)\n- `loss_agg_mode`: String specifying aggregation mode\n- `\"token-mean\"`: Average over all tokens (**DAPO recommended**)\n- `\"seq-mean-token-sum\"`: Sum tokens within sequences first, then average across sequences\n- `\"seq-mean-token-mean\"`: Average tokens within sequences first, then average across sequences\n- `\"seq-mean-token-sum-norm\"`: Sum sequences then normalize by sequence length", "detailed_description": "The core function is to aggregate a 2D loss matrix into a single scalar value, supporting the following four aggregation strategies: token-mean, seq-mean-token-sum, seq-mean-token-mean, seq-mean-token-sum-norm.", "output": "- Scalar loss value for backpropagation", "func_implementation": "", "test_code_path": "code/test_code/test_agg_loss.py", "implementation_location": "code/verl/trainer/ppo/core_algos.py:line 704-737", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `agg_loss`.\n\n**Function Overview:**\nAggregates loss matrix to scalar, supporting multiple aggregation modes. DAPO recommends using `token-mean` mode, making each token's contribution to loss equal, avoiding long sequences dominating gradients.\n\n**Function Signature:**\n```python\ndef agg_loss(loss_mat: torch.Tensor, loss_mask: torch.Tensor, loss_agg_mode: str):\n```\n\n**Input Parameters:**\n- `loss_mat`: Loss matrix with shape `(batch_size, response_length)`\n- `loss_mask`: Mask tensor with shape `(batch_size, response_length)` (response part is 1, padding is 0)\n- `loss_agg_mode`: String specifying aggregation mode\n- `\"token-mean\"`: Average over all tokens (**DAPO recommended**)\n- `\"seq-mean-token-sum\"`: Sum tokens within sequences first, then average across sequences\n- `\"seq-mean-token-mean\"`: Average tokens within sequences first, then average across sequences\n- `\"seq-mean-token-sum-norm\"`: Sum sequences then normalize by sequence length\n\n**Detailed Description:**\nThe core function is to aggregate a 2D loss matrix into a single scalar value, supporting the following four aggregation strategies: token-mean, seq-mean-token-sum, seq-mean-token-mean, seq-mean-token-sum-norm.\n\n**Expected Output:**\n- Scalar loss value for backpropagation\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport torch\n\ndef agg_loss(loss_mat: torch.Tensor, loss_mask: torch.Tensor, loss_agg_mode: str):\n    \"\"\"\n    Aggregates loss matrix to scalar, supporting multiple aggregation modes.\n    \n    Args:\n        loss_mat: Loss matrix with shape (batch_size, response_length)\n        loss_mask: Mask tensor with shape (batch_size, response_length) (response part is 1, padding is 0)\n        loss_agg_mode: String specifying aggregation mode\n        \n    Returns:\n        torch.Tensor: Scalar loss value for backpropagation\n    \"\"\"\n    # Apply mask to the loss matrix to ensure padding doesn't contribute\n    masked_loss = loss_mat * loss_mask\n    \n    if loss_agg_mode == \"token-mean\":\n        # DAPO recommended: Average over all valid tokens across the entire batch\n        # Total sum of loss / Total number of valid tokens\n        return masked_loss.sum() / loss_mask.sum().clamp(min=1.0)\n\n    elif loss_agg_mode == \"seq-mean-token-sum\":\n        # Sum tokens within each sequence first, then average across the batch dimension\n        # (Sum per row) -> Mean of sums\n        seq_sum = masked_loss.sum(dim=-1)\n        return seq_sum.mean()\n\n    elif loss_agg_mode == \"seq-mean-token-mean\":\n        # Average tokens within each sequence first, then average across the batch dimension\n        # (Sum per row / valid tokens per row) -> Mean of means\n        seq_sum = masked_loss.sum(dim=-1)\n        seq_counts = loss_mask.sum(dim=-1).clamp(min=1.0)\n        seq_mean = seq_sum / seq_counts\n        return seq_mean.mean()\n\n    elif loss_agg_mode == \"seq-mean-token-sum-norm\":\n        # Sum sequences then normalize by sequence length\n        # This is often used to prevent long sequences from having disproportionately large gradients\n        # while maintaining a sequence-level normalization perspective.\n        seq_sum = masked_loss.sum(dim=-1)\n        # Normalize by the average sequence length in the batch\n        avg_seq_len = loss_mask.sum(dim=-1).mean().clamp(min=1.0)\n        return seq_sum.mean() / avg_seq_len\n\n    else:\n        raise ValueError(f\"Unknown loss_agg_mode: {loss_agg_mode}. \"\n                         f\"Supported modes: ['token-mean', 'seq-mean-token-sum', \"\n                         f\"'seq-mean-token-mean', 'seq-mean-token-sum-norm']\")\n```"], "usage": {"prompt_tokens": 508, "completion_tokens": 627, "total_tokens": 1135, "per_completion": [{"prompt_tokens": 508, "completion_tokens": 627, "total_tokens": 1135}]}, "results": [false], "pass_ratios": [0.9090909090909091]}
{"function_name": "compute_policy_loss_vanilla", "overview": "Computes PPO policy gradient loss, supporting decoupled clipping (DAPO's core innovation). Through `clip_ratio_low` and `clip_ratio_high` parameters, allows asymmetric clipping for positive and negative advantages, implementing the \"Clip-Higher\" mechanism.", "function_signature": "def compute_policy_loss_vanilla(\nold_log_prob: torch.Tensor,\nlog_prob: torch.Tensor,\nadvantages: torch.Tensor,\nresponse_mask: torch.Tensor,\nloss_agg_mode: str = \"token-mean\",\nconfig: Optional[DictConfig | AlgoConfig] = None,\nrollout_log_probs: torch.Tensor | None = None,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:", "input_parameters": "- `old_log_prob`: Old policy log probabilities with shape `(batch_size, response_length)`\n- `log_prob`: Current policy log probabilities with shape `(batch_size, response_length)`\n- `advantages`: Advantage estimates with shape `(batch_size, response_length)`\n- `response_mask`: Response mask with shape `(batch_size, response_length)`\n- `loss_agg_mode`: Loss aggregation mode (default is token-mean)\n- `config`: Actor configuration object, containing:\n- `clip_ratio`: Default clipping parameter (backward compatibility)\n- `clip_ratio_low`: Lower bound clipping parameter (**DAPO new**)\n- `clip_ratio_high`: Upper bound clipping parameter (**DAPO new**)\n- `clip_ratio_c`: Dual clipping lower bound (handles extreme negative advantages, default 3.0)\n- `rollout_log_probs`: Optional, Rollout policy log probabilities (for off-policy training)", "detailed_description": "The function implements enhanced PPO policy loss computation, including standard PPO clipped loss calculation, dual-clip PPO, and truncated importance sampling (optional), ultimately calling related functions to implement loss aggregation.\nStandard PPO clipping mechanism supports asymmetric clipping, requiring computation of unclipped original policy gradient loss and asymmetrically clipped loss, taking the maximum of both as pessimistic update strategy. When advantage value is negative, apply dual clipping to more conservatively reduce the probability of that action. If truncated importance sampling is enabled and rollout policy log probabilities are provided, the function computes the importance ratio of old policy relative to rollout policy, clips it to the configured upper limit, then multiplies it into the policy gradient loss.", "output": "Returns four scalar tensors:\n- `pg_loss`: Policy gradient loss\n- `pg_clipfrac`: Clipping fraction (proportion of clipped tokens, for monitoring training dynamics)\n- `ppo_kl`: Approximate KL divergence\n- `pg_clipfrac_lower`: Lower bound clipping fraction (proportion of tokens where dual clipping takes effect)", "func_implementation": "", "test_code_path": "code/test_code/test_compute_policy_loss_vanilla.py", "implementation_location": "code/verl/trainer/ppo/core_algos.py:line 816-902", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_policy_loss_vanilla`.\n\n**Function Overview:**\nComputes PPO policy gradient loss, supporting decoupled clipping (DAPO's core innovation). Through `clip_ratio_low` and `clip_ratio_high` parameters, allows asymmetric clipping for positive and negative advantages, implementing the \"Clip-Higher\" mechanism.\n\n**Function Signature:**\n```python\ndef compute_policy_loss_vanilla(\nold_log_prob: torch.Tensor,\nlog_prob: torch.Tensor,\nadvantages: torch.Tensor,\nresponse_mask: torch.Tensor,\nloss_agg_mode: str = \"token-mean\",\nconfig: Optional[DictConfig | AlgoConfig] = None,\nrollout_log_probs: torch.Tensor | None = None,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n```\n\n**Input Parameters:**\n- `old_log_prob`: Old policy log probabilities with shape `(batch_size, response_length)`\n- `log_prob`: Current policy log probabilities with shape `(batch_size, response_length)`\n- `advantages`: Advantage estimates with shape `(batch_size, response_length)`\n- `response_mask`: Response mask with shape `(batch_size, response_length)`\n- `loss_agg_mode`: Loss aggregation mode (default is token-mean)\n- `config`: Actor configuration object, containing:\n- `clip_ratio`: Default clipping parameter (backward compatibility)\n- `clip_ratio_low`: Lower bound clipping parameter (**DAPO new**)\n- `clip_ratio_high`: Upper bound clipping parameter (**DAPO new**)\n- `clip_ratio_c`: Dual clipping lower bound (handles extreme negative advantages, default 3.0)\n- `rollout_log_probs`: Optional, Rollout policy log probabilities (for off-policy training)\n\n**Detailed Description:**\nThe function implements enhanced PPO policy loss computation, including standard PPO clipped loss calculation, dual-clip PPO, and truncated importance sampling (optional), ultimately calling related functions to implement loss aggregation.\nStandard PPO clipping mechanism supports asymmetric clipping, requiring computation of unclipped original policy gradient loss and asymmetrically clipped loss, taking the maximum of both as pessimistic update strategy. When advantage value is negative, apply dual clipping to more conservatively reduce the probability of that action. If truncated importance sampling is enabled and rollout policy log probabilities are provided, the function computes the importance ratio of old policy relative to rollout policy, clips it to the configured upper limit, then multiplies it into the policy gradient loss.\n\n**Expected Output:**\nReturns four scalar tensors:\n- `pg_loss`: Policy gradient loss\n- `pg_clipfrac`: Clipping fraction (proportion of clipped tokens, for monitoring training dynamics)\n- `ppo_kl`: Approximate KL divergence\n- `pg_clipfrac_lower`: Lower bound clipping fraction (proportion of tokens where dual clipping takes effect)\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport torch\nfrom typing import Optional\nfrom omegaconf import DictConfig\n\ndef compute_policy_loss_vanilla(\n    old_log_prob: torch.Tensor,\n    log_prob: torch.Tensor,\n    advantages: torch.Tensor,\n    response_mask: torch.Tensor,\n    loss_agg_mode: str = \"token-mean\",\n    config: Optional[DictConfig] = None,\n    rollout_log_probs: torch.Tensor | None = None,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Computes PPO policy gradient loss with support for decoupled clipping (DAPO) and dual-clipping.\n    \"\"\"\n    # 1. Extract clipping parameters from config\n    # Support for DAPO (Decoupled Advantage Policy Optimization)\n    clip_ratio = config.get('clip_ratio', 0.2)\n    clip_ratio_low = config.get('clip_ratio_low', clip_ratio)\n    clip_ratio_high = config.get('clip_ratio_high', clip_ratio)\n    clip_ratio_c = config.get('clip_ratio_c', 3.0) # Dual-clip parameter\n    \n    # 2. Calculate importance sampling ratio\n    # ratio = exp(curr_log_prob - old_log_prob)\n    ratio = torch.exp(log_prob - old_log_prob)\n    \n    # 3. Truncated Importance Sampling (if rollout_log_probs provided)\n    # Used for off-policy correction when the 'old' policy is different from the 'rollout' policy\n    if rollout_log_probs is not None:\n        # ratio_old_rollout = pi_old / pi_rollout\n        ratio_old_rollout = torch.exp(old_log_prob - rollout_log_probs)\n        # Truncate the ratio to prevent high variance\n        ratio_old_rollout = torch.clamp(ratio_old_rollout, max=config.get('truncate_ratio_upper', 10.0))\n    else:\n        ratio_old_rollout = 1.0\n\n    # 4. Standard PPO Clipped Loss with Asymmetric Clipping (DAPO)\n    # pg_loss1: Unclipped loss\n    pg_loss1 = -advantages * ratio\n    \n    # pg_loss2: Clipped loss\n    # If advantage > 0: clip ratio to [1, 1 + clip_ratio_high]\n    # If advantage < 0: clip ratio to [1 - clip_ratio_low, 1]\n    clip_range_low = 1.0 - clip_ratio_low\n    clip_range_high = 1.0 + clip_ratio_high\n    \n    ratio_clipped = torch.where(\n        advantages > 0,\n        torch.clamp(ratio, max=clip_range_high),\n        torch.clamp(ratio, min=clip_range_low)\n    )\n    pg_loss2 = -advantages * ratio_clipped\n    \n    # Pessimistic update: max of unclipped and clipped loss\n    pg_loss = torch.max(pg_loss1, pg_loss2)\n    \n    # 5. Dual-clip PPO\n    # When advantage is very negative, we further clip the loss to prevent too large an update\n    if clip_ratio_c > 0:\n        # pg_loss_c is the lower bound for the loss when advantage < 0\n        pg_loss_c = -clip_ratio_c * advantages\n        # If advantage < 0, we take the max of (standard_ppo_loss, dual_clip_loss)\n        # Since pg_loss is already negative (loss = -adv * ratio), \n        # this effectively bounds how much we can decrease the probability.\n        pg_loss = torch.where(\n            advantages < 0,\n            torch.max(pg_loss, pg_loss_c),\n            pg_loss\n        )\n\n    # Apply off-policy correction if applicable\n    pg_loss = pg_loss * ratio_old_rollout\n\n    # 6. Calculate Monitoring Metrics\n    with torch.no_grad():\n        # pg_clipfrac: proportion of tokens where the ratio was clipped by standard PPO\n        is_clipped_upper = (advantages > 0) & (ratio > clip_range_high)\n        is_clipped_lower = (advantages < 0) & (ratio < clip_range_low)\n        pg_clipfrac = (is_clipped_upper | is_clipped_lower).float()\n        \n        # pg_clipfrac_lower: proportion of tokens where dual clipping took effect\n        # This happens when advantage < 0 and the ratio is so small that pg_loss < pg_loss_c\n        pg_clipfrac_lower = ((advantages < 0) & (pg_loss1 < pg_loss_c)).float()\n        \n        # Approximate KL Divergence: (exp(log_ratio) - 1) - log_ratio\n        log_ratio = log_prob - old_log_prob\n        ppo_kl = (torch.exp(log_ratio) - 1) - log_ratio\n\n    # 7. Aggregation\n    def aggregate_loss(tensor, mask, mode):\n        masked_tensor = tensor * mask\n        if mode == \"token-mean\":\n            return masked_tensor.sum() / mask.sum().clamp(min=1.0)\n        elif mode == \"batch-mean\":\n            return masked_tensor.sum() / mask.shape[0]\n        else:\n            return masked_tensor.sum()\n\n    pg_loss_agg = aggregate_loss(pg_loss, response_mask, loss_agg_mode)\n    pg_clipfrac_agg = aggregate_loss(pg_clipfrac, response_mask, loss_agg_mode)\n    ppo_kl_agg = aggregate_loss(ppo_kl, response_mask, loss_agg_mode)\n    pg_clipfrac_lower_agg = aggregate_loss(pg_clipfrac_lower, response_mask, loss_agg_mode)\n\n    return pg_loss_agg, pg_clipfrac_agg, ppo_kl_agg, pg_clipfrac_lower_agg\n```"], "usage": {"prompt_tokens": 828, "completion_tokens": 1445, "total_tokens": 2273, "per_completion": [{"prompt_tokens": 828, "completion_tokens": 1445, "total_tokens": 2273}]}, "results": [false], "pass_ratios": [0.0]}
{"function_name": "DAPORewardManager.__call__", "overview": "DAPO's reward manager, responsible for computing task rewards and applying overlong penalty. This is one of the key components distinguishing DAPO from other methods, preventing the model from generating meaningless lengthy outputs through a linear penalty mechanism.", "function_signature": "def __call__(self, data: DataProto, return_dict: bool = False):", "input_parameters": "- `data`: DataProto object containing:\n- `batch[\"prompts\"]`: Prompt token IDs with shape `(bs, prompt_len)`\n- `batch[\"responses\"]`: Response token IDs with shape `(bs, response_len)`\n- `batch[\"attention_mask\"]`: Attention mask with shape `(bs, total_len)`\n- `non_tensor_batch[\"reward_model\"][\"ground_truth\"]`: Ground truth answers\n- `non_tensor_batch[\"data_source\"]`: Data source (e.g., \"gsm8k\", \"math\")\n- `return_dict`: Boolean, whether to return dictionary format (including additional information like accuracy, overlong penalty, etc.)", "detailed_description": "The function is responsible for computing reward scores for generated responses. First checks if there are pre-computed rewards; if so, returns directly; otherwise processes each response individually, decoding token sequences into text and computing task rewards based on ground truth answers (needs to call related functional functions). Computed scores are placed at the last valid token position in the response sequence. If length control is enabled, the function applies additional negative reward penalty for overlong responses (linear penalty mechanism). Throughout the process, detailed information for some samples is output for debugging verification, ultimately returning complete reward tensor or dictionary containing additional evaluation metrics.", "output": "- If `return_dict=False`: Returns sparse reward tensor with shape `(batch_size, response_length)` (only last valid token position is non-zero)\n- If `return_dict=True`: Returns dictionary containing:\n- `\"reward_tensor\"`: Reward tensor as above\n- `\"reward_extra_info\"`: Dictionary containing `\"acc\"` (accuracy list), `\"overlong_reward\"` (overlong penalty list), `\"overlong\"` (boolean list indicating if overlong), etc.", "func_implementation": "", "test_code_path": "code/test_code/test_dapo_reward_manager.py", "implementation_location": "code/verl/workers/reward_manager/dapo.py:line 53-149", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `DAPORewardManager.__call__`.\n\n**Function Overview:**\nDAPO's reward manager, responsible for computing task rewards and applying overlong penalty. This is one of the key components distinguishing DAPO from other methods, preventing the model from generating meaningless lengthy outputs through a linear penalty mechanism.\n\n**Function Signature:**\n```python\ndef __call__(self, data: DataProto, return_dict: bool = False):\n```\n\n**Input Parameters:**\n- `data`: DataProto object containing:\n- `batch[\"prompts\"]`: Prompt token IDs with shape `(bs, prompt_len)`\n- `batch[\"responses\"]`: Response token IDs with shape `(bs, response_len)`\n- `batch[\"attention_mask\"]`: Attention mask with shape `(bs, total_len)`\n- `non_tensor_batch[\"reward_model\"][\"ground_truth\"]`: Ground truth answers\n- `non_tensor_batch[\"data_source\"]`: Data source (e.g., \"gsm8k\", \"math\")\n- `return_dict`: Boolean, whether to return dictionary format (including additional information like accuracy, overlong penalty, etc.)\n\n**Detailed Description:**\nThe function is responsible for computing reward scores for generated responses. First checks if there are pre-computed rewards; if so, returns directly; otherwise processes each response individually, decoding token sequences into text and computing task rewards based on ground truth answers (needs to call related functional functions). Computed scores are placed at the last valid token position in the response sequence. If length control is enabled, the function applies additional negative reward penalty for overlong responses (linear penalty mechanism). Throughout the process, detailed information for some samples is output for debugging verification, ultimately returning complete reward tensor or dictionary containing additional evaluation metrics.\n\n**Expected Output:**\n- If `return_dict=False`: Returns sparse reward tensor with shape `(batch_size, response_length)` (only last valid token position is non-zero)\n- If `return_dict=True`: Returns dictionary containing:\n- `\"reward_tensor\"`: Reward tensor as above\n- `\"reward_extra_info\"`: Dictionary containing `\"acc\"` (accuracy list), `\"overlong_reward\"` (overlong penalty list), `\"overlong\"` (boolean list indicating if overlong), etc.\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport torch\nfrom verl import DataProto\nfrom verl.utils.reward_score import gsm8k, math, multiply, codecontests, algorithmic\n\nclass DAPORewardManager:\n    \"\"\"\n    DAPO's reward manager responsible for computing task rewards and applying \n    overlong penalties to prevent length-hacking.\n    \"\"\"\n\n    def __init__(self, tokenizer, num_examine=1, compute_score=None, length_limit=None, length_penalty=None):\n        self.tokenizer = tokenizer\n        self.num_examine = num_examine\n        self.compute_score = compute_score\n        self.length_limit = length_limit\n        self.length_penalty = length_penalty\n\n    def __call__(self, data: DataProto, return_dict: bool = False):\n        \"\"\"\n        Computes rewards for the generated responses.\n        \"\"\"\n        # 1. Check if rewards are already computed\n        if 'rm_scores' in data.batch.keys():\n            return data.batch['rm_scores']\n\n        prompts = data.batch['prompts']\n        responses = data.batch['responses']\n        # attention_mask includes both prompt and response\n        attention_mask = data.batch['attention_mask']\n        \n        # Extract non-tensor metadata\n        ground_truths = data.non_tensor_batch['reward_model']['ground_truth']\n        data_sources = data.non_tensor_batch['data_source']\n        \n        batch_size = responses.shape[0]\n        response_length = responses.shape[1]\n        \n        # Initialize reward tensor\n        reward_tensor = torch.zeros((batch_size, response_length), dtype=torch.float32, device=responses.device)\n        \n        # Metrics for return_dict\n        accuracy_list = []\n        overlong_reward_list = []\n        is_overlong_list = []\n\n        # 2. Process each response\n        for i in range(batch_size):\n            # Decode response text\n            # We only decode the response part. Note: responses are usually padded.\n            # We find the actual length using the attention mask\n            curr_response_ids = responses[i]\n            curr_prompt_ids = prompts[i]\n            \n            # Calculate actual response length (excluding padding)\n            # In verl, responses are typically appended to prompts. \n            # The response mask is usually where attention_mask is 1 but after prompt_len.\n            # However, DataProto usually provides response-specific masks or we can infer from EOS.\n            # Here we assume responses might have padding tokens (0 or tokenizer.pad_token_id)\n            valid_response_mask = curr_response_ids != self.tokenizer.pad_token_id\n            actual_response_len = valid_response_mask.sum().item()\n            \n            response_str = self.tokenizer.decode(curr_response_ids[:actual_response_len], skip_special_tokens=True)\n            ground_truth = ground_truths[i]\n            data_source = data_sources[i]\n\n            # 3. Compute Task Reward (Accuracy)\n            # Use the provided compute_score or fallback to default dispatch\n            if self.compute_score is not None:\n                score = self.compute_score(data_source, response_str, ground_truth)\n            else:\n                # Default dispatch logic based on data source\n                if data_source in ['gsm8k']:\n                    score = gsm8k.compute_score(response_str, ground_truth)\n                elif data_source in ['math']:\n                    score = math.compute_score(response_str, ground_truth)\n                else:\n                    # Default to 0 or generic evaluation\n                    score = 0.0\n            \n            accuracy_list.append(score)\n\n            # 4. Apply Overlong Penalty (Linear Penalty Mechanism)\n            overlong_penalty = 0.0\n            is_overlong = False\n            if self.length_limit is not None and actual_response_len > self.length_limit:\n                is_overlong = True\n                # Linear penalty: (current_len - limit) * penalty_coefficient\n                overlong_penalty = (actual_response_len - self.length_limit) * self.length_penalty\n            \n            overlong_reward_list.append(-overlong_penalty)\n            is_overlong_list.append(is_overlong)\n\n            # Total reward for this sample\n            total_reward = score - overlong_penalty\n\n            # 5. Place reward at the last valid token position\n            if actual_response_len > 0:\n                # last_idx is actual_response_len - 1\n                reward_tensor[i, actual_response_len - 1] = total_reward\n            \n            # Debugging: Print first few samples\n            if i < self.num_examine:\n                print(f\"--- Sample {i} ---\")\n                print(f\"Prompt: {self.tokenizer.decode(curr_prompt_ids, skip_special_tokens=True)[:100]}...\")\n                print(f\"Response: {response_str}\")\n                print(f\"GT: {ground_truth}\")\n                print(f\"Score: {score}, Penalty: {overlong_penalty}, Total: {total_reward}\")\n\n        if return_dict:\n            return {\n                \"reward_tensor\": reward_tensor,\n                \"reward_extra_info\": {\n                    \"acc\": accuracy_list,\n                    \"overlong_reward\": overlong_reward_list,\n                    \"overlong\": is_overlong_list,\n                }\n            }\n        \n        return reward_tensor\n```"], "usage": {"prompt_tokens": 677, "completion_tokens": 1305, "total_tokens": 1982, "per_completion": [{"prompt_tokens": 677, "completion_tokens": 1305, "total_tokens": 1982}]}, "results": [false], "pass_ratios": [0.0]}
