{"function_name": "compute_score", "overview": "Computes ARES hierarchical entropy rewards, dynamically adjusting reward signals based on sample difficulty, accuracy, and High Window Entropy (HWE) token count.", "function_signature": "def compute_score(reward_inputs: List[Dict[str, Any]], format_weight: float = 0.1, alpha_length_bonus: float = 0.5, alpha_entropy: float = 0.5) -> List[Dict[str, float]]:", "input_parameters": "- `reward_inputs`: List of reward input dictionaries, each containing:\n- `response`: Model-generated response text\n- `ground_truth`: Ground truth answer\n- `difficulty`: Sample difficulty label (\"easy\" / \"medium\" / \"hard\")\n- `high_entropy_token_num`: Number of HWE tokens in response\n- `target_high_entropy_token_num`: Target HWE token count (difficulty-related)\n- `alpha_entropy`: Entropy reward coefficient (difficulty-related)\n- `accuracy`: Accuracy label (optional, if not provided, extracted from response and evaluated)\n- `format_weight`: Format reward weight, default 0.1\n- `alpha_length_bonus`: Length reward coefficient (legacy parameter, actual usage is alpha_entropy)\n- `alpha_entropy`: Default entropy reward coefficient, default 0.5 (used if sample doesn't specify)", "detailed_description": "This function implements ARES's core reward shaping mechanism, adaptively adjusting reasoning resource allocation based on task difficulty and model performance. The function computes format reward, accuracy reward, and entropy reward. For easy questions, when the answer is correct, it penalizes overly long reasoning chains, while when the answer is incorrect, it provides moderate positive rewards to encourage the model to explore more possibilities. Medium-difficulty questions require reasoning length closer to the target value; both excessive and insufficient reasoning are penalized when the answer is correct, but longer reasoning is still encouraged when the answer is incorrect. Hard questions adopt the opposite strategy: when the answer is correct, reasoning chains close to or exceeding the target are rewarded, and penalties are only applied when reasoning is significantly insufficient; when the answer is incorrect, longer reasoning is also encouraged to find the correct path.\nThe entire reward mechanism uses Huber loss function to smoothly handle penalty terms, Sigmoid function to achieve reward saturation effects, and sets different tolerance boundaries and reward caps for different difficulties.\nDifficulty-related hyperparameters that may be used:\n- `MARGIN_FRAC`: Tolerance band width (easy: 15%, medium: 25%, hard: 35%)\n- `KAPPA`: Huber function inflection point (easy: 2.0, medium: 3.0, hard: 4.0)\n- `TEMP`: Sigmoid temperature parameter (easy: 2.0, medium: 2.5, hard: 3.0)\n- `CAP_SCALE`: Entropy reward cap coefficient (easy: 1.0, medium: 1.0, hard: 1.2)", "output": "- `scores`: List of score dictionaries, each containing:\n- `overall`: Total score = accuracy + entropy_score\n- `accuracy`: Accuracy score (0.0 or 1.0)\n- `format`: Format score (0.0 or 1.0)\n- `high_entropy_token_num_score`: Entropy reward score", "func_implementation": "", "test_code_path": "code/test_code/test_compute_score.py", "implementation_location": "code/examples/reward_function/aepo.py:line 58-133", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_score`.\n\n**Function Overview:**\nComputes ARES hierarchical entropy rewards, dynamically adjusting reward signals based on sample difficulty, accuracy, and High Window Entropy (HWE) token count.\n\n**Function Signature:**\n```python\ndef compute_score(reward_inputs: List[Dict[str, Any]], format_weight: float = 0.1, alpha_length_bonus: float = 0.5, alpha_entropy: float = 0.5) -> List[Dict[str, float]]:\n```\n\n**Input Parameters:**\n- `reward_inputs`: List of reward input dictionaries, each containing:\n- `response`: Model-generated response text\n- `ground_truth`: Ground truth answer\n- `difficulty`: Sample difficulty label (\"easy\" / \"medium\" / \"hard\")\n- `high_entropy_token_num`: Number of HWE tokens in response\n- `target_high_entropy_token_num`: Target HWE token count (difficulty-related)\n- `alpha_entropy`: Entropy reward coefficient (difficulty-related)\n- `accuracy`: Accuracy label (optional, if not provided, extracted from response and evaluated)\n- `format_weight`: Format reward weight, default 0.1\n- `alpha_length_bonus`: Length reward coefficient (legacy parameter, actual usage is alpha_entropy)\n- `alpha_entropy`: Default entropy reward coefficient, default 0.5 (used if sample doesn't specify)\n\n**Detailed Description:**\nThis function implements ARES's core reward shaping mechanism, adaptively adjusting reasoning resource allocation based on task difficulty and model performance. The function computes format reward, accuracy reward, and entropy reward. For easy questions, when the answer is correct, it penalizes overly long reasoning chains, while when the answer is incorrect, it provides moderate positive rewards to encourage the model to explore more possibilities. Medium-difficulty questions require reasoning length closer to the target value; both excessive and insufficient reasoning are penalized when the answer is correct, but longer reasoning is still encouraged when the answer is incorrect. Hard questions adopt the opposite strategy: when the answer is correct, reasoning chains close to or exceeding the target are rewarded, and penalties are only applied when reasoning is significantly insufficient; when the answer is incorrect, longer reasoning is also encouraged to find the correct path.\nThe entire reward mechanism uses Huber loss function to smoothly handle penalty terms, Sigmoid function to achieve reward saturation effects, and sets different tolerance boundaries and reward caps for different difficulties.\nDifficulty-related hyperparameters that may be used:\n- `MARGIN_FRAC`: Tolerance band width (easy: 15%, medium: 25%, hard: 35%)\n- `KAPPA`: Huber function inflection point (easy: 2.0, medium: 3.0, hard: 4.0)\n- `TEMP`: Sigmoid temperature parameter (easy: 2.0, medium: 2.5, hard: 3.0)\n- `CAP_SCALE`: Entropy reward cap coefficient (easy: 1.0, medium: 1.0, hard: 1.2)\n\n**Expected Output:**\n- `scores`: List of score dictionaries, each containing:\n- `overall`: Total score = accuracy + entropy_score\n- `accuracy`: Accuracy score (0.0 or 1.0)\n- `format`: Format score (0.0 or 1.0)\n- `high_entropy_token_num_score`: Entropy reward score\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport math\nimport re\nfrom typing import List, Dict, Any\n\ndef compute_score(\n    reward_inputs: List[Dict[str, Any]], \n    format_weight: float = 0.1, \n    alpha_length_bonus: float = 0.5, \n    alpha_entropy: float = 0.5\n) -> List[Dict[str, float]]:\n    \"\"\"\n    Computes ARES hierarchical entropy rewards, dynamically adjusting reward signals \n    based on sample difficulty, accuracy, and High Window Entropy (HWE) token count.\n    \"\"\"\n    \n    def extract_answer(text: str) -> str:\n        \"\"\"Extracts the answer from the model response (looking for \\boxed{...}).\"\"\"\n        pattern = r\"\\\\boxed\\{(.*?)\\}\"\n        match = re.search(pattern, text)\n        if match:\n            return match.group(1).strip()\n        return \"\"\n\n    def huber_loss(x: float, kappa: float) -> float:\n        \"\"\"Smooth penalty function.\"\"\"\n        abs_x = abs(x)\n        if abs_x <= kappa:\n            return 0.5 * (x ** 2)\n        else:\n            return kappa * (abs_x - 0.5 * kappa)\n\n    def sigmoid(x: float, temp: float) -> float:\n        \"\"\"Sigmoid function for reward saturation.\"\"\"\n        return 1 / (1 + math.exp(-x / temp))\n\n    # Difficulty-specific hyperparameters\n    difficulty_configs = {\n        \"easy\": {\"margin_frac\": 0.15, \"kappa\": 2.0, \"temp\": 2.0, \"cap_scale\": 1.0},\n        \"medium\": {\"margin_frac\": 0.25, \"kappa\": 3.0, \"temp\": 2.5, \"cap_scale\": 1.0},\n        \"hard\": {\"margin_frac\": 0.35, \"kappa\": 4.0, \"temp\": 3.0, \"cap_scale\": 1.2}\n    }\n\n    scores = []\n\n    for item in reward_inputs:\n        response = item.get(\"response\", \"\")\n        ground_truth = str(item.get(\"ground_truth\", \"\"))\n        difficulty = item.get(\"difficulty\", \"medium\").lower()\n        hwe_count = item.get(\"high_entropy_token_num\", 0)\n        target_hwe = item.get(\"target_high_entropy_token_num\", 10)\n        \n        # Use sample-specific alpha_entropy if provided, else use default\n        current_alpha = item.get(\"alpha_entropy\", alpha_entropy)\n        \n        # 1. Format Reward\n        # Check if \\boxed{} exists and contains content\n        extracted = extract_answer(response)\n        format_score = 1.0 if extracted else 0.0\n        \n        # 2. Accuracy Reward\n        # If accuracy is pre-provided, use it; otherwise evaluate\n        if \"accuracy\" in item:\n            acc_score = float(item[\"accuracy\"])\n        else:\n            acc_score = 1.0 if extracted == ground_truth else 0.0\n\n        # 3. Entropy Reward (ARES Logic)\n        config = difficulty_configs.get(difficulty, difficulty_configs[\"medium\"])\n        margin = target_hwe * config[\"margin_frac\"]\n        \n        entropy_reward = 0.0\n        \n        if acc_score > 0.5:  # Correct Answer\n            if difficulty == \"easy\":\n                # Penalize long reasoning for easy questions\n                penalty = huber_loss(max(0, hwe_count - margin), config[\"kappa\"])\n                entropy_reward = -current_alpha * sigmoid(penalty, config[\"temp\"])\n            \n            elif difficulty == \"medium\":\n                # Penalize both excessive and insufficient reasoning\n                diff = abs(hwe_count - target_hwe)\n                penalty = huber_loss(max(0, diff - margin), config[\"kappa\"])\n                entropy_reward = -current_alpha * sigmoid(penalty, config[\"temp\"])\n                \n            elif difficulty == \"hard\":\n                # Reward reasoning close to or exceeding target, penalize only if significantly low\n                if hwe_count < (target_hwe - margin):\n                    penalty = huber_loss(target_hwe - margin - hwe_count, config[\"kappa\"])\n                    entropy_reward = -current_alpha * sigmoid(penalty, config[\"temp\"])\n                else:\n                    # Small bonus for sufficient reasoning on hard tasks\n                    bonus = min(hwe_count / target_hwe, config[\"cap_scale\"])\n                    entropy_reward = current_alpha * 0.5 * bonus\n\n        else:  # Incorrect Answer\n            # Encourage exploration (longer reasoning) across all difficulties when wrong\n            # The reward saturates as it approaches/exceeds target\n            exploration_gain = min(hwe_count / (target_hwe + 1e-6), config[\"cap_scale\"])\n            entropy_reward = current_alpha * exploration_gain\n\n        # Calculate final scores\n        # Total = Accuracy + Entropy (Format is usually part of accuracy or a separate signal)\n        # Here we follow the prompt: overall = accuracy + entropy_score\n        total_score = acc_score + entropy_reward\n        \n        scores.append({\n            \"overall\": total_score,\n            \"accuracy\": acc_score,\n            \"format\": format_score,\n            \"high_entropy_token_num_score\": entropy_reward\n        })\n\n    return scores\n```"], "usage": {"prompt_tokens": 931, "completion_tokens": 1287, "total_tokens": 2218, "per_completion": [{"prompt_tokens": 931, "completion_tokens": 1287, "total_tokens": 2218}]}, "results": [false], "pass_ratios": [0.0]}
{"function_name": "RayPPOTrainer.update_difficulty_and_skip_gid_set", "overview": "Updates sample difficulty dictionary and adaptive entropy coefficients based on current batch rollout results, implementing online difficulty bucketing and resource budget adjustment.", "function_signature": "def update_difficulty_and_skip_gid_set(self, batch: DataProto) -> None:", "input_parameters": "- `batch`: DataProto object containing:\n- `global_id`: Sample global ID (for cross-batch tracking)\n- `accuracy`: Accuracy list (accuracy of each response)\n- `entropies`: Average entropy list (average token entropy of each response)\n- `high_entropy_token_num`: HWE token count list (HWE token count of each response)", "detailed_description": "This function is responsible for dynamically updating question difficulty classification and adaptively adjusting entropy penalty coefficients during training. The function needs to aggregate accuracy, entropy, and high-entropy token counts for each question by global identifier. Questions are categorized into easy, medium, and hard levels based on accuracy. Based on high-entropy token counts, the function needs to compute target reasoning length for each difficulty level and adjust entropy penalty coefficients based on deviation between actual generated length and target value. When adjusting entropy penalty coefficients, gradient ascent can be applied. For easy and medium questions, if reasoning is too long, increase penalty strength; for hard questions, increase encouragement when reasoning is insufficient.\nDifficulty assignment based on accuracy thresholds:\n- `acc_rate ≥ 2/3` → \"easy\"\n- `1/3 ≤ acc_rate < 2/3` → \"medium\"\n- `acc_rate < 1/3` → \"hard\"\n(Optional) The function adds samples that are \"all correct with extremely low average entropy\" to `self.skip_gid_set`, and subsequent training will skip these overly simple samples, focusing on challenging data.", "output": "- No direct return value, but updates the following class attributes:\n- `self.difficulty_dict_train`: Sample difficulty dictionary `{global_id: difficulty}`\n- `self.target_high_entropy_token_num_dict`: Target HWE token count `{difficulty: target_num}`\n- `self.alpha_entropy_dict`: Entropy coefficient dictionary `{difficulty: alpha}`\n- `self.skip_gid_set`: Skip sample set (optional)", "func_implementation": "", "test_code_path": "code/test_code/test_update_difficulty.py", "implementation_location": "code/verl/trainer/ray_trainer.py:line 775-825", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `RayPPOTrainer.update_difficulty_and_skip_gid_set`.\n\n**Function Overview:**\nUpdates sample difficulty dictionary and adaptive entropy coefficients based on current batch rollout results, implementing online difficulty bucketing and resource budget adjustment.\n\n**Function Signature:**\n```python\ndef update_difficulty_and_skip_gid_set(self, batch: DataProto) -> None:\n```\n\n**Input Parameters:**\n- `batch`: DataProto object containing:\n- `global_id`: Sample global ID (for cross-batch tracking)\n- `accuracy`: Accuracy list (accuracy of each response)\n- `entropies`: Average entropy list (average token entropy of each response)\n- `high_entropy_token_num`: HWE token count list (HWE token count of each response)\n\n**Detailed Description:**\nThis function is responsible for dynamically updating question difficulty classification and adaptively adjusting entropy penalty coefficients during training. The function needs to aggregate accuracy, entropy, and high-entropy token counts for each question by global identifier. Questions are categorized into easy, medium, and hard levels based on accuracy. Based on high-entropy token counts, the function needs to compute target reasoning length for each difficulty level and adjust entropy penalty coefficients based on deviation between actual generated length and target value. When adjusting entropy penalty coefficients, gradient ascent can be applied. For easy and medium questions, if reasoning is too long, increase penalty strength; for hard questions, increase encouragement when reasoning is insufficient.\nDifficulty assignment based on accuracy thresholds:\n- `acc_rate ≥ 2/3` → \"easy\"\n- `1/3 ≤ acc_rate < 2/3` → \"medium\"\n- `acc_rate < 1/3` → \"hard\"\n(Optional) The function adds samples that are \"all correct with extremely low average entropy\" to `self.skip_gid_set`, and subsequent training will skip these overly simple samples, focusing on challenging data.\n\n**Expected Output:**\n- No direct return value, but updates the following class attributes:\n- `self.difficulty_dict_train`: Sample difficulty dictionary `{global_id: difficulty}`\n- `self.target_high_entropy_token_num_dict`: Target HWE token count `{difficulty: target_num}`\n- `self.alpha_entropy_dict`: Entropy coefficient dictionary `{difficulty: alpha}`\n- `self.skip_gid_set`: Skip sample set (optional)\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport torch\nimport numpy as np\nfrom verl import DataProto\n\ndef update_difficulty_and_skip_gid_set(self, batch: DataProto) -> None:\n    \"\"\"\n    Updates sample difficulty dictionary and adaptive entropy coefficients based on current batch rollout results.\n    Implements online difficulty bucketing, resource budget adjustment, and sample skipping.\n    \"\"\"\n    # 1. Extract data from DataProto\n    # global_id: [batch_size]\n    # accuracy: [batch_size, n_responses_per_query]\n    # entropies: [batch_size, n_responses_per_query]\n    # high_entropy_token_num: [batch_size, n_responses_per_query]\n    \n    global_ids = batch.non_tensor_batch.get('global_id')\n    accuracies = batch.batch['accuracy']\n    entropies = batch.batch['entropies']\n    hwe_token_nums = batch.batch['high_entropy_token_num']\n\n    # Aggregators for the current batch\n    batch_difficulty_stats = {\n        'easy': [],\n        'medium': [],\n        'hard': []\n    }\n\n    # 2. Process each sample in the batch\n    for i, gid in enumerate(global_ids):\n        sample_accs = accuracies[i]\n        sample_entropies = entropies[i]\n        sample_hwe_nums = hwe_token_nums[i]\n\n        # Calculate average accuracy for this question across all responses\n        acc_rate = torch.mean(sample_accs.float()).item()\n        avg_entropy = torch.mean(sample_entropies.float()).item()\n        avg_hwe_num = torch.mean(sample_hwe_nums.float()).item()\n\n        # 3. Difficulty Assignment\n        if acc_rate >= 2/3:\n            difficulty = \"easy\"\n        elif 1/3 <= acc_rate < 2/3:\n            difficulty = \"medium\"\n        else:\n            difficulty = \"hard\"\n\n        # Update the global difficulty dictionary\n        self.difficulty_dict_train[gid] = difficulty\n        batch_difficulty_stats[difficulty].append(avg_hwe_num)\n\n        # 4. Optional: Update skip_gid_set\n        # Logic: All correct (acc=1.0) and extremely low entropy (converged/too simple)\n        entropy_threshold = getattr(self.config, 'skip_entropy_threshold', 0.01)\n        if acc_rate == 1.0 and avg_entropy < entropy_threshold:\n            self.skip_gid_set.add(gid)\n\n    # 5. Update Target Reasoning Length (HWE token count) per difficulty\n    # Usually, we want easy/medium to be concise and hard to have enough reasoning steps\n    for diff in ['easy', 'medium', 'hard']:\n        if len(batch_difficulty_stats[diff]) > 0:\n            current_avg_hwe = np.mean(batch_difficulty_stats[diff])\n            # Moving average update for target (or fixed logic based on config)\n            # Here we update the target based on a momentum-based approach or observation\n            ema_alpha = 0.1\n            self.target_high_entropy_token_num_dict[diff] = (\n                (1 - ema_alpha) * self.target_high_entropy_token_num_dict.get(diff, current_avg_hwe) +\n                ema_alpha * current_avg_hwe\n            )\n\n    # 6. Adaptive Entropy Coefficient Adjustment (Gradient Ascent/Descent)\n    # alpha_entropy_dict: {difficulty: alpha}\n    # Logic: \n    # - For Easy/Medium: If actual HWE > target, increase alpha (penalty) to shorten reasoning.\n    # - For Hard: If actual HWE < target, decrease alpha (or make it more negative) to encourage exploration/length.\n    \n    learning_rate_alpha = getattr(self.config, 'alpha_lr', 1e-4)\n    \n    for diff in ['easy', 'medium', 'hard']:\n        if len(batch_difficulty_stats[diff]) > 0:\n            actual_hwe = np.mean(batch_difficulty_stats[diff])\n            target_hwe = self.target_high_entropy_token_num_dict[diff]\n            \n            # Deviation: positive means reasoning is longer than target\n            diff_val = actual_hwe - target_hwe\n            \n            if diff in ['easy', 'medium']:\n                # Increase penalty if too long\n                self.alpha_entropy_dict[diff] += learning_rate_alpha * diff_val\n            else: # hard\n                # Decrease penalty (encourage exploration) if too short\n                # Note: If alpha is a penalty (positive), we subtract to reduce penalty\n                self.alpha_entropy_dict[diff] -= learning_rate_alpha * (target_hwe - actual_hwe)\n\n            # Ensure alpha stays within reasonable bounds (e.g., non-negative for penalties)\n            self.alpha_entropy_dict[diff] = max(0.0, self.alpha_entropy_dict[diff])\n```"], "usage": {"prompt_tokens": 707, "completion_tokens": 1198, "total_tokens": 1905, "per_completion": [{"prompt_tokens": 707, "completion_tokens": 1198, "total_tokens": 1905}]}, "results": [false], "pass_ratios": [0.0]}
{"function_name": "TokenLevelAdaptiveKLController.update", "overview": "Implements update logic for token-level adaptive KL controller, dynamically adjusting KL coefficients using dual ascent mechanism based on deviation between actual KL divergence and target for each difficulty bucket.", "function_signature": "def update(self, current_kls: Dict[str, float], n_steps: Dict[str, int]):", "input_parameters": "- `current_kls`: Average KL divergence for each difficulty bucket at current step, dictionary format `{difficulty: kl_value}`\n- Example: `{\"easy\": 0.06, \"medium\": 0.12, \"hard\": 0.18}`\n- `n_steps`: Sample count for each difficulty bucket (used for weighting, may be simplified in actual implementation)", "detailed_description": "Token-level adaptive KL control allows using different KL budgets for different difficulty tasks, automatically adjusting KL divergence coefficients for each difficulty bucket through dual ascent mechanism. When actual KL divergence exceeds target value, increase penalty coefficient; when below target value, decrease coefficient.", "output": "- No direct return value, but updates class attributes:\n- `self.kl_coefs`: Updated KL coefficient dictionary `{difficulty: beta}`\n- `self.lambdas`: Updated Lagrange multiplier dictionary `{difficulty: lambda}`", "func_implementation": "", "test_code_path": "code/test_code/test_kl_controller.py", "implementation_location": "code/verl/trainer/core_algos.py:line 77-86", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `TokenLevelAdaptiveKLController.update`.\n\n**Function Overview:**\nImplements update logic for token-level adaptive KL controller, dynamically adjusting KL coefficients using dual ascent mechanism based on deviation between actual KL divergence and target for each difficulty bucket.\n\n**Function Signature:**\n```python\ndef update(self, current_kls: Dict[str, float], n_steps: Dict[str, int]):\n```\n\n**Input Parameters:**\n- `current_kls`: Average KL divergence for each difficulty bucket at current step, dictionary format `{difficulty: kl_value}`\n- Example: `{\"easy\": 0.06, \"medium\": 0.12, \"hard\": 0.18}`\n- `n_steps`: Sample count for each difficulty bucket (used for weighting, may be simplified in actual implementation)\n\n**Detailed Description:**\nToken-level adaptive KL control allows using different KL budgets for different difficulty tasks, automatically adjusting KL divergence coefficients for each difficulty bucket through dual ascent mechanism. When actual KL divergence exceeds target value, increase penalty coefficient; when below target value, decrease coefficient.\n\n**Expected Output:**\n- No direct return value, but updates class attributes:\n- `self.kl_coefs`: Updated KL coefficient dictionary `{difficulty: beta}`\n- `self.lambdas`: Updated Lagrange multiplier dictionary `{difficulty: lambda}`\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport numpy as np\nfrom typing import Dict\n\nclass TokenLevelAdaptiveKLController:\n    def __init__(self, kl_targets: Dict[str, float], initial_kl_coefs: Dict[str, float], lr: float = 0.1):\n        \"\"\"\n        Initialize the Token-Level Adaptive KL Controller.\n        \n        Args:\n            kl_targets: Target KL divergence for each difficulty bucket.\n            initial_kl_coefs: Initial KL penalty coefficients (beta) for each bucket.\n            lr: Learning rate for the dual ascent update.\n        \"\"\"\n        self.kl_targets = kl_targets\n        self.kl_coefs = initial_kl_coefs\n        self.lr = lr\n        # In dual ascent for KL control, the KL coefficient (beta) acts as the Lagrange multiplier\n        # We maintain self.lambdas to track the state for the update logic\n        self.lambdas = {k: float(v) for k, v in initial_kl_coefs.items()}\n\n    def update(self, current_kls: Dict[str, float], n_steps: Dict[str, int]):\n        \"\"\"\n        Implements update logic for token-level adaptive KL controller.\n        \n        Dynamically adjusts KL coefficients using a dual ascent mechanism based on the \n        deviation between actual KL divergence and the target for each difficulty bucket.\n        \n        Args:\n            current_kls: Average KL divergence for each difficulty bucket at current step.\n                         Format: {\"easy\": 0.06, \"medium\": 0.12, ...}\n            n_steps: Sample count for each difficulty bucket (can be used for weighted updates).\n        \"\"\"\n        for difficulty, kl_val in current_kls.items():\n            if difficulty not in self.kl_targets:\n                continue\n            \n            target_kl = self.kl_targets[difficulty]\n            \n            # Dual Ascent Logic:\n            # We want to solve: min E[Reward] s.t. KL <= Target\n            # Lagrangian: L = E[Reward] - lambda * (KL - Target)\n            # Update rule: lambda_{t+1} = max(0, lambda_t + lr * (KL - Target))\n            \n            # Calculate the error (deviation from target)\n            # If kl_val > target_kl, error is positive -> increase lambda (penalty)\n            # If kl_val < target_kl, error is negative -> decrease lambda (penalty)\n            error = kl_val - target_kl\n            \n            # Update the Lagrange multiplier (lambda)\n            # Note: Some implementations use exponential updates for stability: \n            # self.lambdas[difficulty] *= np.exp(self.lr * error)\n            # Here we implement the standard additive dual ascent step\n            new_lambda = self.lambdas[difficulty] + self.lr * error\n            \n            # Ensure the coefficient remains non-negative\n            self.lambdas[difficulty] = max(0.0, new_lambda)\n            \n            # Update the actual KL coefficient used in the reward function\n            # In many RL frameworks, kl_coef is synonymous with the multiplier lambda\n            self.kl_coefs[difficulty] = self.lambdas[difficulty]\n\n        # Note: n_steps is provided for potential weighting in distributed settings \n        # or to scale the learning rate, but standard dual ascent typically updates \n        # per-step based on the batch average KL.\n```"], "usage": {"prompt_tokens": 475, "completion_tokens": 776, "total_tokens": 1251, "per_completion": [{"prompt_tokens": 475, "completion_tokens": 776, "total_tokens": 1251}]}, "results": [false], "pass_ratios": [0.0]}
{"function_name": "apply_kl_penalty", "overview": "Applies KL divergence penalty to token-level rewards and updates KL controller.", "function_signature": "def apply_kl_penalty(data: DataProto, kl_ctrl: KLController, kl_penalty=\"kl\"):", "input_parameters": "- `data`: DataProto object containing:\n- `token_level_scores`: Original token-level rewards [batch_size, response_length]\n- `old_log_probs`: Actor old policy log probabilities [batch_size, response_length]\n- `ref_log_probs`: Ref policy log probabilities [batch_size, response_length]\n- `response_mask`: Response mask [batch_size, response_length]\n- `kl_ctrl`: KL controller (FixedKLController or AdaptiveKLController)\n- `kl_penalty`: KL penalty type (\"kl\", \"abs\", \"mse\", \"low_var_kl\", \"full\")", "detailed_description": "This function implements computation and application of token-level KL penalty. It computes token-level KL divergence based on `kl_penalty` type, then subtracts KL penalty term from original rewards to obtain token-level rewards, and computes sequence-level KL divergence and batch-level KL divergence.\nThe function also needs to implement KL controller update.", "output": "- `data`: Updated DataProto where `data.batch[\"token_level_rewards\"]` has KL penalty applied\n- `metrics`: Metrics dictionary `{\"critic/kl\": current_kl, \"critic/kl_coef\": kl_ctrl.kl_coef}`", "func_implementation": "", "test_code_path": "code/test_code/test_apply_kl_penalty.py", "implementation_location": "code/verl/trainer/ray_trainer.py:line 107-124", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `apply_kl_penalty`.\n\n**Function Overview:**\nApplies KL divergence penalty to token-level rewards and updates KL controller.\n\n**Function Signature:**\n```python\ndef apply_kl_penalty(data: DataProto, kl_ctrl: KLController, kl_penalty=\"kl\"):\n```\n\n**Input Parameters:**\n- `data`: DataProto object containing:\n- `token_level_scores`: Original token-level rewards [batch_size, response_length]\n- `old_log_probs`: Actor old policy log probabilities [batch_size, response_length]\n- `ref_log_probs`: Ref policy log probabilities [batch_size, response_length]\n- `response_mask`: Response mask [batch_size, response_length]\n- `kl_ctrl`: KL controller (FixedKLController or AdaptiveKLController)\n- `kl_penalty`: KL penalty type (\"kl\", \"abs\", \"mse\", \"low_var_kl\", \"full\")\n\n**Detailed Description:**\nThis function implements computation and application of token-level KL penalty. It computes token-level KL divergence based on `kl_penalty` type, then subtracts KL penalty term from original rewards to obtain token-level rewards, and computes sequence-level KL divergence and batch-level KL divergence.\nThe function also needs to implement KL controller update.\n\n**Expected Output:**\n- `data`: Updated DataProto where `data.batch[\"token_level_rewards\"]` has KL penalty applied\n- `metrics`: Metrics dictionary `{\"critic/kl\": current_kl, \"critic/kl_coef\": kl_ctrl.kl_coef}`\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport torch\nfrom verl import DataProto\n\ndef apply_kl_penalty(data: DataProto, kl_ctrl, kl_penalty=\"kl\"):\n    \"\"\"\n    Applies KL divergence penalty to token-level rewards and updates the KL controller.\n\n    Args:\n        data: DataProto object containing token_level_scores, old_log_probs, \n              ref_log_probs, and response_mask.\n        kl_ctrl: KL controller (FixedKLController or AdaptiveKLController).\n        kl_penalty: KL penalty type (\"kl\", \"abs\", \"mse\", \"low_var_kl\", \"full\").\n\n    Returns:\n        data: Updated DataProto with \"token_level_rewards\" in its batch.\n        metrics: Dictionary containing current KL and KL coefficient.\n    \"\"\"\n    # Extract tensors from DataProto\n    # token_level_scores: [batch_size, seq_len]\n    # old_log_probs: [batch_size, seq_len]\n    # ref_log_probs: [batch_size, seq_len]\n    # response_mask: [batch_size, seq_len]\n    token_level_scores = data.batch['token_level_scores']\n    old_log_probs = data.batch['old_log_probs']\n    ref_log_probs = data.batch['ref_log_probs']\n    mask = data.batch['response_mask'].to(torch.float32)\n\n    # Compute token-level KL divergence based on the specified penalty type\n    if kl_penalty == 'kl':\n        # Standard KL: log(p/q) = log(p) - log(q)\n        kl_divergence = old_log_probs - ref_log_probs\n    elif kl_penalty == 'abs':\n        kl_divergence = torch.abs(old_log_probs - ref_log_probs)\n    elif kl_penalty == 'mse':\n        kl_divergence = 0.5 * (old_log_probs - ref_log_probs)**2\n    elif kl_penalty == 'low_var_kl':\n        # Low variance estimator: 1/2 * (log p - log q)^2\n        # Often used to reduce variance in RLHF\n        kl_divergence = 0.5 * (old_log_probs - ref_log_probs)**2\n    elif kl_penalty == 'full':\n        # This usually implies the full KL including the constant term if applicable,\n        # but in log-prob space, it's typically the same as 'kl'\n        kl_divergence = old_log_probs - ref_log_probs\n    else:\n        raise NotImplementedError(f\"KL penalty type '{kl_penalty}' is not implemented.\")\n\n    # Apply KL coefficient\n    kl_penalty_term = kl_ctrl.value * kl_divergence\n\n    # Compute token-level rewards: reward = score - beta * KL\n    token_level_rewards = token_level_scores - kl_penalty_term\n    \n    # Store the result back into DataProto\n    data.batch['token_level_rewards'] = token_level_rewards\n\n    # Compute sequence-level and batch-level KL for monitoring and controller update\n    # We use the standard KL (old_log_probs - ref_log_probs) for the controller update\n    actual_kl = old_log_probs - ref_log_probs\n    \n    # Masked mean of KL divergence\n    masked_kl = actual_kl * mask\n    # Sum KL per sequence, then average over the batch\n    # Note: verl usually tracks the mean KL per token across the batch\n    batch_kl = torch.sum(masked_kl) / torch.sum(mask)\n    \n    # Update the KL controller (e.g., AdaptiveKLController adjusts beta based on target KL)\n    kl_ctrl.update(batch_kl.item(), n_steps=1)\n\n    metrics = {\n        \"critic/kl\": batch_kl.item(),\n        \"critic/kl_coef\": kl_ctrl.value\n    }\n\n    return data, metrics\n```"], "usage": {"prompt_tokens": 540, "completion_tokens": 928, "total_tokens": 1468, "per_completion": [{"prompt_tokens": 540, "completion_tokens": 928, "total_tokens": 1468}]}, "results": [false], "pass_ratios": [0.0]}
