{"function_name": "ProcessRewardModelWorker._forward_micro_batch", "overview": "Computes token-level scores from the Process Reward Model (PRM) and performs approximate weighting for min-form credit assignment based on configuration.", "function_signature": "def _forward_micro_batch(self, micro_batch):", "input_parameters": "- `micro_batch`: Micro-batch data containing `input_ids`, `attention_mask`, `position_ids`, `responses`, `reward_mask`, etc.\n- `response_length`: Response sequence length (inferred from batch tensors)\n- `temperature`: Temperature for approximate min-form (when `reward_model.credit_assignment` is a float)", "detailed_description": "Performs forward pass on the Process Reward Model (PRM), computing token-level scores via softmax to obtain the positive-negative class probability difference. Uses masking to retain only valid scores from the response portion.\nIf depadding or sequence parallelism optimization is enabled, padding must be removed before computation and restored afterward.\nCredit assignment weighting executes different strategies based on configuration mode: min-form, strict min-form, or gamma-decay. Lower-scoring tokens should receive higher weights.", "output": "- `rm_score`: Token-level process reward scores (shape `[batch_size, response_length]`)", "func_implementation": "", "test_code_path": "code/test_code/test_forward_micro_batch.py", "implementation_location": "code/verl/workers/fsdp_workers.py:line 1462-1539", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `ProcessRewardModelWorker._forward_micro_batch`.\n\n**Function Overview:**\nComputes token-level scores from the Process Reward Model (PRM) and performs approximate weighting for min-form credit assignment based on configuration.\n\n**Function Signature:**\n```python\ndef _forward_micro_batch(self, micro_batch):\n```\n\n**Input Parameters:**\n- `micro_batch`: Micro-batch data containing `input_ids`, `attention_mask`, `position_ids`, `responses`, `reward_mask`, etc.\n- `response_length`: Response sequence length (inferred from batch tensors)\n- `temperature`: Temperature for approximate min-form (when `reward_model.credit_assignment` is a float)\n\n**Detailed Description:**\nPerforms forward pass on the Process Reward Model (PRM), computing token-level scores via softmax to obtain the positive-negative class probability difference. Uses masking to retain only valid scores from the response portion.\nIf depadding or sequence parallelism optimization is enabled, padding must be removed before computation and restored afterward.\nCredit assignment weighting executes different strategies based on configuration mode: min-form, strict min-form, or gamma-decay. Lower-scoring tokens should receive higher weights.\n\n**Expected Output:**\n- `rm_score`: Token-level process reward scores (shape `[batch_size, response_length]`)\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}]}
{"function_name": "ProcessRewardModelWorker.compute_rm_score", "overview": "Batch-constructs PRM inputs (splitting by steps and appending newlines after each step), performs inference with (optional) variable-length sharding/backfilling, and aggregates to obtain batch-level token-level `rm_scores`.", "function_signature": "def compute_rm_score(self, data:DataProto):", "input_parameters": "- `data`: `DataProto` containing `responses`, `reward_mask`, etc.;", "detailed_description": "Splits response sequences by steps to construct input format for the process reward model.\nIn dynamic batch mode, reorganizes data into multiple mini-batches based on total token count limit and records original order; in fixed batch mode, evenly splits by sample count.\nCalls forward function for each mini-batch to compute token-level scores, concatenating results from all batches. In dynamic batch mode, restores results to original order based on recorded indices.", "output": "- `DataProto`: `batch['rm_scores']` with shape `[batch_size, response_length]`", "func_implementation": "", "test_code_path": "code/test_code/test_compute_rm_score.py", "implementation_location": "code/verl/workers/fsdp_workers.py:line 1541-1594", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `ProcessRewardModelWorker.compute_rm_score`.\n\n**Function Overview:**\nBatch-constructs PRM inputs (splitting by steps and appending newlines after each step), performs inference with (optional) variable-length sharding/backfilling, and aggregates to obtain batch-level token-level `rm_scores`.\n\n**Function Signature:**\n```python\ndef compute_rm_score(self, data:DataProto):\n```\n\n**Input Parameters:**\n- `data`: `DataProto` containing `responses`, `reward_mask`, etc.;\n\n**Detailed Description:**\nSplits response sequences by steps to construct input format for the process reward model.\nIn dynamic batch mode, reorganizes data into multiple mini-batches based on total token count limit and records original order; in fixed batch mode, evenly splits by sample count.\nCalls forward function for each mini-batch to compute token-level scores, concatenating results from all batches. In dynamic batch mode, restores results to original order based on recorded indices.\n\n**Expected Output:**\n- `DataProto`: `batch['rm_scores']` with shape `[batch_size, response_length]`\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}]}
{"function_name": "compute_return", "overview": "Computes token-level return sequences based on `method`", "function_signature": "def compute_return(token_level_rewards, eos_mask, method='sum', gamma=1.0):", "input_parameters": "- `token_level_rewards`: `[batch_size, response_length]`\n- `eos_mask`: `[batch_size, response_length]`\n- `method`: `'sum'|'min'`\n- `gamma`: Discount factor (applicable in `'sum'` mode)", "detailed_description": "Computes token-level return sequences based on method, using masking to ensure only valid positions are processed.\n\"method\" includes \"sum\" and \"min\".", "output": "- `returns`: `[batch_size, response_length]`", "func_implementation": "", "test_code_path": "code/test_code/test_compute_return.py", "implementation_location": "code/verl/trainer/ppo/core_algos.py:line 70-88", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_return`.\n\n**Function Overview:**\nComputes token-level return sequences based on `method`\n\n**Function Signature:**\n```python\ndef compute_return(token_level_rewards, eos_mask, method='sum', gamma=1.0):\n```\n\n**Input Parameters:**\n- `token_level_rewards`: `[batch_size, response_length]`\n- `eos_mask`: `[batch_size, response_length]`\n- `method`: `'sum'|'min'`\n- `gamma`: Discount factor (applicable in `'sum'` mode)\n\n**Detailed Description:**\nComputes token-level return sequences based on method, using masking to ensure only valid positions are processed.\n\"method\" includes \"sum\" and \"min\".\n\n**Expected Output:**\n- `returns`: `[batch_size, response_length]`\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}]}
