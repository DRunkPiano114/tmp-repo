{"function_name": "compute_grpo_outcome_advantage_split", "overview": "Computes group GRPO (Group Relative Policy Optimization) advantage estimation, with the key innovation of using only on-policy samples to compute within-group baseline.", "function_signature": "def compute_grpo_outcome_advantage_split(token_level_rewards: torch.Tensor,\neos_mask: torch.Tensor,\nindex: torch.Tensor,\non_policy_mask: torch.Tensor,\nepsilon: float = 1e-6,\nuse_std: bool = True):", "input_parameters": "- `token_level_rewards`: Token-level reward tensor with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `eos_mask`: Valid token mask with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `index`: Sample grouping indices, n responses from the same prompt share the same index. **Note: This parameter should be a Python list or indexable sequence (e.g., `[0, 0, 1, 1]`), not `torch.Tensor`, as index values will be used as dictionary keys**\n- `on_policy_mask`: Boolean mask with shape `[batch_size]`, type `torch.Tensor`, marking which samples are on-policy generated\n- `epsilon`: Numerical stability constant, default 1e-6\n- `use_std`: Whether to use standard deviation for normalization, default True", "detailed_description": "This function needs to sum token-level reward tensors along the time dimension for each response sequence to obtain sequence-level total scores.\nThe main implementation performs grouped normalization (including both on-policy and off-policy) for multiple responses from the same prompt, computing relative advantage values. During normalization, only samples marked as on-policy are selected to compute the group's mean and standard deviation. If a group has only one on-policy sample, set mean to 0 and standard deviation to 1; if there are multiple samples, compute statistics normally. For cases where standard deviation is 0, replace it with 1 to avoid division by zero errors.\nIf standard deviation normalization is enabled, compute (score - mean) / (std + epsilon); otherwise only subtract the mean. Finally, expand sequence-level advantage values to token-level advantage tensors.", "output": "- `advantages`: Token-level advantage tensor with shape `[batch_size, response_length]`\n- `returns`: Token-level return tensor with shape `[batch_size, response_length]`, same as advantages", "func_implementation": "", "test_code_path": "code/test_code/test_compute_grpo_outcome_advantage_split.py", "implementation_location": "code/luffy/verl/verl/mix_src/mix_core_alg.py:line 12-67", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_grpo_outcome_advantage_split`.\n\n**Function Overview:**\nComputes group GRPO (Group Relative Policy Optimization) advantage estimation, with the key innovation of using only on-policy samples to compute within-group baseline.\n\n**Function Signature:**\n```python\ndef compute_grpo_outcome_advantage_split(token_level_rewards: torch.Tensor,\neos_mask: torch.Tensor,\nindex: torch.Tensor,\non_policy_mask: torch.Tensor,\nepsilon: float = 1e-6,\nuse_std: bool = True):\n```\n\n**Input Parameters:**\n- `token_level_rewards`: Token-level reward tensor with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `eos_mask`: Valid token mask with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `index`: Sample grouping indices, n responses from the same prompt share the same index. **Note: This parameter should be a Python list or indexable sequence (e.g., `[0, 0, 1, 1]`), not `torch.Tensor`, as index values will be used as dictionary keys**\n- `on_policy_mask`: Boolean mask with shape `[batch_size]`, type `torch.Tensor`, marking which samples are on-policy generated\n- `epsilon`: Numerical stability constant, default 1e-6\n- `use_std`: Whether to use standard deviation for normalization, default True\n\n**Detailed Description:**\nThis function needs to sum token-level reward tensors along the time dimension for each response sequence to obtain sequence-level total scores.\nThe main implementation performs grouped normalization (including both on-policy and off-policy) for multiple responses from the same prompt, computing relative advantage values. During normalization, only samples marked as on-policy are selected to compute the group's mean and standard deviation. If a group has only one on-policy sample, set mean to 0 and standard deviation to 1; if there are multiple samples, compute statistics normally. For cases where standard deviation is 0, replace it with 1 to avoid division by zero errors.\nIf standard deviation normalization is enabled, compute (score - mean) / (std + epsilon); otherwise only subtract the mean. Finally, expand sequence-level advantage values to token-level advantage tensors.\n\n**Expected Output:**\n- `advantages`: Token-level advantage tensor with shape `[batch_size, response_length]`\n- `returns`: Token-level return tensor with shape `[batch_size, response_length]`, same as advantages\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def compute_grpo_outcome_advantage_split(token_level_rewards: torch.Tensor,\neos_mask: torch.Tensor,\nindex: torch.Tensor,\non_policy_mask: torch.Tensor,\nepsilon: float = 1e-6,\nuse_std: bool = True):\n    \"\"\"\n    Compute group GRPO (Group Relative Policy Optimization) advantages and returns\n    using only on-policy samples to compute the within-group baseline.\n\n    Args:\n        token_level_rewards: Tensor of shape [batch_size, response_length]\n        eos_mask: Valid token mask of shape [batch_size, response_length]\n        index: Sequence/grouping indices. Should be a Python list or indexable sequence\n               of length batch_size, e.g., [0, 0, 1, 1].\n        on_policy_mask: Boolean tensor of shape [batch_size] indicating on-policy samples\n        epsilon: Small constant for numerical stability in normalization\n        use_std: If True, normalize by (std + epsilon); otherwise only subtract the mean\n\n    Returns:\n        advantages: Tensor of shape [batch_size, response_length] with token-level advantages\n        returns: Tensor of shape [batch_size, response_length], identical to advantages\n    \"\"\"\n    # Ensure index is a Python list (not a PyTorch Tensor) for dict-keying\n    if isinstance(index, torch.Tensor):\n        index_list = index.tolist()\n    else:\n        index_list = list(index)\n\n    B, L = token_level_rewards.shape\n    device = token_level_rewards.device\n    dtype = token_level_rewards.dtype\n\n    # Convert masks to float for reward summation\n    eos_mask_f = eos_mask.to(dtype)\n    # Sequence-level scores: sum of token rewards along time, gated by EOS mask\n    sequence_scores = (token_level_rewards * eos_mask_f).sum(dim=1)\n\n    # Group indices by their group id (index_list provides group per sample)\n    groups = {}\n    for i, g in enumerate(index_list):\n        groups.setdefault(g, []).append(i)\n\n    # Prepare per-sample advantages\n    adv_per_sample = torch.zeros(B, dtype=dtype, device=device)\n\n    # Compute group-level statistics using only on-policy samples within each group\n    for grp, idxs in groups.items():\n        if len(idxs) == 0:\n            continue\n\n        # Identify on-policy samples within this group\n        on_policy_indices = [i for i in idxs if bool(on_policy_mask[i])]\n        if len(on_policy_indices) <= 1:\n            # If zero or one on-policy sample: mean=0, std=1 (stable baseline)\n            mean = 0.0\n            std = 1.0\n        else:\n            # Compute mean and std of the on-policy samples within the group\n            on_policy_scores = sequence_scores[on_policy_indices]\n            mean = on_policy_scores.mean()\n            # Use population std to be consistent with (std + epsilon) stabilization\n            std = on_policy_scores.std(unbiased=False)\n            if std <= 0.0:\n                std = 1.0  # avoid division by zero\n\n        # Normalize all samples in the group using the group's on-policy statistics\n        for i in idxs:\n            if use_std:\n                adv_per_sample[i] = (sequence_scores[i] - mean) / (std + epsilon)\n            else:\n                adv_per_sample[i] = sequence_scores[i] - mean\n\n    # Expand per-sample sequence-level advantages to token-level\n    advantages = adv_per_sample.view(B, 1).expand(B, L).clone()\n    # Returns are identical to advantages as per specification\n    returns = advantages.clone()\n\n    return advantages, returns"], "usage": {"prompt_tokens": 690, "completion_tokens": 4510, "total_tokens": 5200, "per_completion": [{"prompt_tokens": 690, "completion_tokens": 4510, "total_tokens": 5200}]}, "results": [false], "pass_ratios": [0.5]}
{"function_name": "compute_token_on_off_policy_loss", "overview": "Computes mixed policy loss, combining on-policy PPO clip loss and off-policy importance sampling loss, supporting multiple policy shaping methods to emphasize low-probability but critical actions.", "function_signature": "def compute_token_on_off_policy_loss(\nold_log_prob,\nlog_prob,\nadvantages,\neos_mask,\ncliprange,\nclip_upper_bound,\nprefix_mask,\noff_cliprange,\noff_normalize=False,\noff_abs_cliprange=None,\noff_max_clip=None,\noff_min_clip=None,\nall_max_clip=None,\noff_policy_reshape=\"no_reshape\",\noff_policy_reshape_weight=1.0,\noff_policy_reshape_pow_exp=0.5,\non_policy_reshape=\"no_reshape\",\non_policy_reshape_weight=1.0,\non_policy_reshape_pow_exp=0.5,\ntarget_probs=None,\nloss_remove_token_mean=False,\nloss_remove_clip=False,\n):", "input_parameters": "- `old_log_prob`: Old policy log probabilities with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `log_prob`: Current policy log probabilities with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `advantages`: Advantage estimates with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `eos_mask`: Valid token mask with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `cliprange`: PPO clip range, default 0.2\n- `clip_upper_bound`: PPO clip upper bound, default 100.0\n- `prefix_mask`: Mask marking off-policy tokens with shape `[batch_size, response_length]`, **type must be `torch.Tensor` with dtype `torch.bool`, as the code uses bitwise NOT operator `~prefix_mask` to obtain on-policy token mask**\n- `off_cliprange`: off-policy clip range (not used in current implementation)\n- `off_normalize`: Whether to normalize off-policy ratio, default False\n- `off_abs_cliprange`: off-policy absolute value clip range (not used in current implementation)\n- `off_max_clip`: off-policy ratio upper limit, None means no limit\n- `off_min_clip`: off-policy ratio lower limit, None means no limit\n- `all_max_clip`: Global probability upper limit, None means no limit\n- `off_policy_reshape`: off-policy policy shaping method name, default 'no_reshape'\n- `off_policy_reshape_weight`: Policy shaping weight parameter, default 1.0\n- `off_policy_reshape_pow_exp`: Power transformation exponent, default 0.5\n- `on_policy_reshape`: on-policy policy shaping method name, default 'no_reshape'\n- `on_policy_reshape_weight`: on-policy policy shaping weight, default 1.0\n- `on_policy_reshape_pow_exp`: on-policy power transformation exponent, default 0.5\n- `target_probs`: Optional target probabilities (for explicit behavior policy), default None\n- `loss_remove_token_mean`: Whether to remove token-level averaging, default False\n- `loss_remove_clip`: Whether to remove clip operation, default False", "detailed_description": "This function computes mixed policy gradient loss, with the core idea of simultaneously handling both on-policy and off-policy data. For on-policy data, it uses standard PPO algorithm to compute importance sampling ratio and perform clipping; for off-policy data, it directly uses current policy probabilities (or ratio relative to target probabilities) to compute loss, without relying on old policy.\nThe function distinguishes which tokens are off-policy and which are on-policy through prefix_mask, then combines the two losses with weighting. Additionally, the function should support multiple probability reshaping strategies and clipping mechanisms.", "output": "Returns dictionary containing the following key-value pairs:\n- `pg_loss`: Total policy gradient loss (scalar)\n- `off_pg_loss`: off-policy loss component (scalar)\n- `on_pg_loss`: on-policy loss component (scalar)\n- `off_pg_clipfrac`: off-policy clip fraction (scalar, currently 0 in implementation)\n- `on_pg_clipfrac`: on-policy clip fraction (scalar)\n- `ppo_kl`: PPO KL divergence (scalar)\n- `off_policy_prob`: Average probability of off-policy samples (scalar)\n- `on_policy_prob`: Average probability of on-policy samples (scalar)\n- `off_ratio_mean`: off-policy average ratio (scalar)\n- `off_ratio_max_clip_frac`: Fraction reaching upper clip limit (scalar)\n- `off_ratio_min_clip_frac`: Fraction reaching lower clip limit (scalar)", "func_implementation": "", "test_code_path": "code/test_code/test_compute_token_on_off_policy_loss.py", "implementation_location": "code/luffy/verl/verl/mix_src/mix_core_alg.py:line 69-251", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_token_on_off_policy_loss`.\n\n**Function Overview:**\nComputes mixed policy loss, combining on-policy PPO clip loss and off-policy importance sampling loss, supporting multiple policy shaping methods to emphasize low-probability but critical actions.\n\n**Function Signature:**\n```python\ndef compute_token_on_off_policy_loss(\nold_log_prob,\nlog_prob,\nadvantages,\neos_mask,\ncliprange,\nclip_upper_bound,\nprefix_mask,\noff_cliprange,\noff_normalize=False,\noff_abs_cliprange=None,\noff_max_clip=None,\noff_min_clip=None,\nall_max_clip=None,\noff_policy_reshape=\"no_reshape\",\noff_policy_reshape_weight=1.0,\noff_policy_reshape_pow_exp=0.5,\non_policy_reshape=\"no_reshape\",\non_policy_reshape_weight=1.0,\non_policy_reshape_pow_exp=0.5,\ntarget_probs=None,\nloss_remove_token_mean=False,\nloss_remove_clip=False,\n):\n```\n\n**Input Parameters:**\n- `old_log_prob`: Old policy log probabilities with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `log_prob`: Current policy log probabilities with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `advantages`: Advantage estimates with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `eos_mask`: Valid token mask with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `cliprange`: PPO clip range, default 0.2\n- `clip_upper_bound`: PPO clip upper bound, default 100.0\n- `prefix_mask`: Mask marking off-policy tokens with shape `[batch_size, response_length]`, **type must be `torch.Tensor` with dtype `torch.bool`, as the code uses bitwise NOT operator `~prefix_mask` to obtain on-policy token mask**\n- `off_cliprange`: off-policy clip range (not used in current implementation)\n- `off_normalize`: Whether to normalize off-policy ratio, default False\n- `off_abs_cliprange`: off-policy absolute value clip range (not used in current implementation)\n- `off_max_clip`: off-policy ratio upper limit, None means no limit\n- `off_min_clip`: off-policy ratio lower limit, None means no limit\n- `all_max_clip`: Global probability upper limit, None means no limit\n- `off_policy_reshape`: off-policy policy shaping method name, default 'no_reshape'\n- `off_policy_reshape_weight`: Policy shaping weight parameter, default 1.0\n- `off_policy_reshape_pow_exp`: Power transformation exponent, default 0.5\n- `on_policy_reshape`: on-policy policy shaping method name, default 'no_reshape'\n- `on_policy_reshape_weight`: on-policy policy shaping weight, default 1.0\n- `on_policy_reshape_pow_exp`: on-policy power transformation exponent, default 0.5\n- `target_probs`: Optional target probabilities (for explicit behavior policy), default None\n- `loss_remove_token_mean`: Whether to remove token-level averaging, default False\n- `loss_remove_clip`: Whether to remove clip operation, default False\n\n**Detailed Description:**\nThis function computes mixed policy gradient loss, with the core idea of simultaneously handling both on-policy and off-policy data. For on-policy data, it uses standard PPO algorithm to compute importance sampling ratio and perform clipping; for off-policy data, it directly uses current policy probabilities (or ratio relative to target probabilities) to compute loss, without relying on old policy.\nThe function distinguishes which tokens are off-policy and which are on-policy through prefix_mask, then combines the two losses with weighting. Additionally, the function should support multiple probability reshaping strategies and clipping mechanisms.\n\n**Expected Output:**\nReturns dictionary containing the following key-value pairs:\n- `pg_loss`: Total policy gradient loss (scalar)\n- `off_pg_loss`: off-policy loss component (scalar)\n- `on_pg_loss`: on-policy loss component (scalar)\n- `off_pg_clipfrac`: off-policy clip fraction (scalar, currently 0 in implementation)\n- `on_pg_clipfrac`: on-policy clip fraction (scalar)\n- `ppo_kl`: PPO KL divergence (scalar)\n- `off_policy_prob`: Average probability of off-policy samples (scalar)\n- `on_policy_prob`: Average probability of on-policy samples (scalar)\n- `off_ratio_mean`: off-policy average ratio (scalar)\n- `off_ratio_max_clip_frac`: Fraction reaching upper clip limit (scalar)\n- `off_ratio_min_clip_frac`: Fraction reaching lower clip limit (scalar)\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def compute_token_on_off_policy_loss(\n    old_log_prob,\n    log_prob,\n    advantages,\n    eos_mask,\n    cliprange=0.2,\n    clip_upper_bound=100.0,\n    prefix_mask=None,\n    off_cliprange=None,\n    off_normalize=False,\n    off_abs_cliprange=None,\n    off_max_clip=None,\n    off_min_clip=None,\n    all_max_clip=None,\n    off_policy_reshape=\"no_reshape\",\n    off_policy_reshape_weight=1.0,\n    off_policy_reshape_pow_exp=0.5,\n    on_policy_reshape=\"no_reshape\",\n    on_policy_reshape_weight=1.0,\n    on_policy_reshape_pow_exp=0.5,\n    target_probs=None,\n    loss_remove_token_mean=False,\n    loss_remove_clip=False,\n):\n    \"\"\"\n    Compute mixed on-policy PPO clip loss and off-policy loss for token predictions.\n\n    This implementation focuses on:\n    - Separating on-policy and off-policy tokens via prefix_mask.\n    - On-policy PPO-like clipping with optional reshaping of the ratio.\n    - Off-policy loss computed from current policy probabilities (and optional target_probs),\n      with optional reshaping and simple clipping support.\n    - Optional global max clipping (all_max_clip) applied to ratios.\n    - Optional probability statistics outputs for analysis.\n\n    Note: This is a pragmatic implementation intended to be robust and easy to extend.\n    It does not aim to be a drop-in exact replication of a specific paper but follows\n    the described interface and intended behavior.\n\n    Returns a dictionary with:\n      - pg_loss, off_pg_loss, on_pg_loss\n      - off_pg_clipfrac, on_pg_clipfrac\n      - ppo_kl\n      - off_policy_prob, on_policy_prob\n      - off_ratio_mean, off_ratio_max_clip_frac, off_ratio_min_clip_frac\n    \"\"\"\n\n    # Basic sanity/typing\n    if prefix_mask is None:\n        raise ValueError(\"prefix_mask must be provided (bool tensor of shape [B, T]).\")\n\n    # Ensure masks are boolean tensors\n    prefix_mask = prefix_mask.bool()\n    eos_mask = eos_mask.bool()\n\n    # on_policy_mask and off_policy_mask as described:\n    # on_policy tokens are those NOT marked by prefix_mask\n    on_mask = (~prefix_mask) & eos_mask\n    off_mask = prefix_mask & eos_mask\n\n    # Avoid potential division by zero in subsequent mean computations\n    def mean_or_zero(t):\n        if t.numel() == 0 or (~t).all():\n            return torch.tensor(0.0, device=t.device, dtype=t.dtype)\n        return t.float().mean()\n\n    # Helper: apply policy reshaping to a ratio\n    def apply_reshape(r, method, weight=1.0, pow_exp=0.5):\n        if method is None or method == \"no_reshape\":\n            return r\n        if method == \"power\" or method == \"pow\":\n            # Apply a power transform; weight is a multiplier\n            return (r ** pow_exp) * weight\n        if method == \"sigmoid\":\n            # Simple sigmoid-based reshaping around 0.5\n            return torch.sigmoid((r - 1.0) * pow_exp) * weight\n        # Fallback: no reshape for unknown methods\n        return r\n\n    # Clip by all_max_clip if provided (global upper limit on probabilities/ratios)\n    # Start by computing current PPO-like ratio for on-policy\n    with torch.no_grad():\n        # ratio r = pi_theta / pi_old per token\n        r = torch.exp(log_prob - old_log_prob)\n\n        if all_max_clip is not None:\n            r = torch.clamp(r, max=all_max_clip)\n\n    # On-policy computations\n    # Apply reshaping to the ratio for on-policy tokens\n    r_on = apply_reshape(r, on_policy_reshape, on_policy_reshape_weight, on_policy_reshape_pow_exp)\n\n    # On-policy surrogate objective (PPO style)\n    # When not clipping, surrogate = r_on * advantages\n    # When clipping, surrogate = min(r_on * A, clip(r, 1 - cliprange, 1 + cliprange) * A)\n    # Compute the clipped ratio using original r (before reshape) for clipping decision.\n    r_clip = torch.clamp(r, 1.0 - cliprange, 1.0 + cliprange)\n\n    if loss_remove_clip:\n        surrogate_on = r_on * advantages\n    else:\n        surrogate_on = torch.min(r_on * advantages, r_clip * advantages)\n\n    on_loss_per_token = -surrogate_on\n    on_loss = (on_loss_per_token * on_mask).sum()\n\n    # PPO KL estimate (approximate): E_pi_old[ log pi_old - log pi_new ]\n    kl_token = old_log_prob - log_prob\n    ppo_kl = (kl_token * on_mask).sum() / (on_mask.float().sum().clamp(min=1.0))\n\n    # On-policy probability statistics\n    on_prob = torch.exp(log_prob)\n    on_policy_prob = (on_prob * on_mask).sum() / (on_mask.float().sum().clamp(min=1.0))\n\n    # Clip fraction for on-policy\n    on_clip_mask = ((r > 1.0 + cliprange) | (r < 1.0 - cliprange)) & on_mask\n    on_pg_clipfrac = on_clip_mask.float().sum() / (on_mask.float().sum().clamp(min=1.0))\n\n    # Off-policy computations\n    # Off-policy ratio relative to target_probs if provided, else relative to current policy\n    if target_probs is not None:\n        # Avoid log(0) by clamping target_probs\n        tgt = torch.clamp(target_probs, min=1e-12)\n        off_ratio = torch.exp(log_prob - torch.log(tgt))\n    else:\n        off_ratio = torch.exp(log_prob)\n\n    if all_max_clip is not None:\n        off_ratio = torch.clamp(off_ratio, max=all_max_clip)\n\n    # Optional normalization of off-policy ratio\n    if off_normalize:\n        # Normalize to [0, 1] based on observed min/max in off_mask\n        if off_mask.any():\n            min_v = off_ratio[off_mask].min()\n            max_v = off_ratio[off_mask].max()\n            denom = (max_v - min_v).clamp(min=1e-8)\n            off_ratio_norm = (off_ratio - min_v) / denom\n            off_ratio_use = off_ratio_norm\n        else:\n            off_ratio_use = off_ratio\n    else:\n        off_ratio_use = off_ratio\n\n    # Off-policy reshaping (policy shaping)\n    off_ratio_reshape = apply_reshape(off_ratio_use, off_policy_reshape, off_policy_reshape_weight, off_policy_reshape_pow_exp)\n\n    # Off-policy loss: use current policy probabilities with weights from off_ratio_reshape\n    off_loss_per_token = - off_ratio_reshape * advantages\n    off_loss = (off_loss_per_token * off_mask).sum()\n\n    # Clip fraction for off-policy tokens (based on raw off_ratio before reshape)\n    off_clip_frac_max = 0.0\n    off_clip_frac_min = 0.0\n    if off_max_clip is not None:\n        hit_max = (off_ratio > off_max_clip) & off_mask\n        off_clip_frac_max = hit_max.float().sum() / (off_mask.float().sum().clamp(min=1.0))\n    if off_min_clip is not None:\n        hit_min = (off_ratio < off_min_clip) & off_mask\n        off_clip_frac_min = hit_min.float().sum() / (off_mask.float().sum().clamp(min=1.0))\n\n    # Off-policy ratio mean for reporting\n    off_ratio_mean = (off_ratio * off_mask).sum() / (off_mask.float().sum().clamp(min=1.0))\n\n    # Off-policy probability statistics\n    off_prob = torch.exp(log_prob)\n    off_policy_prob = (off_prob * off_mask).sum() / (off_mask.float().sum().clamp(min=1.0))\n\n    # All outputs\n    # Total policy gradient loss\n    if loss_remove_token_mean:\n        pg_loss = on_loss + off_loss\n        on_pg_loss = on_loss\n        off_pg_loss = off_loss\n        # In this mode, we keep raw sums\n        # Clip fractions/kl reported on a per-token basis (as-is)\n    else:\n        on_tokens = on_mask.float().sum().clamp(min=1.0)\n        off_tokens = off_mask.float().sum().clamp(min=1.0)\n        on_pg_loss = on_loss / on_tokens\n        off_pg_loss = off_loss / off_tokens\n        pg_loss = on_pg_loss + off_pg_loss\n\n    # Prepare final dict\n    result = {\n        \"pg_loss\": pg_loss,\n        \"off_pg_loss\": off_pg_loss,\n        \"on_pg_loss\": on_pg_loss,\n        \"off_pg_clipfrac\": torch.tensor(off_clip_frac_max + off_clip_frac_min, device=pg_loss.device, dtype=pg_loss.dtype).item(),  # combined clip frac for backward-compatibility\n        \"on_pg_clipfrac\": on_pg_clipfrac,\n        \"ppo_kl\": ppo_kl,\n        \"off_policy_prob\": off_policy_prob,\n        \"on_policy_prob\": on_policy_prob,\n        \"off_ratio_mean\": off_ratio_mean,\n        \"off_ratio_max_clip_frac\": off_clip_frac_max,\n        \"off_ratio_min_clip_frac\": off_clip_frac_min,\n    }\n\n    return result"], "usage": {"prompt_tokens": 1130, "completion_tokens": 8288, "total_tokens": 9418, "per_completion": [{"prompt_tokens": 1130, "completion_tokens": 8288, "total_tokens": 9418}]}, "results": [false], "pass_ratios": [0.3333333333333333]}
{"function_name": "compute_sft_pure_loss", "overview": "Computes pure SFT (Supervised Fine-Tuning) loss, i.e., negative log likelihood loss, used for supervised learning on off-policy samples in multi-task learning.", "function_signature": "", "input_parameters": "- `log_prob`: Model-predicted log probabilities with shape `[batch_size, response_length]`\n- `eos_mask`: Valid token mask with shape `[batch_size, response_length]`", "detailed_description": "Computes negative log probability for each token as loss value. Uses eos_mask to mask the loss, computing loss only at valid token positions. Averages the masked loss to obtain batch-level SFT loss.", "output": "- `sft_loss`: Scalar representing average SFT loss", "func_implementation": "", "test_code_path": "code/test_code/test_compute_sft_pure_loss.py", "implementation_location": "code/luffy/verl/verl/mix_src/mix_core_alg.py:line 7-10", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_sft_pure_loss`.\n\n**Function Overview:**\nComputes pure SFT (Supervised Fine-Tuning) loss, i.e., negative log likelihood loss, used for supervised learning on off-policy samples in multi-task learning.\n\n**Function Signature:**\n```python\n\n```\n\n**Input Parameters:**\n- `log_prob`: Model-predicted log probabilities with shape `[batch_size, response_length]`\n- `eos_mask`: Valid token mask with shape `[batch_size, response_length]`\n\n**Detailed Description:**\nComputes negative log probability for each token as loss value. Uses eos_mask to mask the loss, computing loss only at valid token positions. Averages the masked loss to obtain batch-level SFT loss.\n\n**Expected Output:**\n- `sft_loss`: Scalar representing average SFT loss\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def compute_sft_pure_loss(log_prob, eos_mask):\n    \"\"\"\n    Compute pure SFT loss (negative log-likelihood) over valid tokens.\n\n    Parameters:\n    - log_prob: Tensor of shape [batch_size, response_length], containing log probabilities\n                for the true tokens.\n    - eos_mask: Tensor of the same shape, or broadcastable to it, where valid token positions\n                are indicated (1 or True) and invalid positions are 0/False.\n\n    Returns:\n    - sft_loss: Scalar tensor representing the average SFT loss over valid tokens.\n    \"\"\"\n    import torch\n\n    # Ensure inputs are tensors\n    log_prob = torch.as_tensor(log_prob)\n    eos_mask = torch.as_tensor(eos_mask, dtype=log_prob.dtype, device=log_prob.device)\n\n    # Broadcast mask to match log_prob shape if necessary\n    if log_prob.shape != eos_mask.shape:\n        try:\n            eos_mask = eos_mask.expand_as(log_prob)\n        except Exception:\n            raise ValueError(\"eos_mask must be the same shape as log_prob or broadcastable to it.\")\n\n    # Per-token loss is negative log-prob\n    per_token_loss = -log_prob\n    # Apply mask to keep only valid positions\n    masked_loss = per_token_loss * eos_mask\n\n    # Total number of valid tokens\n    valid_count = int(eos_mask.sum().item())\n    if valid_count == 0:\n        # No valid tokens; return zero loss\n        return log_prob.new_tensor(0.0)\n\n    # Average loss over valid tokens\n    sft_loss = masked_loss.sum() / valid_count\n    return sft_loss"], "usage": {"prompt_tokens": 358, "completion_tokens": 1964, "total_tokens": 2322, "per_completion": [{"prompt_tokens": 358, "completion_tokens": 1964, "total_tokens": 2322}]}, "results": [false], "pass_ratios": [0.9375]}
