{"function_name": "compute_score", "overview": "Computes ARES hierarchical entropy rewards, dynamically adjusting reward signals based on sample difficulty, accuracy, and High Window Entropy (HWE) token count.", "function_signature": "def compute_score(reward_inputs: List[Dict[str, Any]], format_weight: float = 0.1, alpha_length_bonus: float = 0.5, alpha_entropy: float = 0.5) -> List[Dict[str, float]]:", "input_parameters": "- `reward_inputs`: List of reward input dictionaries, each containing:\n- `response`: Model-generated response text\n- `ground_truth`: Ground truth answer\n- `difficulty`: Sample difficulty label (\"easy\" / \"medium\" / \"hard\")\n- `high_entropy_token_num`: Number of HWE tokens in response\n- `target_high_entropy_token_num`: Target HWE token count (difficulty-related)\n- `alpha_entropy`: Entropy reward coefficient (difficulty-related)\n- `accuracy`: Accuracy label (optional, if not provided, extracted from response and evaluated)\n- `format_weight`: Format reward weight, default 0.1\n- `alpha_length_bonus`: Length reward coefficient (legacy parameter, actual usage is alpha_entropy)\n- `alpha_entropy`: Default entropy reward coefficient, default 0.5 (used if sample doesn't specify)", "detailed_description": "This function implements ARES's core reward shaping mechanism, adaptively adjusting reasoning resource allocation based on task difficulty and model performance. The function computes format reward, accuracy reward, and entropy reward. For easy questions, when the answer is correct, it penalizes overly long reasoning chains, while when the answer is incorrect, it provides moderate positive rewards to encourage the model to explore more possibilities. Medium-difficulty questions require reasoning length closer to the target value; both excessive and insufficient reasoning are penalized when the answer is correct, but longer reasoning is still encouraged when the answer is incorrect. Hard questions adopt the opposite strategy: when the answer is correct, reasoning chains close to or exceeding the target are rewarded, and penalties are only applied when reasoning is significantly insufficient; when the answer is incorrect, longer reasoning is also encouraged to find the correct path.\nThe entire reward mechanism uses Huber loss function to smoothly handle penalty terms, Sigmoid function to achieve reward saturation effects, and sets different tolerance boundaries and reward caps for different difficulties.\nDifficulty-related hyperparameters that may be used:\n- `MARGIN_FRAC`: Tolerance band width (easy: 15%, medium: 25%, hard: 35%)\n- `KAPPA`: Huber function inflection point (easy: 2.0, medium: 3.0, hard: 4.0)\n- `TEMP`: Sigmoid temperature parameter (easy: 2.0, medium: 2.5, hard: 3.0)\n- `CAP_SCALE`: Entropy reward cap coefficient (easy: 1.0, medium: 1.0, hard: 1.2)", "output": "- `scores`: List of score dictionaries, each containing:\n- `overall`: Total score = accuracy + entropy_score\n- `accuracy`: Accuracy score (0.0 or 1.0)\n- `format`: Format score (0.0 or 1.0)\n- `high_entropy_token_num_score`: Entropy reward score", "func_implementation": "", "test_code_path": "code/test_code/test_compute_score.py", "implementation_location": "code/examples/reward_function/aepo.py:line 58-133", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_score`.\n\n**Function Overview:**\nComputes ARES hierarchical entropy rewards, dynamically adjusting reward signals based on sample difficulty, accuracy, and High Window Entropy (HWE) token count.\n\n**Function Signature:**\n```python\ndef compute_score(reward_inputs: List[Dict[str, Any]], format_weight: float = 0.1, alpha_length_bonus: float = 0.5, alpha_entropy: float = 0.5) -> List[Dict[str, float]]:\n```\n\n**Input Parameters:**\n- `reward_inputs`: List of reward input dictionaries, each containing:\n- `response`: Model-generated response text\n- `ground_truth`: Ground truth answer\n- `difficulty`: Sample difficulty label (\"easy\" / \"medium\" / \"hard\")\n- `high_entropy_token_num`: Number of HWE tokens in response\n- `target_high_entropy_token_num`: Target HWE token count (difficulty-related)\n- `alpha_entropy`: Entropy reward coefficient (difficulty-related)\n- `accuracy`: Accuracy label (optional, if not provided, extracted from response and evaluated)\n- `format_weight`: Format reward weight, default 0.1\n- `alpha_length_bonus`: Length reward coefficient (legacy parameter, actual usage is alpha_entropy)\n- `alpha_entropy`: Default entropy reward coefficient, default 0.5 (used if sample doesn't specify)\n\n**Detailed Description:**\nThis function implements ARES's core reward shaping mechanism, adaptively adjusting reasoning resource allocation based on task difficulty and model performance. The function computes format reward, accuracy reward, and entropy reward. For easy questions, when the answer is correct, it penalizes overly long reasoning chains, while when the answer is incorrect, it provides moderate positive rewards to encourage the model to explore more possibilities. Medium-difficulty questions require reasoning length closer to the target value; both excessive and insufficient reasoning are penalized when the answer is correct, but longer reasoning is still encouraged when the answer is incorrect. Hard questions adopt the opposite strategy: when the answer is correct, reasoning chains close to or exceeding the target are rewarded, and penalties are only applied when reasoning is significantly insufficient; when the answer is incorrect, longer reasoning is also encouraged to find the correct path.\nThe entire reward mechanism uses Huber loss function to smoothly handle penalty terms, Sigmoid function to achieve reward saturation effects, and sets different tolerance boundaries and reward caps for different difficulties.\nDifficulty-related hyperparameters that may be used:\n- `MARGIN_FRAC`: Tolerance band width (easy: 15%, medium: 25%, hard: 35%)\n- `KAPPA`: Huber function inflection point (easy: 2.0, medium: 3.0, hard: 4.0)\n- `TEMP`: Sigmoid temperature parameter (easy: 2.0, medium: 2.5, hard: 3.0)\n- `CAP_SCALE`: Entropy reward cap coefficient (easy: 1.0, medium: 1.0, hard: 1.2)\n\n**Expected Output:**\n- `scores`: List of score dictionaries, each containing:\n- `overall`: Total score = accuracy + entropy_score\n- `accuracy`: Accuracy score (0.0 or 1.0)\n- `format`: Format score (0.0 or 1.0)\n- `high_entropy_token_num_score`: Entropy reward score\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def compute_score(reward_inputs: List[Dict[str, Any]], format_weight: float = 0.1, alpha_length_bonus: float = 0.5, alpha_entropy: float = 0.5) -> List[Dict[str, float]]:\n    \"\"\"\n    Compute ARES hierarchical entropy rewards for a batch of reward inputs.\n\n    This implementation follows a practical interpretation of the described mechanism:\n    - accuracy_score: 1.0 if ground truth appears in the model response, else 0.0\n    - format_score: 1.0 if the response includes the ground truth (simple format check), else 0.0\n    - entropy_score: an adaptive reward based on the deviation of actual High Entropy tokens\n      from the target, using a smooth (Huber) penalty and a cap. The final entropy term\n      may be scaled by a per-sample entropy coefficient.\n    - overall = accuracy + (format_weight * format_score) + (entropy_term * entropy_coefficient)\n\n    The function uses per-difficulty hyperparameters to shape the reward:\n      easy:   MARGIN_FRAC=0.15, KAPPA=2.0, TEMP=2.0, CAP_SCALE=1.0\n      medium: MARGIN_FRAC=0.25, KAPPA=3.0, TEMP=2.5, CAP_SCALE=1.0\n      hard:   MARGIN_FRAC=0.35, KAPPA=4.0, TEMP=3.0, CAP_SCALE=1.2\n\n    Note:\n    - alpha_entropy from each reward input takes precedence over the function default.\n    - alpha_length_bonus is treated as legacy parameter and not directly used beyond the\n      function signature for compatibility.\n\n    Parameters:\n    - reward_inputs: list of dicts with keys such as:\n        response, ground_truth, difficulty, high_entropy_token_num,\n        target_high_entropy_token_num, accuracy (optional), alpha_entropy (optional), etc.\n    - format_weight: weight applied to the format_score contribution to the entropy/overall score\n    - alpha_length_bonus: legacy parameter (not directly applied in this simplified\n      implementation; kept for compatibility)\n    - alpha_entropy: default entropy coefficient if not provided per-sample\n\n    Returns:\n    - List of dicts with keys: overall, accuracy, format, high_entropy_token_num_score\n    \"\"\"\n    from math import exp\n\n    # Huber loss helper\n    def huber_loss(x: float, delta: float) -> float:\n        ax = abs(x)\n        if ax <= delta:\n            return 0.5 * (x * x)\n        else:\n            return delta * (ax - 0.5 * delta)\n\n    # Difficulty-specific hyperparameters\n    diff_params = {\n        'easy':   {'marg_frac': 0.15, 'kappa': 2.0, 'temp': 2.0, 'cap_scale': 1.0},\n        'medium': {'marg_frac': 0.25, 'kappa': 3.0, 'temp': 2.5, 'cap_scale': 1.0},\n        'hard':   {'marg_frac': 0.35, 'kappa': 4.0, 'temp': 3.0, 'cap_scale': 1.2},\n    }\n\n    scores: List[Dict[str, float]] = []\n\n    for item in reward_inputs:\n        response = item.get('response', '')\n        ground_truth = item.get('ground_truth', '')\n        difficulty = str(item.get('difficulty', 'medium')).lower() if item.get('difficulty') is not None else 'medium'\n\n        # Normalize inputs\n        if response is None:\n            response = ''\n        if ground_truth is None:\n            ground_truth = ''\n\n        # Accuracy: use explicit field if provided, else rely on containment check\n        acc = 0.0\n        if 'accuracy' in item and item['accuracy'] is not None:\n            try:\n                acc = float(item['accuracy'])\n            except (ValueError, TypeError):\n                acc = 0.0\n        else:\n            if isinstance(ground_truth, str) and isinstance(response, str) and ground_truth.strip() != '':\n                acc = 1.0 if ground_truth.lower() in response.lower() else 0.0\n            else:\n                acc = 0.0\n\n        # Format score: simple containment check (used to influence entropy reward)\n        format_score = 1.0 if (isinstance(ground_truth, str) and isinstance(response, str) and\n                               ground_truth.strip() != '' and ground_truth.lower() in response.lower()) else 0.0\n\n        # High Entropy Token (HWE) statistics\n        hwe = float(item.get('high_entropy_token_num', 0.0))\n        target_hwe = float(item.get('target_high_entropy_token_num', 0.0))\n        diff = hwe - target_hwe  # positive means more reasoning than target\n\n        # Select parameters based on difficulty\n        if difficulty not in diff_params:\n            difficulty_key = 'medium'\n        else:\n            difficulty_key = difficulty\n        p = diff_params[difficulty_key]\n        kappa = p['kappa']\n        cap = p['cap_scale']\n\n        # Hubered penalty for deviation\n        huber = huber_loss(diff, kappa)\n\n        # Base entropy term: higher when deviation is small; clipped by CAP_SCALE\n        entropy_term = max(0.0, cap - huber)\n\n        # Difficulty/accuracy adjustments (simple, interpretable shaping)\n        # If answer is incorrect, encourage exploration (slightly boost entropy reward)\n        if acc == 0.0 and entropy_term > 0.0:\n            entropy_term = min(cap, entropy_term * 1.25)\n\n        # Per-sample entropy coefficient (default uses function param if not provided)\n        entropy_coeff = item.get('alpha_entropy', alpha_entropy)\n        # Allow legacy alpha_length_bonus to influence entropy if provided (optional, not required)\n        _ = item.get('alpha_length_bonus', alpha_length_bonus)  # kept for compatibility; not directly used\n\n        entropy_score_final = entropy_term * float(entropy_coeff)\n\n        # Final overall score logic\n        # Incorporate format contribution scaled by format_weight\n        overall = float(acc) + (format_weight * float(format_score)) + entropy_score_final\n\n        scores.append({\n            'overall': float(overall),\n            'accuracy': float(acc),\n            'format': float(format_score),\n            'high_entropy_token_num_score': float(entropy_score_final)\n        })\n\n    return scores"], "usage": {"prompt_tokens": 884, "completion_tokens": 8469, "total_tokens": 9353, "per_completion": [{"prompt_tokens": 884, "completion_tokens": 8469, "total_tokens": 9353}]}}
{"function_name": "RayPPOTrainer.update_difficulty_and_skip_gid_set", "overview": "Updates sample difficulty dictionary and adaptive entropy coefficients based on current batch rollout results, implementing online difficulty bucketing and resource budget adjustment.", "function_signature": "def update_difficulty_and_skip_gid_set(self, batch: DataProto) -> None:", "input_parameters": "- `batch`: DataProto object containing:\n- `global_id`: Sample global ID (for cross-batch tracking)\n- `accuracy`: Accuracy list (accuracy of each response)\n- `entropies`: Average entropy list (average token entropy of each response)\n- `high_entropy_token_num`: HWE token count list (HWE token count of each response)", "detailed_description": "This function is responsible for dynamically updating question difficulty classification and adaptively adjusting entropy penalty coefficients during training. The function needs to aggregate accuracy, entropy, and high-entropy token counts for each question by global identifier. Questions are categorized into easy, medium, and hard levels based on accuracy. Based on high-entropy token counts, the function needs to compute target reasoning length for each difficulty level and adjust entropy penalty coefficients based on deviation between actual generated length and target value. When adjusting entropy penalty coefficients, gradient ascent can be applied. For easy and medium questions, if reasoning is too long, increase penalty strength; for hard questions, increase encouragement when reasoning is insufficient.\nDifficulty assignment based on accuracy thresholds:\n- `acc_rate ≥ 2/3` → \"easy\"\n- `1/3 ≤ acc_rate < 2/3` → \"medium\"\n- `acc_rate < 1/3` → \"hard\"\n(Optional) The function adds samples that are \"all correct with extremely low average entropy\" to `self.skip_gid_set`, and subsequent training will skip these overly simple samples, focusing on challenging data.", "output": "- No direct return value, but updates the following class attributes:\n- `self.difficulty_dict_train`: Sample difficulty dictionary `{global_id: difficulty}`\n- `self.target_high_entropy_token_num_dict`: Target HWE token count `{difficulty: target_num}`\n- `self.alpha_entropy_dict`: Entropy coefficient dictionary `{difficulty: alpha}`\n- `self.skip_gid_set`: Skip sample set (optional)", "func_implementation": "", "test_code_path": "code/test_code/test_update_difficulty.py", "implementation_location": "code/verl/trainer/ray_trainer.py:line 775-825", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `RayPPOTrainer.update_difficulty_and_skip_gid_set`.\n\n**Function Overview:**\nUpdates sample difficulty dictionary and adaptive entropy coefficients based on current batch rollout results, implementing online difficulty bucketing and resource budget adjustment.\n\n**Function Signature:**\n```python\ndef update_difficulty_and_skip_gid_set(self, batch: DataProto) -> None:\n```\n\n**Input Parameters:**\n- `batch`: DataProto object containing:\n- `global_id`: Sample global ID (for cross-batch tracking)\n- `accuracy`: Accuracy list (accuracy of each response)\n- `entropies`: Average entropy list (average token entropy of each response)\n- `high_entropy_token_num`: HWE token count list (HWE token count of each response)\n\n**Detailed Description:**\nThis function is responsible for dynamically updating question difficulty classification and adaptively adjusting entropy penalty coefficients during training. The function needs to aggregate accuracy, entropy, and high-entropy token counts for each question by global identifier. Questions are categorized into easy, medium, and hard levels based on accuracy. Based on high-entropy token counts, the function needs to compute target reasoning length for each difficulty level and adjust entropy penalty coefficients based on deviation between actual generated length and target value. When adjusting entropy penalty coefficients, gradient ascent can be applied. For easy and medium questions, if reasoning is too long, increase penalty strength; for hard questions, increase encouragement when reasoning is insufficient.\nDifficulty assignment based on accuracy thresholds:\n- `acc_rate ≥ 2/3` → \"easy\"\n- `1/3 ≤ acc_rate < 2/3` → \"medium\"\n- `acc_rate < 1/3` → \"hard\"\n(Optional) The function adds samples that are \"all correct with extremely low average entropy\" to `self.skip_gid_set`, and subsequent training will skip these overly simple samples, focusing on challenging data.\n\n**Expected Output:**\n- No direct return value, but updates the following class attributes:\n- `self.difficulty_dict_train`: Sample difficulty dictionary `{global_id: difficulty}`\n- `self.target_high_entropy_token_num_dict`: Target HWE token count `{difficulty: target_num}`\n- `self.alpha_entropy_dict`: Entropy coefficient dictionary `{difficulty: alpha}`\n- `self.skip_gid_set`: Skip sample set (optional)\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def update_difficulty_and_skip_gid_set(self, batch: DataProto) -> None:\n    \"\"\"\n    Update per-sample difficulty classification, adaptive entropy penalties, and skip-set\n    based on the current batch rollout results.\n\n    - Aggregates accuracy, entropy, and high-entropy-token counts by global_id.\n    - Assigns difficulty: easy (acc >= 2/3), medium (1/3 <= acc < 2/3), hard (acc < 1/3).\n    - Computes target high-entropy-token counts per difficulty using a moving average.\n    - Adjusts entropy penalty coefficients (alpha) per difficulty using a simple gradient-ascent-like update.\n    - Optionally adds samples that are all-correct with extremely low entropy to skip_gid_set.\n    \"\"\"\n    # Initialize containers if not present\n    if not hasattr(self, \"difficulty_dict_train\") or self.difficulty_dict_train is None:\n        self.difficulty_dict_train = {}\n    if not hasattr(self, \"target_high_entropy_token_num_dict\") or self.target_high_entropy_token_num_dict is None:\n        self.target_high_entropy_token_num_dict = {}\n    if not hasattr(self, \"alpha_entropy_dict\") or self.alpha_entropy_dict is None:\n        self.alpha_entropy_dict = {}\n    if not hasattr(self, \"skip_gid_set\") or self.skip_gid_set is None:\n        self.skip_gid_set = set()\n\n    # Safely fetch batch fields, default to empty list if missing\n    gids = getattr(batch, \"global_id\", [])\n    accs = getattr(batch, \"accuracy\", [])\n    ents = getattr(batch, \"entropies\", [])\n    hwes = getattr(batch, \"high_entropy_token_num\", [])\n\n    def to_list(x):\n        if isinstance(x, (list, tuple)):\n            return list(x)\n        if x is None:\n            return []\n        return [x]\n\n    gids_list = to_list(gids)\n    accs_list = to_list(accs)\n    ents_list = to_list(ents)\n    hwes_list = to_list(hwes)\n\n    # Determine number of samples to process\n    n = min(len(gids_list), len(accs_list), len(ents_list), len(hwes_list))\n    if n <= 0:\n        return\n\n    # Helper to extract a per-sample value, supporting nested per-response lists\n    def extract_value(field_list, idx):\n        val = field_list[idx]\n        if isinstance(val, (list, tuple)):\n            if len(val) == 0:\n                return 0.0\n            # Average across inner values; inner items may be numeric or list of numerics\n            s = 0.0\n            count = 0\n            for inner in val:\n                if isinstance(inner, (list, tuple)):\n                    if len(inner) > 0:\n                        s += sum(inner) / len(inner)\n                        count += 1\n                else:\n                    try:\n                        s += float(inner)\n                        count += 1\n                    except (TypeError, ValueError):\n                        continue\n            if count == 0:\n                return 0.0\n            return s / count\n        else:\n            try:\n                return float(val)\n            except (TypeError, ValueError):\n                return 0.0\n\n    # Thresholds for skip condition\n    ACC_ALL_CORRECT_THRESHOLD = 0.999\n    ENTROPY_LOW_THRESHOLD = 0.05\n\n    # Local accumulators to compute per-difficulty statistics across this batch\n    batch_count_by_diff = {\"easy\": 0, \"medium\": 0, \"hard\": 0}\n    batch_hwe_by_diff = {\"easy\": 0.0, \"medium\": 0.0, \"hard\": 0.0}\n    # Map to store per-sample diff (optional, for completeness)\n    batch_diff_map = {}\n\n    # Process each sample in the batch\n    for i in range(n):\n        gid = str(gids_list[i])\n\n        acc_val = extract_value(accs_list, i)\n        ent_val = extract_value(ents_list, i)\n        hwe_val = extract_value(hwes_list, i)\n\n        # Determine difficulty\n        if acc_val >= 2.0 / 3.0:\n            diff = \"easy\"\n        elif acc_val >= 1.0 / 3.0:\n            diff = \"medium\"\n        else:\n            diff = \"hard\"\n\n        batch_diff_map[gid] = diff\n        # Update global difficulty dictionary (cross-batch tracking)\n        self.difficulty_dict_train[gid] = diff\n\n        # Accumulate per-diff statistics\n        batch_count_by_diff[diff] = batch_count_by_diff.get(diff, 0) + 1\n        batch_hwe_by_diff[diff] = batch_hwe_by_diff.get(diff, 0.0) + float(hwe_val)\n\n        # Optional: add trivially easy samples to skip set\n        if acc_val >= ACC_ALL_CORRECT_THRESHOLD and ent_val <= ENTROPY_LOW_THRESHOLD:\n            self.skip_gid_set.add(gid)\n\n    # Update target HWE and entropy coefficients per difficulty\n    for diff in (\"easy\", \"medium\", \"hard\"):\n        count = batch_count_by_diff.get(diff, 0)\n        if count <= 0:\n            continue\n\n        avg_hwe = batch_hwe_by_diff[diff] / float(count)\n\n        # Moving-average target update\n        old_target = self.target_high_entropy_token_num_dict.get(diff, avg_hwe)\n        lr = getattr(self, \"target_update_lr\", 0.1)\n        new_target = (1.0 - lr) * old_target + lr * avg_hwe\n        self.target_high_entropy_token_num_dict[diff] = max(0.0, new_target)\n\n        # Gradient-ascent-like update of entropy penalty coefficients\n        current_alpha = self.alpha_entropy_dict.get(diff, 0.0)\n        target = self.target_high_entropy_token_num_dict.get(diff, new_target)\n\n        if diff in (\"easy\", \"medium\"):\n            if avg_hwe > target * 1.01:\n                # Reasoning too long for easy/medium -> increase penalty\n                delta = max(0.001, 0.01 * (avg_hwe - target))\n                self.alpha_entropy_dict[diff] = current_alpha + delta\n            elif avg_hwe < target * 0.99:\n                # Slightly reduce penalty if overly short\n                delta = max(0.001, 0.01 * (target - avg_hwe))\n                self.alpha_entropy_dict[diff] = max(0.0, current_alpha - delta)\n        else:  # hard\n            if avg_hwe < target * 0.99:\n                # If hard questions have too short reasoning, encourage more (increase alpha)\n                delta = max(0.001, 0.01 * (target - avg_hwe))\n                self.alpha_entropy_dict[diff] = current_alpha + delta\n            elif avg_hwe > target * 1.01:\n                # If hard questions generate too long reasoning, reduce penalty\n                delta = max(0.001, 0.01 * (avg_hwe - target))\n                self.alpha_entropy_dict[diff] = max(0.0, current_alpha - delta)\n\n    # End of update\n    return"], "usage": {"prompt_tokens": 655, "completion_tokens": 8017, "total_tokens": 8672, "per_completion": [{"prompt_tokens": 655, "completion_tokens": 8017, "total_tokens": 8672}]}}
{"function_name": "TokenLevelAdaptiveKLController.update", "overview": "Implements update logic for token-level adaptive KL controller, dynamically adjusting KL coefficients using dual ascent mechanism based on deviation between actual KL divergence and target for each difficulty bucket.", "function_signature": "def update(self, current_kls: Dict[str, float], n_steps: Dict[str, int]):", "input_parameters": "- `current_kls`: Average KL divergence for each difficulty bucket at current step, dictionary format `{difficulty: kl_value}`\n- Example: `{\"easy\": 0.06, \"medium\": 0.12, \"hard\": 0.18}`\n- `n_steps`: Sample count for each difficulty bucket (used for weighting, may be simplified in actual implementation)", "detailed_description": "Token-level adaptive KL control allows using different KL budgets for different difficulty tasks, automatically adjusting KL divergence coefficients for each difficulty bucket through dual ascent mechanism. When actual KL divergence exceeds target value, increase penalty coefficient; when below target value, decrease coefficient.", "output": "- No direct return value, but updates class attributes:\n- `self.kl_coefs`: Updated KL coefficient dictionary `{difficulty: beta}`\n- `self.lambdas`: Updated Lagrange multiplier dictionary `{difficulty: lambda}`", "func_implementation": "", "test_code_path": "code/test_code/test_kl_controller.py", "implementation_location": "code/verl/trainer/core_algos.py:line 77-86", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `TokenLevelAdaptiveKLController.update`.\n\n**Function Overview:**\nImplements update logic for token-level adaptive KL controller, dynamically adjusting KL coefficients using dual ascent mechanism based on deviation between actual KL divergence and target for each difficulty bucket.\n\n**Function Signature:**\n```python\ndef update(self, current_kls: Dict[str, float], n_steps: Dict[str, int]):\n```\n\n**Input Parameters:**\n- `current_kls`: Average KL divergence for each difficulty bucket at current step, dictionary format `{difficulty: kl_value}`\n- Example: `{\"easy\": 0.06, \"medium\": 0.12, \"hard\": 0.18}`\n- `n_steps`: Sample count for each difficulty bucket (used for weighting, may be simplified in actual implementation)\n\n**Detailed Description:**\nToken-level adaptive KL control allows using different KL budgets for different difficulty tasks, automatically adjusting KL divergence coefficients for each difficulty bucket through dual ascent mechanism. When actual KL divergence exceeds target value, increase penalty coefficient; when below target value, decrease coefficient.\n\n**Expected Output:**\n- No direct return value, but updates class attributes:\n- `self.kl_coefs`: Updated KL coefficient dictionary `{difficulty: beta}`\n- `self.lambdas`: Updated Lagrange multiplier dictionary `{difficulty: lambda}`\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def update(self, current_kls: Dict[str, float], n_steps: Dict[str, int]):\n    \"\"\"\n    Token-level adaptive KL update using a dual ascent mechanism per difficulty bucket.\n\n    - current_kls: dict of {difficulty: current_kl_value}\n    - n_steps: dict of {difficulty: number_of_samples} used to weight updates\n\n    Updates:\n    - self.lambdas: per-bucket Lagrange multipliers (lambda)\n    - self.kl_coefs: per-bucket KL penalty coefficients (beta)\n\n    The update rules (per bucket):\n    - delta = current_kl - target_kl\n    - lambda_new = max(0, lambda_old + lambda_lr * delta * weight)\n      where weight = n_steps[difficulty] (default 1)\n    - beta_new:\n        if delta > 0  -> beta_old * (1 + beta_rate)\n        if delta < 0  -> max(min_beta, beta_old * (1 - beta_rate))\n        if delta == 0 -> beta_old\n    \"\"\"\n    if current_kls is None:\n        return\n\n    # Lazy initialization of dictionaries\n    if not hasattr(self, \"lambdas\") or self.lambdas is None:\n        self.lambdas = {}\n    if not hasattr(self, \"kl_coefs\") or self.kl_coefs is None:\n        self.kl_coefs = {}\n\n    # Retrieve targets; default to 0.0 if not provided\n    target_kls = getattr(self, \"target_kls\", {}) or {}\n    # Update hyperparameters with sensible defaults if not provided\n    lambda_lr = getattr(self, \"lambda_lr\", 0.01)\n    beta_rate = getattr(self, \"beta_rate\", 0.05)\n    min_beta = getattr(self, \"min_beta\", 0.0)\n    max_beta = getattr(self, \"max_beta\", 1e6)\n    initial_beta = getattr(self, \"initial_beta\", 0.1)\n\n    for diff, kl in current_kls.items():\n        target = target_kls.get(diff, 0.0)\n        delta = float(kl) - float(target)\n\n        # Weight updates by number of samples if provided\n        weight = float(n_steps.get(diff, 1)) if n_steps else 1.0\n\n        # Update Lagrange multiplier (lambda) with dual ascent\n        lambda_old = self.lambdas.get(diff, 0.0)\n        lambda_new = lambda_old + lambda_lr * delta * weight\n        if lambda_new < 0.0:\n            lambda_new = 0.0\n        self.lambdas[diff] = lambda_new\n\n        # Update KL coefficient (beta) based on sign of delta\n        beta_old = self.kl_coefs.get(diff, initial_beta)\n        if delta > 0.0:\n            beta_new = beta_old * (1.0 + beta_rate)\n        elif delta < 0.0:\n            beta_new = max(min_beta, beta_old * (1.0 - beta_rate))\n        else:\n            beta_new = beta_old\n\n        # Clamp beta within allowed range\n        if beta_new < min_beta:\n            beta_new = min_beta\n        if beta_new > max_beta:\n            beta_new = max_beta\n\n        self.kl_coefs[diff] = beta_new\n\n    # Function updates in-place; no explicit return value needed\n    return None"], "usage": {"prompt_tokens": 455, "completion_tokens": 3576, "total_tokens": 4031, "per_completion": [{"prompt_tokens": 455, "completion_tokens": 3576, "total_tokens": 4031}]}}
{"function_name": "apply_kl_penalty", "overview": "Applies KL divergence penalty to token-level rewards and updates KL controller.", "function_signature": "def apply_kl_penalty(data: DataProto, kl_ctrl: KLController, kl_penalty=\"kl\"):", "input_parameters": "- `data`: DataProto object containing:\n- `token_level_scores`: Original token-level rewards [batch_size, response_length]\n- `old_log_probs`: Actor old policy log probabilities [batch_size, response_length]\n- `ref_log_probs`: Ref policy log probabilities [batch_size, response_length]\n- `response_mask`: Response mask [batch_size, response_length]\n- `kl_ctrl`: KL controller (FixedKLController or AdaptiveKLController)\n- `kl_penalty`: KL penalty type (\"kl\", \"abs\", \"mse\", \"low_var_kl\", \"full\")", "detailed_description": "This function implements computation and application of token-level KL penalty. It computes token-level KL divergence based on `kl_penalty` type, then subtracts KL penalty term from original rewards to obtain token-level rewards, and computes sequence-level KL divergence and batch-level KL divergence.\nThe function also needs to implement KL controller update.", "output": "- `data`: Updated DataProto where `data.batch[\"token_level_rewards\"]` has KL penalty applied\n- `metrics`: Metrics dictionary `{\"critic/kl\": current_kl, \"critic/kl_coef\": kl_ctrl.kl_coef}`", "func_implementation": "", "test_code_path": "code/test_code/test_apply_kl_penalty.py", "implementation_location": "code/verl/trainer/ray_trainer.py:line 107-124", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `apply_kl_penalty`.\n\n**Function Overview:**\nApplies KL divergence penalty to token-level rewards and updates KL controller.\n\n**Function Signature:**\n```python\ndef apply_kl_penalty(data: DataProto, kl_ctrl: KLController, kl_penalty=\"kl\"):\n```\n\n**Input Parameters:**\n- `data`: DataProto object containing:\n- `token_level_scores`: Original token-level rewards [batch_size, response_length]\n- `old_log_probs`: Actor old policy log probabilities [batch_size, response_length]\n- `ref_log_probs`: Ref policy log probabilities [batch_size, response_length]\n- `response_mask`: Response mask [batch_size, response_length]\n- `kl_ctrl`: KL controller (FixedKLController or AdaptiveKLController)\n- `kl_penalty`: KL penalty type (\"kl\", \"abs\", \"mse\", \"low_var_kl\", \"full\")\n\n**Detailed Description:**\nThis function implements computation and application of token-level KL penalty. It computes token-level KL divergence based on `kl_penalty` type, then subtracts KL penalty term from original rewards to obtain token-level rewards, and computes sequence-level KL divergence and batch-level KL divergence.\nThe function also needs to implement KL controller update.\n\n**Expected Output:**\n- `data`: Updated DataProto where `data.batch[\"token_level_rewards\"]` has KL penalty applied\n- `metrics`: Metrics dictionary `{\"critic/kl\": current_kl, \"critic/kl_coef\": kl_ctrl.kl_coef}`\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def apply_kl_penalty(data: DataProto, kl_ctrl: KLController, kl_penalty=\"kl\"):\n    \"\"\"\n    Applies KL penalty to token-level rewards, updates data.batch[\"token_level_rewards\"],\n    and computes sequence-level and batch-level KL divergences. Also updates the KL controller.\n\n    Parameters:\n        data (DataProto): Input data containing token-level rewards/log-probs and mask.\n        kl_ctrl (KLController): KL controller (FixedKLController or AdaptiveKLController).\n        kl_penalty (str): Type of KL penalty to apply. One of {\"kl\",\"abs\",\"mse\",\"low_var_kl\",\"full\"}.\n\n    Returns:\n        data (DataProto): Updated with KL-penalized token-level rewards in data.batch.\n        metrics (dict): Metrics including {\"critic/kl\": current_kl, \"critic/kl_coef\": kl_ctrl.kl_coef}\n    \"\"\"\n    import numpy as np\n\n    batch = data.batch\n\n    # Retrieve token-level rewards (prefer existing token_level_rewards, else fall back to token_level_scores)\n    if \"token_level_rewards\" in batch:\n        token_level_rewards = np.array(batch[\"token_level_rewards\"], dtype=float, copy=True)\n    elif \"token_level_scores\" in batch:\n        token_level_rewards = np.array(batch[\"token_level_scores\"], dtype=float, copy=True)\n    else:\n        raise ValueError(\"DataProto.batch must contain 'token_level_rewards' or 'token_level_scores'.\")\n\n    # Required policy log-probabilities and mask\n    old_log_probs = np.array(batch[\"old_log_probs\"], dtype=float)\n    ref_log_probs = np.array(batch[\"ref_log_probs\"], dtype=float)\n    response_mask = np.array(batch[\"response_mask\"], dtype=float)\n\n    # Ensure shapes are compatible\n    if not (old_log_probs.shape == ref_log_probs.shape == response_mask.shape == token_level_rewards.shape):\n        raise ValueError(\"Shape mismatch among old_log_probs, ref_log_probs, response_mask, and token_level_rewards.\")\n\n    # Compute per-token KL penalty based on selected kl_penalty type\n    diff = old_log_probs - ref_log_probs  # log-prob difference per token\n\n    if kl_penalty == \"kl\":\n        penalty_per_token = diff\n    elif kl_penalty == \"abs\":\n        penalty_per_token = np.abs(diff)\n    elif kl_penalty == \"mse\":\n        penalty_per_token = diff ** 2\n    elif kl_penalty == \"low_var_kl\":\n        # Cap the penalty magnitude to reduce variance\n        cap = 0.25\n        penalty_per_token = np.where(np.abs(diff) > cap, cap * np.sign(diff), diff)\n    elif kl_penalty == \"full\":\n        # Approximate full KL using old policy probabilities\n        p_old = np.exp(old_log_probs)\n        penalty_per_token = p_old * (old_log_probs - ref_log_probs)\n    else:\n        raise ValueError(f\"Unknown kl_penalty type: {kl_penalty}\")\n\n    # Apply mask to ignore padding positions\n    penalty_per_token = penalty_per_token * response_mask\n\n    # Subtract KL penalty from original token-level rewards\n    token_level_rewards = token_level_rewards - penalty_per_token\n\n    # Update data.batch with penalized token-level rewards\n    batch[\"token_level_rewards\"] = token_level_rewards\n\n    # Compute sequence-level KL (sum of penalties per sequence) and batch-level KL (averaged over all valid tokens)\n    # seq_kl per sequence\n    seq_kl_per_seq = penalty_per_token.sum(axis=1)\n    # total number of valid tokens in each sequence\n    tokens_per_seq = response_mask.sum(axis=1)\n    tokens_per_seq = np.where(tokens_per_seq == 0, 1.0, tokens_per_seq)\n    per_seq_avg_kl = seq_kl_per_seq / tokens_per_seq\n    # batch-level KL: average of per-seq KL across the batch, weighted by token counts\n    total_tokens = response_mask.sum()\n    if total_tokens > 0:\n        batch_kl = penalty_per_token.sum() / total_tokens\n    else:\n        batch_kl = 0.0\n\n    current_kl = float(batch_kl)\n\n    # Update KL controller if possible\n    if hasattr(kl_ctrl, \"update\"):\n        try:\n            kl_ctrl.update(current_kl)\n        except TypeError:\n            # Fallback if update expects a different signature\n            kl_ctrl.update(current_kl, None)\n\n    # Retrieve current KL coefficient if available\n    kl_coef = getattr(kl_ctrl, \"kl_coef\", None)\n\n    metrics = {\n        \"critic/kl\": current_kl,\n        \"critic/kl_coef\": float(kl_coef) if kl_coef is not None else None,\n    }\n\n    return data, metrics"], "usage": {"prompt_tokens": 499, "completion_tokens": 5344, "total_tokens": 5843, "per_completion": [{"prompt_tokens": 499, "completion_tokens": 5344, "total_tokens": 5843}]}}
