{"function_name": "DataParallelPRIMERewardModel._forward_micro_batch", "overview": "Computes token-level reward scores for implicit process reward model", "function_signature": "def _forward_micro_batch(self, micro_batch, prompt_length):", "input_parameters": "- `micro_batch`: Micro-batch data {input_ids, attention_mask, position_ids, acc}\n- `prompt_length`: Prompt sequence length", "detailed_description": "First selects computation path based on whether padding removal optimization is enabled. For reward model and reference model, performs forward pass to compute output logits, then extracts log probabilities for corresponding tokens.\nImplicit rewards are obtained by computing log probability differences between the two models on response sequences.\nProcess reward generation supports two modes: direct mode and GAE mode.\nIn token granularity, each position receives corresponding reward value; in whole granularity, all rewards accumulate to the last valid token position.", "output": "- `token_level_scores`: Token-level reward scores (torch.Tensor, shape: [batch_size, seq_len])\n- `q_values`: Raw log probability differences (torch.Tensor, shape: [batch_size, seq_len])", "func_implementation": "", "test_code_path": "code/test_code/test_forward_micro_batch.py", "implementation_location": "code/recipe/prime/prime_dp_rm.py:line 51-228", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `DataParallelPRIMERewardModel._forward_micro_batch`.\n\n**Function Overview:**\nComputes token-level reward scores for implicit process reward model\n\n**Function Signature:**\n```python\ndef _forward_micro_batch(self, micro_batch, prompt_length):\n```\n\n**Input Parameters:**\n- `micro_batch`: Micro-batch data {input_ids, attention_mask, position_ids, acc}\n- `prompt_length`: Prompt sequence length\n\n**Detailed Description:**\nFirst selects computation path based on whether padding removal optimization is enabled. For reward model and reference model, performs forward pass to compute output logits, then extracts log probabilities for corresponding tokens.\nImplicit rewards are obtained by computing log probability differences between the two models on response sequences.\nProcess reward generation supports two modes: direct mode and GAE mode.\nIn token granularity, each position receives corresponding reward value; in whole granularity, all rewards accumulate to the last valid token position.\n\n**Expected Output:**\n- `token_level_scores`: Token-level reward scores (torch.Tensor, shape: [batch_size, seq_len])\n- `q_values`: Raw log probability differences (torch.Tensor, shape: [batch_size, seq_len])\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def _forward_micro_batch(self, micro_batch, prompt_length):\n    import torch\n\n    # Extract inputs from micro_batch\n    input_ids = micro_batch['input_ids']  # [B, S]\n    attention_mask = micro_batch.get('attention_mask', None)  # [B, S] or None\n    position_ids = micro_batch.get('position_ids', None)  # [B, S] or None\n\n    device = input_ids.device\n    batch_size, seq_len = input_ids.shape\n\n    def _extract_logits(out):\n        if isinstance(out, dict) and 'logits' in out:\n            return out['logits']\n        return out\n\n    def _forward(model):\n        # Try typical keyword-argument call first\n        try:\n            if (attention_mask is not None) and (position_ids is not None):\n                out = model(input_ids=input_ids, attention_mask=attention_mask, position_ids=position_ids)\n            elif attention_mask is not None:\n                out = model(input_ids=input_ids, attention_mask=attention_mask)\n            elif position_ids is not None:\n                out = model(input_ids=input_ids, position_ids=position_ids)\n            else:\n                out = model(input_ids=input_ids)\n        except TypeError:\n            # Fallback to positional arguments if needed\n            try:\n                if (attention_mask is not None) and (position_ids is not None):\n                    out = model(input_ids, attention_mask, position_ids)\n                elif attention_mask is not None:\n                    out = model(input_ids, attention_mask)\n                elif position_ids is not None:\n                    out = model(input_ids, position_ids)\n                else:\n                    out = model(input_ids)\n            except Exception:\n                # Last resort\n                out = model(input_ids)\n        return _extract_logits(out)\n\n    # Compute logits for reward model and reference model\n    logits_r = _forward(self.reward_model)\n    logits_ref = _forward(self.reference_model)\n\n    # Compute log probabilities\n    log_probs_r = torch.log_softmax(logits_r, dim=-1)  # [B, S, V]\n    log_probs_ref = torch.log_softmax(logits_ref, dim=-1)\n\n    # Gather log-probs for the actual tokens in the input sequence\n    # token_log_probs_{r,ref} shape: [B, S]\n    token_log_probs_r = log_probs_r.gather(dim=-1, index=input_ids.unsqueeze(-1)).squeeze(-1)\n    token_log_probs_ref = log_probs_ref.gather(dim=-1, index=input_ids.unsqueeze(-1)).squeeze(-1)\n\n    # Raw log-prob difference per token\n    q_values = token_log_probs_r - token_log_probs_ref  # [B, S]\n\n    # Create a mask for the response tokens (after prompt_length)\n    idx = torch.arange(seq_len, device=device)\n    resp_mask = idx >= int(prompt_length)  # [S]\n    resp_mask = resp_mask.unsqueeze(0).expand(batch_size, -1)  # [B, S]\n\n    # Determine mode: direct vs GAE\n    mode = 'direct'\n    if isinstance(self.config, dict):\n        mode = self.config.get('reward_mode', 'direct')\n    else:\n        mode = getattr(self, 'reward_mode', 'direct')\n\n    if mode == 'gae':\n        # Simple GAE-like accumulation over the response tokens\n        gamma = 0.99\n        if isinstance(self.config, dict):\n            gamma = self.config.get('gae_gamma', 0.99)\n\n        token_level_scores = torch.zeros_like(q_values)\n        running = torch.zeros(batch_size, device=device, dtype=q_values.dtype)\n\n        for t in range(seq_len - 1, -1, -1):\n            if int(t) >= int(prompt_length):\n                # accumulate discounted future rewards for response region\n                running = q_values[:, t] + gamma * running\n                token_level_scores[:, t] = running\n            else:\n                # before prompt: keep zero\n                token_level_scores[:, t] = 0.0\n                running = gamma * running  # propagate decay across non-response area\n\n        # Ensure non-response positions are zeroed\n        token_level_scores = token_level_scores * resp_mask.to(q_values.dtype)\n    else:\n        # Direct mode: token-level scores equal q_values on response tokens, zero elsewhere\n        token_level_scores = torch.zeros_like(q_values)\n        token_level_scores = torch.where(resp_mask, q_values, torch.zeros_like(q_values))\n\n    return token_level_scores, q_values"], "usage": {"prompt_tokens": 427, "completion_tokens": 6329, "total_tokens": 6756, "per_completion": [{"prompt_tokens": 427, "completion_tokens": 6329, "total_tokens": 6756}]}}
{"function_name": "compute_ce_dpo_loss_rm", "overview": "Computes cross-entropy style DPO loss", "function_signature": "def compute_ce_dpo_loss_rm(token_level_scores, acc, response_mask, beta):", "input_parameters": "- `token_level_scores`: Token-level rewards\n- `acc`: Binary accuracy labels\n- `response_mask`: Response mask\n- `beta`: Temperature parameter", "detailed_description": "Computes sequence-level scores, converts them to probabilities, and finally computes cross-entropy loss.", "output": "Scalar loss value", "func_implementation": "", "test_code_path": "code/test_code/test_ce_dpo_loss.py", "implementation_location": "code/recipe/prime/prime_core_algos.py:line 82-85", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_ce_dpo_loss_rm`.\n\n**Function Overview:**\nComputes cross-entropy style DPO loss\n\n**Function Signature:**\n```python\ndef compute_ce_dpo_loss_rm(token_level_scores, acc, response_mask, beta):\n```\n\n**Input Parameters:**\n- `token_level_scores`: Token-level rewards\n- `acc`: Binary accuracy labels\n- `response_mask`: Response mask\n- `beta`: Temperature parameter\n\n**Detailed Description:**\nComputes sequence-level scores, converts them to probabilities, and finally computes cross-entropy loss.\n\n**Expected Output:**\nScalar loss value\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def compute_ce_dpo_loss_rm(token_level_scores, acc, response_mask, beta):\n    \"\"\"\n    Computes a cross-entropy style DPO loss (reward modeling) using token-level scores.\n\n    Assumes:\n    - token_level_scores has shape [B, 2, L], representing per-token scores for two options.\n    - response_mask has shape [B, 2, L], with 1s for tokens belonging to the response and 0s otherwise.\n    - acc has shape [B], with binary values {0, 1} indicating the preferred option for each sample.\n    - beta is a scalar used to scale logits (acts as inverse temperature).\n\n    Returns:\n    - A scalar tensor representing the mean cross-entropy loss over the batch.\n    \"\"\"\n    import torch\n\n    # Ensure tensors\n    if not torch.is_tensor(token_level_scores):\n        token_level_scores = torch.tensor(token_level_scores)\n    if not torch.is_tensor(acc):\n        acc = torch.tensor(acc)\n    if not torch.is_tensor(response_mask):\n        response_mask = torch.tensor(response_mask)\n\n    device = token_level_scores.device\n    token_level_scores = token_level_scores.to(device)\n    response_mask = response_mask.to(device)\n    acc = acc.to(device)\n\n    # Validate expected shape\n    if token_level_scores.dim() != 3 or token_level_scores.shape[1] != 2:\n        raise ValueError(\"token_level_scores must have shape [B, 2, L]\")\n\n    if response_mask.shape != token_level_scores.shape:\n        raise ValueError(\"response_mask must have the same shape as token_level_scores\")\n\n    # Compute per-sequence (per-option) scores by summing masked token scores\n    # Result: [B, 2]\n    per_option_scores = (token_level_scores * response_mask).sum(dim=-1)\n\n    # Convert scores to probabilities with temperature scaling\n    # Using beta as inverse temperature: p = softmax(beta * score)\n    logits = per_option_scores  # shape [B, 2]\n    probs = torch.softmax(logits * beta, dim=-1)\n\n    # Gather the probability corresponding to the preferred option (acc in {0,1})\n    B = probs.shape[0]\n    target_indices = acc.view(-1).long()\n    target_indices = torch.clamp(target_indices, 0, 1)\n    selected_probs = probs[torch.arange(B, device=device), target_indices]\n\n    # Cross-entropy loss: -log(p[target])\n    eps = 1e-12\n    loss = -torch.log(selected_probs + eps)\n\n    return loss.mean()"], "usage": {"prompt_tokens": 313, "completion_tokens": 3487, "total_tokens": 3800, "per_completion": [{"prompt_tokens": 313, "completion_tokens": 3487, "total_tokens": 3800}]}}
{"function_name": "compute_detach_dpo_loss_rm", "overview": "Computes detached DPO loss, supporting BoN (Best-of-N) mode", "function_signature": "def compute_detach_dpo_loss_rm(token_level_scores, acc, Q_bc, acc_bc, response_mask, beta, bon_mode=\"none\"):", "input_parameters": "- Base: `token_level_scores`, `acc`, `response_mask`, `beta`\n- Contrast: `Q_bc` (batch Q-value matrix), `acc_bc` (batch accuracy matrix)\n- `bon_mode`: Weight mode {\"none\", \"bon_rm\", \"bon_acc\"}", "detailed_description": "Computes detached DPO loss based on contrastive samples, selecting samples with opposite accuracy labels within batch as contrastive baseline, and supports assigning higher training weights to high-quality samples through BoN weight mechanism.\nBoN weights include three modes: none mode, bon_rm mode, and bon_acc mode.", "output": "Weighted DPO loss value", "func_implementation": "", "test_code_path": "code/test_code/test_detach_dpo_loss.py", "implementation_location": "code/recipe/prime/prime_core_algos.py:line 88-116", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_detach_dpo_loss_rm`.\n\n**Function Overview:**\nComputes detached DPO loss, supporting BoN (Best-of-N) mode\n\n**Function Signature:**\n```python\ndef compute_detach_dpo_loss_rm(token_level_scores, acc, Q_bc, acc_bc, response_mask, beta, bon_mode=\"none\"):\n```\n\n**Input Parameters:**\n- Base: `token_level_scores`, `acc`, `response_mask`, `beta`\n- Contrast: `Q_bc` (batch Q-value matrix), `acc_bc` (batch accuracy matrix)\n- `bon_mode`: Weight mode {\"none\", \"bon_rm\", \"bon_acc\"}\n\n**Detailed Description:**\nComputes detached DPO loss based on contrastive samples, selecting samples with opposite accuracy labels within batch as contrastive baseline, and supports assigning higher training weights to high-quality samples through BoN weight mechanism.\nBoN weights include three modes: none mode, bon_rm mode, and bon_acc mode.\n\n**Expected Output:**\nWeighted DPO loss value\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def compute_detach_dpo_loss_rm(token_level_scores, acc, Q_bc, acc_bc, response_mask, beta, bon_mode=\"none\"):\n    \"\"\"\n    Compute detached DPO loss with Best-of-N (BoN) weighting.\n\n    This implementation follows a contrastive, pairwise DPO-like objective:\n    - For each pair (i, j) with opposite accuracy labels in the batch (acc[i] != acc[j]),\n      a binary cross-entropy loss is computed using a sigmoid on the Q-values:\n        p(i > j) = sigmoid(beta * Q_bc[i, j])\n      and the target is y(i > j) = 1 if acc[i] > acc[j], else 0.\n\n    - BoN weighting is applied across samples. Supported modes:\n      - \"none\": uniform weight\n      - \"bon_rm\": weight per-sample by softmax of per-sample scores (reward mean proxy)\n      - \"bon_acc\": weight per-sample by accuracy label (0 or 1)\n\n    - The Q_bc matrix is treated as detached to avoid backprop through Q-values.\n\n    Parameters:\n    - token_level_scores: Tensor of shape [N, L] or compatible; per-token scores.\n    - acc: Tensor of shape [N], binary accuracy labels (0 or 1).\n    - Q_bc: Tensor of shape [N, N], batch Q-values for pairwise comparisons.\n    - acc_bc: Tensor, not used in this implementation but kept for API compatibility.\n    - response_mask: Tensor of shape [N, L], 1 indicates token is part of response.\n    - beta: float, scaling factor for Q-values.\n    - bon_mode: one of {\"none\", \"bon_rm\", \"bon_acc\"}\n\n    Returns:\n    - loss: scalar tensor representing the weighted DPO loss (mean over considered pairs)\n    \"\"\"\n    import torch\n\n    device = acc.device if isinstance(acc, torch.Tensor) else torch.device(\"cpu\")\n    eps = 1e-7\n\n    # Ensure tensors\n    if not isinstance(acc, torch.Tensor):\n        acc = torch.tensor(acc, device=device, dtype=torch.float32)\n    if not isinstance(Q_bc, torch.Tensor):\n        Q_bc = torch.tensor(Q_bc, device=device, dtype=torch.float32)\n    if response_mask is None:\n        response_mask = torch.zeros(0, device=device)\n    if not isinstance(token_level_scores, torch.Tensor):\n        token_level_scores = torch.tensor(token_level_scores, device=device, dtype=torch.float32)\n\n    # Determine number of samples\n    N = acc.shape[0]\n\n    # Compute per-sample scores s_i from token_level_scores and response_mask\n    # Support common shapes: [N, L] or [N] or more; fallback to mean across tokens when possible.\n    with torch.no_grad():\n        if token_level_scores.dim() == 1:\n            s = token_level_scores.float()\n        elif token_level_scores.dim() == 2:\n            if response_mask.numel() > 0 and response_mask.shape[0] == token_level_scores.shape[0]:\n                mask = response_mask.float()\n                # Avoid division by zero\n                denom = mask.sum(dim=1) + 1e-6\n                s = (token_level_scores.float() * mask).sum(dim=1) / denom\n            else:\n                s = token_level_scores.mean(dim=1)\n        else:\n            # Generic fallback: average over last dim(s)\n            s = token_level_scores.mean(dim=list(range(token_level_scores.dim() - 1, token_level_scores.dim())))\n\n        s = s.to(dtype=torch.float32, device=device)\n\n    # BoN weights per sample\n    if bon_mode == \"bon_rm\":\n        # Use softmax over s as a proxy for goodness/reward mean\n        w_raw = torch.exp(s - torch.max(s, dim=0, keepdim=True)[0])\n        w = w_raw / (w_raw.sum() + 1e-12)\n        w = w.squeeze()\n    elif bon_mode == \"bon_acc\":\n        w = acc.float()\n    else:  # \"none\" or any other value\n        w = torch.ones(N, dtype=torch.float32, device=device)\n\n    # Prepare pairwise indices (i < j) to avoid double counting\n    idx_i, idx_j = torch.triu_indices(N, N, offset=1)\n\n    acc_i = acc[idx_i]\n    acc_j = acc[idx_j]\n\n    # Opposite-accuracy mask: only pairs with acc different\n    opp_mask = (acc_i != acc_j)\n\n    if opp_mask.sum() == 0:\n        # No contrasting pairs; return zero loss\n        return torch.tensor(0.0, device=device, dtype=torch.float32)\n\n    i_sel = idx_i[opp_mask]\n    j_sel = idx_j[opp_mask]\n\n    # True label: 1 if i has higher acc than j, else 0\n    y = (acc[i_sel] > acc[j_sel]).float()\n\n    # Detach Q-values to avoid gradient propagation through Q_bc\n    Q_detached = Q_bc.detach()\n\n    # Predicted probability that i is preferred over j\n    p = torch.sigmoid(beta * Q_detached[i_sel, j_sel])\n\n    # Binary cross-entropy with tiny epsilon for stability\n    loss_ij = - (y * torch.log(p + eps) + (1.0 - y) * torch.log(1.0 - p + eps))\n\n    # Pairwise weights\n    if bon_mode == \"bon_rm\":\n        w_pair = (w[i_sel] + w[j_sel]) / 2.0\n    elif bon_mode == \"bon_acc\":\n        w_pair = (acc[i_sel].float() + acc[j_sel].float()) / 2.0\n    else:\n        w_pair = torch.ones_like(loss_ij)\n\n    weighted_loss = w_pair * loss_ij\n\n    # Mean loss over all considered pairs\n    loss = weighted_loss.mean()\n\n    return loss"], "usage": {"prompt_tokens": 401, "completion_tokens": 8104, "total_tokens": 8505, "per_completion": [{"prompt_tokens": 401, "completion_tokens": 8104, "total_tokens": 8505}]}}
{"function_name": "compute_dpo_accuracy", "overview": "Computes pairwise comparison DPO accuracy", "function_signature": "def compute_dpo_accuracy(token_level_scores, acc, response_mask, n_samples):", "input_parameters": "- `token_level_scores`: Token-level reward scores\n- `acc`: Accuracy labels\n- `response_mask`: Response mask\n- `n_samples`: Sample count per group", "detailed_description": "Uses pairwise comparison to evaluate reward model accuracy. Aggregates token-level scores to sequence-level scores, constructs sample pairs for comparison. Judges directional consistency by computing score differences and accuracy differences.\nUses weighted average to compute accuracy, with higher weights for sample pairs with larger accuracy differences. Returns 0.5 when all samples have the same accuracy.", "output": "Weighted pairwise comparison accuracy", "func_implementation": "", "test_code_path": "code/test_code/test_dpo_accuracy.py", "implementation_location": "code/recipe/prime/prime_core_algos.py:line 119-143", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_dpo_accuracy`.\n\n**Function Overview:**\nComputes pairwise comparison DPO accuracy\n\n**Function Signature:**\n```python\ndef compute_dpo_accuracy(token_level_scores, acc, response_mask, n_samples):\n```\n\n**Input Parameters:**\n- `token_level_scores`: Token-level reward scores\n- `acc`: Accuracy labels\n- `response_mask`: Response mask\n- `n_samples`: Sample count per group\n\n**Detailed Description:**\nUses pairwise comparison to evaluate reward model accuracy. Aggregates token-level scores to sequence-level scores, constructs sample pairs for comparison. Judges directional consistency by computing score differences and accuracy differences.\nUses weighted average to compute accuracy, with higher weights for sample pairs with larger accuracy differences. Returns 0.5 when all samples have the same accuracy.\n\n**Expected Output:**\nWeighted pairwise comparison accuracy\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def compute_dpo_accuracy(token_level_scores, acc, response_mask, n_samples):\n    \"\"\"\n    Compute weighted pairwise comparison DPO accuracy.\n\n    Process:\n    - Aggregate token-level scores to sequence-level scores using the response mask.\n    - Within each group of size n_samples, form all unordered pairs (i, j).\n    - For each pair, compute delta_acc = acc[i] - acc[j] and delta_score = score[i] - score[j].\n    - If delta_acc != 0, evaluate if the sign(delta_score) matches sign(delta_acc).\n      If so, pair contributes 1; otherwise 0. Weight by |delta_acc|.\n    - Return weighted accuracy = sum(weight * correct) / sum(weight).\n    - If all samples have the same accuracy (no contributing pairs), return 0.5.\n\n    Args:\n        token_level_scores: Tensor or array-like of token-level reward scores with shape (N, L).\n        acc: Tensor or array-like of accuracy labels with length N.\n        response_mask: Tensor or array-like mask with shape (N, L). Non-zero values are treated as valid tokens.\n        n_samples: Integer, number of samples per group for pairwise comparison.\n\n    Returns:\n        A float representing the weighted pairwise DPO accuracy.\n    \"\"\"\n    import torch\n\n    # Convert inputs to tensors if needed\n    tls = token_level_scores\n    if not isinstance(tls, torch.Tensor):\n        tls = torch.tensor(tls)\n\n    rm = response_mask\n    if not isinstance(rm, torch.Tensor):\n        rm = torch.tensor(rm)\n\n    a = acc\n    if not isinstance(a, torch.Tensor):\n        a = torch.tensor(a)\n\n    # Basic shape checks (2D token scores and mask)\n    if tls.dim() != 2 or rm.dim() != 2:\n        raise ValueError(\"token_level_scores and response_mask must be 2D tensors with shape (N, L)\")\n\n    N = tls.shape[0]\n    if rm.shape[0] != N:\n        raise ValueError(\"response_mask's first dimension must match token_level_scores' first dimension\")\n    if a.shape[0] != N:\n        raise ValueError(\"acc length must match number of samples in token_level_scores\")\n\n    # Aggregate token-level scores to sequence-level scores using the mask\n    # seq_score[i] = sum_j tls[i, j] * rm[i, j]\n    seq_scores = (tls * rm).sum(dim=1)\n\n    # Number of complete groups\n    if n_samples <= 0:\n        raise ValueError(\"n_samples must be a positive integer\")\n\n    ngroups = N // n_samples\n    if ngroups == 0:\n        # Not enough samples to form a group\n        return 0.5\n\n    weighted_correct = 0.0\n    weighted_total = 0.0\n\n    # Iterate over groups and accumulate pairwise results\n    for g in range(ngroups):\n        base = g * n_samples\n        for i in range(n_samples - 1):\n            idx_i = base + i\n            for j in range(i + 1, n_samples):\n                idx_j = base + j\n\n                delta_acc = float(a[idx_i].item() - a[idx_j].item())\n                delta_score = float(seq_scores[idx_i].item() - seq_scores[idx_j].item())\n\n                weight = abs(delta_acc)\n                if weight > 0.0:\n                    # Check directional consistency: delta_score and delta_acc should have the same sign\n                    if (delta_score > 0 and delta_acc > 0) or (delta_score < 0 and delta_acc < 0):\n                        correct = 1.0\n                    else:\n                        correct = 0.0\n                    weighted_correct += weight * correct\n                    weighted_total += weight\n\n    if weighted_total == 0.0:\n        return 0.5\n\n    return weighted_correct / weighted_total"], "usage": {"prompt_tokens": 363, "completion_tokens": 4478, "total_tokens": 4841, "per_completion": [{"prompt_tokens": 363, "completion_tokens": 4478, "total_tokens": 4841}]}}
{"function_name": "compute_dpo_abs_accuracy", "overview": "Computes absolute accuracy metric", "function_signature": "def compute_dpo_abs_accuracy(token_level_scores, acc, response_mask, n_samples):", "input_parameters": "- `token_level_scores`: Token-level reward scores\n- `acc`: Accuracy labels\n- `response_mask`: Response mask", "detailed_description": "Converts aggregated token-level scores to predicted labels, compares with true labels to compute absolute accuracy.", "output": "Absolute classification accuracy", "func_implementation": "", "test_code_path": "code/test_code/test_dpo_abs_accuracy.py", "implementation_location": "code/recipe/prime/prime_core_algos.py:line 146-147", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_dpo_abs_accuracy`.\n\n**Function Overview:**\nComputes absolute accuracy metric\n\n**Function Signature:**\n```python\ndef compute_dpo_abs_accuracy(token_level_scores, acc, response_mask, n_samples):\n```\n\n**Input Parameters:**\n- `token_level_scores`: Token-level reward scores\n- `acc`: Accuracy labels\n- `response_mask`: Response mask\n\n**Detailed Description:**\nConverts aggregated token-level scores to predicted labels, compares with true labels to compute absolute accuracy.\n\n**Expected Output:**\nAbsolute classification accuracy\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def compute_dpo_abs_accuracy(token_level_scores, acc, response_mask, n_samples):\n    \"\"\"\n    Computes absolute accuracy by aggregating token-level scores with a response mask\n    and comparing the resulting predicted labels to the ground-truth labels.\n\n    Args:\n        token_level_scores: Token-level reward scores. Can be a numpy array or a torch tensor\n                            of shapes like (n_samples, n_tokens) or (n_samples, n_tokens, d),\n                            etc.\n        acc: Ground-truth accuracy labels (0/1 or probabilities). Can be numpy array or tensor.\n        response_mask: Mask indicating which tokens belong to the response. Shape should be\n                       compatible with token_level_scores (e.g., (n_samples, n_tokens) or\n                       broadcastable to its shape).\n        n_samples: Maximum number of samples to include in the accuracy computation.\n\n    Returns:\n        float: Absolute classification accuracy over the first n_samples samples.\n    \"\"\"\n    import numpy as np\n\n    # Helper to convert to numpy, handling optional torch tensors\n    def to_numpy(x):\n        try:\n            import torch\n        except Exception:\n            torch = None\n        if torch is not None and isinstance(x, torch.Tensor):\n            return x.detach().cpu().numpy()\n        if isinstance(x, np.ndarray):\n            return x\n        return np.asarray(x)\n\n    ts = to_numpy(token_level_scores)\n    rm = to_numpy(response_mask)\n    a = to_numpy(acc)\n\n    # If inputs are not provided, return 0.0\n    if ts is None:\n        return 0.0\n\n    # Compute per-sample score by aggregating token-level scores with the mask\n    if ts.ndim == 2:\n        if rm is not None and rm.ndim == 2:\n            score = (ts * rm).sum(axis=1)\n        else:\n            score = ts.mean(axis=1)\n    elif ts.ndim == 3:\n        if rm is not None and rm.ndim == 2:\n            score = (ts * rm[:, :, None]).sum(axis=1)\n        elif rm is not None and rm.ndim == 3:\n            score = (ts * rm).sum(axis=(1, 2))\n        else:\n            score = ts.mean(axis=1)\n    else:\n        score = ts.mean(axis=-1)\n\n    # Predicted label from aggregated score (threshold at 0)\n    pred = (score > 0).astype(int)\n\n    # Flatten ground-truth labels\n    a_flat = a.reshape(-1) if a.ndim > 1 else a\n    pred_flat = pred.reshape(-1)\n\n    # Convert ground-truth to binary if needed\n    if a_flat.dtype.kind in 'f':\n        a_bin = (a_flat > 0.5).astype(int)\n    else:\n        a_bin = a_flat.astype(int)\n\n    # Determine effective number of samples to evaluate\n    N = int(min(n_samples, pred_flat.shape[0], a_bin.shape[0]))\n    if N <= 0:\n        return 0.0\n\n    correct = (pred_flat[:N] == a_bin[:N]).astype(np.float64)\n    accuracy = float(correct.sum()) / max(1, N)\n    return accuracy"], "usage": {"prompt_tokens": 301, "completion_tokens": 4222, "total_tokens": 4523, "per_completion": [{"prompt_tokens": 301, "completion_tokens": 4222, "total_tokens": 4523}]}}
{"function_name": "compute_rloo_advantage_return", "overview": "Implements RLOO advantage estimation, supporting multiple reward source fusion", "function_signature": "def compute_rloo_advantage_return(data: verl.DataProto, response_mask: torch.Tensor, n_samples, config):", "input_parameters": "- `data`: Data protocol object containing DataProto.batch fields\n- `data.batch[\"rm_scores\"]`: Implicit reward scores\n- `data.batch[\"acc\"]`: Binary accuracy labels\n- `data.batch[\"prompts\"]`: Input prompt sequences\n- `data.batch[\"attention_mask\"]`: Attention mask\n- `response_mask`: Response mask indicating valid token positions\n- `n_samples`: Sample count per group, used for RLOO grouping computation\n- `config`: Reward weight configuration, hierarchical configuration object\n- `config.algorithm.reward_dpo_coef`: DPO reward weight coefficient, default 5.0\n- `config.algorithm.reward_gt_coef`: Accuracy reward weight coefficient, default 5.0", "detailed_description": "Function supports advantage computation with multi-source reward fusion. Reward sources include reward model scores and accuracy rewards.\nGroups multiple responses generated from the same prompt by n_samples intervals.\nUses RLOO to compute advantage function for each reward source and fuses multiple reward sources, computes cumulative return values after reward fusion and applies whitening.", "output": "- `advantages`: Whitened advantage function (torch.Tensor, shape: [batch_size, seq_len])\n- `returns`: Cumulative return values (torch.Tensor, shape: [batch_size, seq_len])", "func_implementation": "", "test_code_path": "code/test_code/test_rloo_advantage_return.py", "implementation_location": "code/recipe/prime/prime_core_algos.py:line 21-79", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_rloo_advantage_return`.\n\n**Function Overview:**\nImplements RLOO advantage estimation, supporting multiple reward source fusion\n\n**Function Signature:**\n```python\ndef compute_rloo_advantage_return(data: verl.DataProto, response_mask: torch.Tensor, n_samples, config):\n```\n\n**Input Parameters:**\n- `data`: Data protocol object containing DataProto.batch fields\n- `data.batch[\"rm_scores\"]`: Implicit reward scores\n- `data.batch[\"acc\"]`: Binary accuracy labels\n- `data.batch[\"prompts\"]`: Input prompt sequences\n- `data.batch[\"attention_mask\"]`: Attention mask\n- `response_mask`: Response mask indicating valid token positions\n- `n_samples`: Sample count per group, used for RLOO grouping computation\n- `config`: Reward weight configuration, hierarchical configuration object\n- `config.algorithm.reward_dpo_coef`: DPO reward weight coefficient, default 5.0\n- `config.algorithm.reward_gt_coef`: Accuracy reward weight coefficient, default 5.0\n\n**Detailed Description:**\nFunction supports advantage computation with multi-source reward fusion. Reward sources include reward model scores and accuracy rewards.\nGroups multiple responses generated from the same prompt by n_samples intervals.\nUses RLOO to compute advantage function for each reward source and fuses multiple reward sources, computes cumulative return values after reward fusion and applies whitening.\n\n**Expected Output:**\n- `advantages`: Whitened advantage function (torch.Tensor, shape: [batch_size, seq_len])\n- `returns`: Cumulative return values (torch.Tensor, shape: [batch_size, seq_len])\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def compute_rloo_advantage_return(data: verl.DataProto, response_mask: torch.Tensor, n_samples, config):\n    \"\"\"\n    Compute RLOO-based advantages and returns with multi-source reward fusion.\n\n    - Rewards sources fused: DPO scores (rm_scores) and accuracy (acc)\n    - Group responses by n_samples to form prompt-driven groups\n    - For each sample within a group, compute Leave-One-Out baseline across the group\n    - Compute returns as discounted sum of fused rewards per token\n    - Apply whitening to the advantages across valid (response) tokens\n\n    Returns:\n        advantages (torch.Tensor): whitened per-token advantages, shape [batch_size, seq_len]\n        returns (torch.Tensor): per-token discounted returns, shape [batch_size, seq_len]\n    \"\"\"\n    import torch\n\n    device = None\n    # Extract base tensors from data, ensuring float for computations\n    rm_scores = data.batch[\"rm_scores\"].to(torch.float32)  # implicit reward scores\n    acc = data.batch[\"acc\"].to(torch.float32)            # accuracy rewards\n\n    # Ensure same device as inputs\n    device = rm_scores.device\n\n    # Reward fusion coefficients with sensible defaults\n    dpo_coef = getattr(getattr(config, \"algorithm\", None), \"reward_dpo_coef\", 5.0)\n    gt_coef = getattr(getattr(config, \"algorithm\", None), \"reward_gt_coef\", 5.0)\n\n    # Fuse reward sources\n    fused_reward = dpo_coef * rm_scores + gt_coef * acc  # shape: [batch_size, seq_len]\n\n    batch_size, seq_len = fused_reward.shape\n\n    # Prepare outputs\n    advantages = torch.zeros_like(fused_reward)\n    returns = torch.zeros_like(fused_reward)\n\n    gamma = 0.99  # discount factor; reasonable default\n\n    # Normalize n_samples to int >= 1\n    grp_size = int(n_samples) if n_samples is not None else 1\n    if grp_size < 1:\n        grp_size = 1\n\n    # Ensure response_mask is binary/int for arithmetic\n    if response_mask.dtype != torch.bool:\n        resp_mask_bin = response_mask.bool()\n    else:\n        resp_mask_bin = response_mask\n\n    # Compute per-group RLOO advantages and per-token returns\n    i = 0\n    while i < batch_size:\n        j = min(batch_size, i + grp_size)\n        group_indices = list(range(i, j))  # inclusive-exclusive\n\n        # Gather group masks and rewards\n        group_mask = resp_mask_bin[group_indices]  # [gs, seq_len]\n        group_rewards = fused_reward[group_indices]  # [gs, seq_len]\n\n        # Per-token group sums and counts (to form leave-one-out baselines)\n        group_sum = (group_rewards * group_mask).sum(dim=0)  # [seq_len]\n        group_count = group_mask.sum(dim=0).clamp(min=1)  # [seq_len]\n\n        for idx_local, idx in enumerate(group_indices):\n            mask_i = group_mask[idx_local]  # [seq_len]\n            # Baseline for each token: average of other samples in the group\n            denom = group_count - mask_i  # [seq_len]\n            denom = denom.clamp(min=1e-6)\n\n            baseline_i = (group_sum - group_rewards[idx_local] * mask_i) / denom  # [seq_len]\n\n            # Leave-one-out advantage for this sample at each token\n            adv_i = group_rewards[idx_local] * mask_i - baseline_i  # [seq_len]\n            advantages[idx] = adv_i\n\n            # Compute returns (discounted fused reward) per token within valid response region\n            running = 0.0\n            r = torch.zeros(seq_len, device=device, dtype=fused_reward.dtype)\n            # Iterate from end to start; reset when encountering non-response tokens\n            for t in range(seq_len - 1, -1, -1):\n                if mask_i[t]:\n                    running = group_rewards[idx_local, t] + gamma * running\n                    r[t] = running\n                else:\n                    running = 0.0\n                    r[t] = 0.0\n            returns[idx] = r\n\n        i = j\n\n    # Whitening (standardize) the advantages across all valid tokens\n    valid_token_mask = resp_mask_bin  # [batch_size, seq_len]\n    adv_valid = advantages[valid_token_mask]  # 1D view of valid positions\n\n    if adv_valid.numel() > 0:\n        mean = adv_valid.mean()\n        std = adv_valid.std(unbiased=False)\n        denom = std if std > 0 else 1.0\n        # Apply whitening in-place to all valid positions\n        advantages[valid_token_mask] = (advantages[valid_token_mask] - mean) / (denom + 1e-6)\n\n    return advantages, returns"], "usage": {"prompt_tokens": 519, "completion_tokens": 5642, "total_tokens": 6161, "per_completion": [{"prompt_tokens": 519, "completion_tokens": 5642, "total_tokens": 6161}]}}
{"function_name": "RayPRIMETrainer.filter_and_downsample", "overview": "Implements intelligent sample filtering and priority downsampling for PRIME algorithm", "function_signature": "def filter_and_downsample(self, scores, batch: DataProto):", "input_parameters": "- `scores`: Verification score list, binary accuracy labels\n- `batch`: Data batch object\n- `config`: Filtering parameter configuration", "detailed_description": "Filters high-quality samples from oversampled candidate responses. Reconstructs verification scores into 2D matrix, performs dual filtering: accuracy filtering and length filtering.\nAccuracy filtering retains sample groups with average accuracy within configured range; length filtering removes sample groups exceeding maximum length threshold.\nSample groups meeting filtering conditions are retained with priority, ultimately retaining 1/oversample_factor proportion of samples. If all groups fail to meet filtering conditions, algorithm still retains corresponding proportion of samples to ensure training continues.", "output": "Filtered high-quality data batch", "func_implementation": "", "test_code_path": "code/test_code/test_filter_and_downsample.py", "implementation_location": "code/recipe/prime/prime_ray_trainer.py:line 540-572", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `RayPRIMETrainer.filter_and_downsample`.\n\n**Function Overview:**\nImplements intelligent sample filtering and priority downsampling for PRIME algorithm\n\n**Function Signature:**\n```python\ndef filter_and_downsample(self, scores, batch: DataProto):\n```\n\n**Input Parameters:**\n- `scores`: Verification score list, binary accuracy labels\n- `batch`: Data batch object\n- `config`: Filtering parameter configuration\n\n**Detailed Description:**\nFilters high-quality samples from oversampled candidate responses. Reconstructs verification scores into 2D matrix, performs dual filtering: accuracy filtering and length filtering.\nAccuracy filtering retains sample groups with average accuracy within configured range; length filtering removes sample groups exceeding maximum length threshold.\nSample groups meeting filtering conditions are retained with priority, ultimately retaining 1/oversample_factor proportion of samples. If all groups fail to meet filtering conditions, algorithm still retains corresponding proportion of samples to ensure training continues.\n\n**Expected Output:**\nFiltered high-quality data batch\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def filter_and_downsample(self, scores, batch: DataProto):\n    \"\"\"\n    Ray PRIME style intelligent filtering and downsampling.\n\n    - Reconstructs scores into a 2D matrix with shape (num_groups, oversample_factor)\n    - Performs dual filtering: accuracy (group-wise average) and length (group max length)\n    - Keeps exactly one sample per group (i.e., 1/oversample_factor of the original samples)\n    - If necessary, falls back to keeping a representative per group to preserve training progress\n\n    Returns:\n        A new DataProto batch containing the selected (subset) samples.\n    \"\"\"\n    import numpy as np\n\n    # Helper to fetch config params robustly from self.config (could be dict-like or object)\n    def _get_config_param(param_name, default_value):\n        cfg = getattr(self, 'config', None)\n        if cfg is None:\n            return default_value\n        if isinstance(cfg, dict):\n            return cfg.get(param_name, default_value)\n        return getattr(cfg, param_name, default_value)\n\n    # Configuration with sane defaults\n    oversample_factor = int(_get_config_param('oversample_factor', 4))\n    if oversample_factor <= 0:\n        oversample_factor = 1\n\n    min_acc = float(_get_config_param('min_accuracy', 0.6))\n    max_acc = float(_get_config_param('max_accuracy', 0.95))\n    max_group_len = int(_get_config_param('max_group_len', 256))\n\n    # If no scores, return batch as-is\n    if scores is None:\n        return batch\n\n    # Flatten scores to a 2D matrix of shape (G, oversample_factor)\n    scores_list = list(scores)\n    total_scores = len(scores_list)\n\n    # Determine number of groups G\n    G = total_scores // oversample_factor if oversample_factor > 0 else total_scores\n    if G <= 0:\n        G = 1\n        oversample_factor = max(1, total_scores)\n\n    total_needed = G * oversample_factor\n    # Pad or trim scores to fit (G x oversample_factor)\n    if total_scores < total_needed:\n        # pad with last value to avoid shape issues\n        pad_val = scores_list[-1] if scores_list else 0.0\n        scores_list = scores_list + [pad_val] * (total_needed - total_scores)\n    else:\n        scores_list = scores_list[:total_needed]\n\n    scores_mat = np.asarray(scores_list, dtype=float).reshape(G, oversample_factor)\n\n    # Group-level accuracy: average of each row\n    group_mean_acc = scores_mat.mean(axis=1)\n\n    # Length filtering: try to use per-sample lengths from batch\n    length_attr_candidates = ['lengths', 'sample_lengths', 'group_lengths']\n    per_sample_lengths = None\n    for attr in length_attr_candidates:\n        if hasattr(batch, attr):\n            per_sample_lengths = getattr(batch, attr)\n            break\n\n    # Build length_pass mask per group\n    if per_sample_lengths is not None:\n        lengths_flat = list(per_sample_lengths)\n        # Ensure the lengths correspond to the total number of scores\n        if len(lengths_flat) >= total_needed:\n            lengths_flat = lengths_flat[:total_needed]\n            lengths_mat = np.asarray(lengths_flat, dtype=int).reshape(G, oversample_factor)\n            max_lengths_per_group = lengths_mat.max(axis=1)\n            length_pass = max_lengths_per_group <= max_group_len\n        else:\n            # If we can't align lengths, disable length filtering for safety\n            length_pass = np.ones(G, dtype=bool)\n    else:\n        # No length data available; skip length filtering\n        length_pass = np.ones(G, dtype=bool)\n\n    # Accuracy filtering\n    acc_pass = (group_mean_acc >= min_acc) & (group_mean_acc <= max_acc)\n\n    # Combined pass mask (not strictly used for final selection, but available for clarity)\n    pass_mask = acc_pass & length_pass\n\n    # Decide which sample to keep in each group.\n    # Strategy: keep exactly one sample per group (1 / oversample_factor of originals).\n    # Within each group, select the sample with the highest score (argmax) to bias towards quality.\n    selected_indices = []\n    for g in range(G):\n        row = scores_mat[g, :]\n        best_local_idx = int(np.argmax(row))\n        global_idx = g * oversample_factor + best_local_idx\n        selected_indices.append(global_idx)\n\n    # Subset the batch to only include selected indices\n    def _subset_batch_by_indices(b, indices):\n        # Try common API hooks first\n        if hasattr(b, 'subset'):\n            return b.subset(indices)\n        if hasattr(b, 'filter_by_indices'):\n            return b.filter_by_indices(indices)\n\n        # Fallback: attempt to create a new instance of the same class and copy fields\n        try:\n            new_batch = b.__class__()\n        except Exception:\n            # If we cannot instantiate a new batch, return the original as a safe fallback\n            return b\n\n        if not isinstance(indices, (list, tuple, np.ndarray)):\n            indices = list(indices)\n\n        # Copy over attributes, filtering list-like fields by indices when possible\n        for key, val in getattr(b, '__dict__', {}).items():\n            try:\n                if isinstance(val, np.ndarray):\n                    new_batch.__dict__[key] = val[indices]\n                elif isinstance(val, (list, tuple)):\n                    filtered = [val[i] for i in indices if i < len(val)]\n                    new_batch.__dict__[key] = filtered if isinstance(val, list) else tuple(filtered)\n                else:\n                    # For non-sequence fields, keep as-is or try a shallow copy\n                    new_batch.__dict__[key] = val\n            except Exception:\n                # If any field can't be subset, just copy non-subset\n                new_batch.__dict__[key] = val\n        return new_batch\n\n    new_batch = _subset_batch_by_indices(batch, selected_indices)\n    return new_batch"], "usage": {"prompt_tokens": 388, "completion_tokens": 8011, "total_tokens": 8399, "per_completion": [{"prompt_tokens": 388, "completion_tokens": 8011, "total_tokens": 8399}]}}
