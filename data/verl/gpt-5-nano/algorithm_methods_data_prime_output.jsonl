{"function_name": "DataParallelPRIMERewardModel._forward_micro_batch", "overview": "Computes token-level reward scores for implicit process reward model", "function_signature": "def _forward_micro_batch(self, micro_batch, prompt_length):", "input_parameters": "- `micro_batch`: Micro-batch data {input_ids, attention_mask, position_ids, acc}\n- `prompt_length`: Prompt sequence length", "detailed_description": "First selects computation path based on whether padding removal optimization is enabled. For reward model and reference model, performs forward pass to compute output logits, then extracts log probabilities for corresponding tokens.\nImplicit rewards are obtained by computing log probability differences between the two models on response sequences.\nProcess reward generation supports two modes: direct mode and GAE mode.\nIn token granularity, each position receives corresponding reward value; in whole granularity, all rewards accumulate to the last valid token position.", "output": "- `token_level_scores`: Token-level reward scores (torch.Tensor, shape: [batch_size, seq_len])\n- `q_values`: Raw log probability differences (torch.Tensor, shape: [batch_size, seq_len])", "func_implementation": "", "test_code_path": "code/test_code/test_forward_micro_batch.py", "implementation_location": "code/recipe/prime/prime_dp_rm.py:line 51-228", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `DataParallelPRIMERewardModel._forward_micro_batch`.\n\n**Function Overview:**\nComputes token-level reward scores for implicit process reward model\n\n**Function Signature:**\n```python\ndef _forward_micro_batch(self, micro_batch, prompt_length):\n```\n\n**Input Parameters:**\n- `micro_batch`: Micro-batch data {input_ids, attention_mask, position_ids, acc}\n- `prompt_length`: Prompt sequence length\n\n**Detailed Description:**\nFirst selects computation path based on whether padding removal optimization is enabled. For reward model and reference model, performs forward pass to compute output logits, then extracts log probabilities for corresponding tokens.\nImplicit rewards are obtained by computing log probability differences between the two models on response sequences.\nProcess reward generation supports two modes: direct mode and GAE mode.\nIn token granularity, each position receives corresponding reward value; in whole granularity, all rewards accumulate to the last valid token position.\n\n**Expected Output:**\n- `token_level_scores`: Token-level reward scores (torch.Tensor, shape: [batch_size, seq_len])\n- `q_values`: Raw log probability differences (torch.Tensor, shape: [batch_size, seq_len])\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def _forward_micro_batch(self, micro_batch, prompt_length):\n    \"\"\"\n    Compute token-level reward scores and raw log-probability differences for a micro-batch.\n\n    Returns:\n        token_level_scores: Tensor of shape [batch_size, seq_len]\n        q_values: Tensor of shape [batch_size, seq_len]\n    \"\"\"\n    import torch\n\n    # Extract batch components\n    input_ids = micro_batch['input_ids']          # [B, S]\n    attention_mask = micro_batch.get('attention_mask', None)  # [B, S] or None\n    position_ids = micro_batch.get('position_ids', None)      # [B, S] or None\n\n    B, S = input_ids.shape\n\n    # Fallback attention_mask if not provided\n    if attention_mask is None:\n        if hasattr(self, 'config') and hasattr(self.config, 'pad_token_id'):\n            pad_id = self.config.pad_token_id\n            attention_mask = (input_ids != pad_id).long()\n        else:\n            attention_mask = torch.ones((B, S), dtype=torch.long, device=input_ids.device)\n\n    # Decide on padding-removal optimization path (best-effort, implement core logic robustly)\n    remove_padding = False\n    if hasattr(self, 'config') and hasattr(self.config, 'remove_padding'):\n        remove_padding = bool(self.config.remove_padding)\n\n    # Retrieve reward and reference models (naming may vary across implementations)\n    reward_model = getattr(self, 'reward_model', None)\n    reference_model = getattr(self, 'reference_model', None)\n    if reward_model is None or reference_model is None:\n        # Fallback aliases\n        reward_model = getattr(self, 'prime_reward_model', reward_model)\n        reference_model = getattr(self, 'prime_reference_model', reference_model)\n\n    if reward_model is None or reference_model is None:\n        raise RuntimeError(\"Reward model and reference model must be configured on DataParallelPRIMERewardModel.\")\n\n    # Forward through both models to obtain logits\n    with torch.no_grad():\n        # Helper to run a forward pass and extract logits robustly\n        def _get_logits(model, batch):\n            # Try common transformers-style forward\n            try:\n                out = model(**batch, return_dict=True)\n                logits = getattr(out, 'logits', None)\n                if logits is None and isinstance(out, tuple):\n                    logits = out[0]\n            except TypeError:\n                # Fallback: call with positional args\n                try:\n                    logits = model(batch['input_ids'], attention_mask=batch.get('attention_mask'), position_ids=batch.get('position_ids')).logits\n                except Exception as e:\n                    # Last resort: raise to surface misconfiguration\n                    raise e\n            return logits\n\n        logits_r = _get_logits(reward_model, {'input_ids': input_ids,\n                                             'attention_mask': attention_mask,\n                                             'position_ids': position_ids})\n        logits_ref = _get_logits(reference_model, {'input_ids': input_ids,\n                                                  'attention_mask': attention_mask,\n                                                  'position_ids': position_ids})\n\n    # Compute log probabilities\n    # Guard against None logits in edge cases\n    if logits_r is None or logits_ref is None:\n        raise RuntimeError(\"Failed to obtain logits from reward or reference model.\")\n\n    log_probs_r = torch.nn.functional.log_softmax(logits_r, dim=-1)  # [B, S, V]\n    log_probs_ref = torch.nn.functional.log_softmax(logits_ref, dim=-1)\n\n    # Gather log probability of the actual tokens at each position\n    # input_ids used as the target tokens\n    gathered_r = log_probs_r.gather(dim=-1, index=input_ids.unsqueeze(-1)).squeeze(-1)  # [B, S]\n    gathered_ref = log_probs_ref.gather(dim=-1, index=input_ids.unsqueeze(-1)).squeeze(-1)  # [B, S]\n\n    # Apply token validity mask\n    valid = attention_mask > 0  # [B, S], bool\n    gathered_r = gathered_r * valid\n    gathered_ref = gathered_ref * valid\n\n    # Log-prob differences per position (response tokens only)\n    q_values = gathered_r - gathered_ref  # [B, S]\n\n    # Only consider response positions after prompt_length for rewards\n    if isinstance(prompt_length, torch.Tensor):\n        prompt_length = int(prompt_length.item())\n    else:\n        prompt_length = int(prompt_length)\n\n    resp_mask = torch.zeros_like(attention_mask, dtype=torch.bool)\n    if prompt_length < S:\n        resp_mask[:, prompt_length:] = True\n    valid_resp = valid & resp_mask  # [B, S]\n\n    # Token-level reward scores initialization\n    token_level_scores = torch.zeros_like(q_values)\n\n    # Granularity handling\n    granularity = getattr(self.config, 'granularity', 'token')\n    if granularity == 'token':\n        token_level_scores = q_values * valid_resp  # per-token rewards for response tokens\n    elif granularity == 'whole':\n        # Accumulate all rewards to the last valid token position per batch\n        token_level_scores = torch.zeros_like(q_values)\n        total_rewards = (q_values * valid).sum(dim=1)  # [B]\n        last_valid_idx = valid.sum(dim=1) - 1  # [B], -1 if none\n        for i in range(B):\n            lv = int(last_valid_idx[i].item())\n            if lv >= 0:\n                token_level_scores[i, lv] = total_rewards[i]\n    else:\n        # Default to token granularity if an unknown mode is provided\n        token_level_scores = q_values * valid_resp\n\n    return token_level_scores, q_values"], "usage": {"prompt_tokens": 427, "completion_tokens": 6937, "total_tokens": 7364, "per_completion": [{"prompt_tokens": 427, "completion_tokens": 6937, "total_tokens": 7364}]}}
{"function_name": "compute_ce_dpo_loss_rm", "overview": "Computes cross-entropy style DPO loss", "function_signature": "def compute_ce_dpo_loss_rm(token_level_scores, acc, response_mask, beta):", "input_parameters": "- `token_level_scores`: Token-level rewards\n- `acc`: Binary accuracy labels\n- `response_mask`: Response mask\n- `beta`: Temperature parameter", "detailed_description": "Computes sequence-level scores, converts them to probabilities, and finally computes cross-entropy loss.", "output": "Scalar loss value", "func_implementation": "", "test_code_path": "code/test_code/test_ce_dpo_loss.py", "implementation_location": "code/recipe/prime/prime_core_algos.py:line 82-85", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_ce_dpo_loss_rm`.\n\n**Function Overview:**\nComputes cross-entropy style DPO loss\n\n**Function Signature:**\n```python\ndef compute_ce_dpo_loss_rm(token_level_scores, acc, response_mask, beta):\n```\n\n**Input Parameters:**\n- `token_level_scores`: Token-level rewards\n- `acc`: Binary accuracy labels\n- `response_mask`: Response mask\n- `beta`: Temperature parameter\n\n**Detailed Description:**\nComputes sequence-level scores, converts them to probabilities, and finally computes cross-entropy loss.\n\n**Expected Output:**\nScalar loss value\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def compute_ce_dpo_loss_rm(token_level_scores, acc, response_mask, beta):\n    \"\"\"\n    Computes cross-entropy style DPO loss.\n\n    Steps:\n    - Compute sequence-level scores by summing token-level scores over response tokens.\n    - Scale by temperature beta and convert to a probability distribution over the batch\n      using softmax.\n    - Compute cross-entropy loss using acc as the target distribution:\n      loss = - sum(acc_i * log(p_i)) / sum(acc_i)\n    \n    Inputs:\n    - token_level_scores: Tensor of shape [B, T, ...] representing per-token scores.\n    - acc: Tensor of shape [B], binary accuracy labels (0 or 1).\n    - response_mask: Tensor broadcastable to token_level_scores, with 1 for response tokens.\n    - beta: float, temperature parameter for softmax.\n\n    Returns:\n    - loss: Scalar tensor (0-d) representing the loss.\n    \"\"\"\n    import torch\n\n    if token_level_scores.dim() < 2:\n        raise ValueError(\"token_level_scores must be at least 2-D (batch x tokens ...).\")\n\n    # Sum over all token dimensions except the batch dimension to get a sequence score per example\n    dims_to_sum = tuple(range(1, token_level_scores.dim()))\n    sequence_scores = (token_level_scores * response_mask).sum(dim=dims_to_sum)  # [B]\n\n    # Apply temperature and convert to probabilities over the batch\n    logits = sequence_scores * beta\n    probs = torch.softmax(logits, dim=0)  # [B]\n\n    # Target distribution from acc (cast to same dtype as scores for math)\n    acc_f = acc.to(token_level_scores.dtype)\n\n    # Cross-entropy with target distribution defined by acc\n    log_p = torch.log(probs.clamp(min=1e-12))\n    loss = - (acc_f * log_p).sum()\n\n    # Normalize by the sum of acc (avoid division by zero via clamping)\n    denom = acc_f.sum().clamp(min=1.0)\n    loss = loss / denom\n\n    return loss"], "usage": {"prompt_tokens": 313, "completion_tokens": 4406, "total_tokens": 4719, "per_completion": [{"prompt_tokens": 313, "completion_tokens": 4406, "total_tokens": 4719}]}}
{"function_name": "compute_detach_dpo_loss_rm", "overview": "Computes detached DPO loss, supporting BoN (Best-of-N) mode", "function_signature": "def compute_detach_dpo_loss_rm(token_level_scores, acc, Q_bc, acc_bc, response_mask, beta, bon_mode=\"none\"):", "input_parameters": "- Base: `token_level_scores`, `acc`, `response_mask`, `beta`\n- Contrast: `Q_bc` (batch Q-value matrix), `acc_bc` (batch accuracy matrix)\n- `bon_mode`: Weight mode {\"none\", \"bon_rm\", \"bon_acc\"}", "detailed_description": "Computes detached DPO loss based on contrastive samples, selecting samples with opposite accuracy labels within batch as contrastive baseline, and supports assigning higher training weights to high-quality samples through BoN weight mechanism.\nBoN weights include three modes: none mode, bon_rm mode, and bon_acc mode.", "output": "Weighted DPO loss value", "func_implementation": "", "test_code_path": "code/test_code/test_detach_dpo_loss.py", "implementation_location": "code/recipe/prime/prime_core_algos.py:line 88-116", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_detach_dpo_loss_rm`.\n\n**Function Overview:**\nComputes detached DPO loss, supporting BoN (Best-of-N) mode\n\n**Function Signature:**\n```python\ndef compute_detach_dpo_loss_rm(token_level_scores, acc, Q_bc, acc_bc, response_mask, beta, bon_mode=\"none\"):\n```\n\n**Input Parameters:**\n- Base: `token_level_scores`, `acc`, `response_mask`, `beta`\n- Contrast: `Q_bc` (batch Q-value matrix), `acc_bc` (batch accuracy matrix)\n- `bon_mode`: Weight mode {\"none\", \"bon_rm\", \"bon_acc\"}\n\n**Detailed Description:**\nComputes detached DPO loss based on contrastive samples, selecting samples with opposite accuracy labels within batch as contrastive baseline, and supports assigning higher training weights to high-quality samples through BoN weight mechanism.\nBoN weights include three modes: none mode, bon_rm mode, and bon_acc mode.\n\n**Expected Output:**\nWeighted DPO loss value\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport torch\nimport torch.nn.functional as F\n\ndef compute_detach_dpo_loss_rm(token_level_scores, acc, Q_bc, acc_bc, response_mask, beta, bon_mode=\"none\"):\n    \"\"\"\n    Compute detached DPO loss with Best-of-N (BoN) weighting modes.\n\n    Args:\n        token_level_scores (Tensor): [B, L], per-token scores (logits/prob scores) for each sample.\n        acc (Tensor or ndarray): [B], accuracy label per sample (0 or 1).\n        Q_bc (Tensor): [B, B], batch Q-values used for BoN rm weighting.\n        acc_bc (Tensor or ndarray): [B, B], unused here but kept for compatibility.\n        response_mask (Tensor): [B, L], 1/0 mask indicating valid tokens in each sample.\n        beta (float): BoN weighting strength.\n        bon_mode (str): BoN mode. One of {\"none\", \"bon_rm\", \"bon_acc\"}.\n\n    Returns:\n        Tensor: scalar loss value (0-dim tensor).\n    \"\"\"\n    # Ensure tensors\n    if not isinstance(token_level_scores, torch.Tensor):\n        token_level_scores = torch.tensor(token_level_scores)\n    if not isinstance(acc, torch.Tensor):\n        acc = torch.tensor(acc)\n    if not isinstance(Q_bc, torch.Tensor):\n        Q_bc = torch.tensor(Q_bc)\n    if not isinstance(acc_bc, torch.Tensor):\n        acc_bc = torch.tensor(acc_bc)\n    if not isinstance(response_mask, torch.Tensor):\n        response_mask = torch.tensor(response_mask)\n\n    # Cast to float for computations\n    token_level_scores = token_level_scores.float()\n    acc = acc.float()\n    Q_bc = Q_bc.float()\n    acc_bc = acc_bc.float()\n    response_mask = response_mask.float()\n\n    # Basics\n    B = token_level_scores.shape[0]  # batch size\n    if token_level_scores.dim() != 2:\n        raise ValueError(\"token_level_scores must be of shape [B, L]\")\n\n    # Base sample score: sum of token scores over valid response tokens\n    # [B]\n    base_score = (token_level_scores * response_mask).sum(dim=1)\n\n    # Contrast selection: opposite accuracy labels within batch\n    # opp_mask[i, j] = 1 if acc[i] != acc[j], else 0\n    acc_col = acc.view(-1, 1)    # [B, 1]\n    acc_row = acc.view(1, -1)    # [1, B]\n    opp_mask = (acc_col != acc_row).float()  # [B, B]\n\n    # Differences between base samples and their contrastive baseline\n    # D[i, j] = base_score[i] - base_score[j]\n    D = base_score.view(B, 1) - base_score.view(1, B)  # [B, B]\n\n    # Detached baseline: detaching the contrast score (base_score[j])\n    # Here we use DPO-style loss: L_ij = softplus(-D_ij / tau)\n    # We set tau = 1.0 as a simple temperature.\n    tau = 1.0\n    L_pair = F.softplus(-D / tau)  # [B, B]\n\n    # Apply contrast mask to consider only opposite-accuracy pairs\n    L = L_pair * opp_mask  # [B, B]\n\n    # Per-sample counts of contrastive pairs\n    opp_count = opp_mask.sum(dim=1).clamp(min=1)  # [B], avoid div by zero\n\n    # Per-sample loss: average over its BoN contrastive samples\n    per_i_loss = L.sum(dim=1) / opp_count  # [B]\n\n    # BoN weighting\n    if bon_mode == \"none\":\n        w = torch.ones(B, device=token_level_scores.device, dtype=per_i_loss.dtype)\n    elif bon_mode == \"bon_acc\":\n        # Weight higher quality samples (acc==1) more\n        w = 1.0 + beta * acc  # acc is [B], values 0 or 1\n    elif bon_mode == \"bon_rm\":\n        # Weight based on Q_bc values across opposite-accuracy pairs\n        # mean_q[i] = mean(Q_bc[i, j] for j with opp_mask[i, j]==1)\n        mean_q = (Q_bc * opp_mask).sum(dim=1) / opp_count\n        # Replace NaNs (if any) with 0\n        mean_q = torch.where(opp_count > 0, mean_q, torch.zeros_like(mean_q))\n        w = 1.0 + beta * mean_q  # [B]\n    else:\n        raise ValueError(f\"Unsupported bon_mode: {bon_mode}\")\n\n    # Final weighted loss: average over batch\n    final_loss = (per_i_loss * w).mean()\n\n    return final_loss\n```"], "usage": {"prompt_tokens": 401, "completion_tokens": 5279, "total_tokens": 5680, "per_completion": [{"prompt_tokens": 401, "completion_tokens": 5279, "total_tokens": 5680}]}}
{"function_name": "compute_dpo_accuracy", "overview": "Computes pairwise comparison DPO accuracy", "function_signature": "def compute_dpo_accuracy(token_level_scores, acc, response_mask, n_samples):", "input_parameters": "- `token_level_scores`: Token-level reward scores\n- `acc`: Accuracy labels\n- `response_mask`: Response mask\n- `n_samples`: Sample count per group", "detailed_description": "Uses pairwise comparison to evaluate reward model accuracy. Aggregates token-level scores to sequence-level scores, constructs sample pairs for comparison. Judges directional consistency by computing score differences and accuracy differences.\nUses weighted average to compute accuracy, with higher weights for sample pairs with larger accuracy differences. Returns 0.5 when all samples have the same accuracy.", "output": "Weighted pairwise comparison accuracy", "func_implementation": "", "test_code_path": "code/test_code/test_dpo_accuracy.py", "implementation_location": "code/recipe/prime/prime_core_algos.py:line 119-143", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_dpo_accuracy`.\n\n**Function Overview:**\nComputes pairwise comparison DPO accuracy\n\n**Function Signature:**\n```python\ndef compute_dpo_accuracy(token_level_scores, acc, response_mask, n_samples):\n```\n\n**Input Parameters:**\n- `token_level_scores`: Token-level reward scores\n- `acc`: Accuracy labels\n- `response_mask`: Response mask\n- `n_samples`: Sample count per group\n\n**Detailed Description:**\nUses pairwise comparison to evaluate reward model accuracy. Aggregates token-level scores to sequence-level scores, constructs sample pairs for comparison. Judges directional consistency by computing score differences and accuracy differences.\nUses weighted average to compute accuracy, with higher weights for sample pairs with larger accuracy differences. Returns 0.5 when all samples have the same accuracy.\n\n**Expected Output:**\nWeighted pairwise comparison accuracy\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def compute_dpo_accuracy(token_level_scores, acc, response_mask, n_samples):\n    \"\"\"\n    Compute weighted pairwise comparison DPO accuracy.\n\n    Steps:\n    - Aggregate token-level scores to sequence-level scores using the response mask.\n    - For every group (first dimension), compare all pairs of samples within that group.\n    - For a pair (i, j), compute diff_score = score_i - score_j and diff_acc = acc_i - acc_j.\n    - If diff_acc != 0, count as correct if sign(diff_score) == sign(diff_acc); weight by abs(diff_acc).\n    - Return weighted accuracy. If no pair has non-zero accuracy difference, return 0.5.\n\n    Parameters:\n    - token_level_scores: array-like of shape (G, S, L) or similar, token-level reward scores.\n    - acc: array-like of shape (G, S) or compatible, accuracy labels per sample.\n    - response_mask: mask indicating valid tokens. Can be shape (G, L) or (G, S, L) or broadcastable.\n    - n_samples: integer, number of samples per group to consider.\n\n    Returns:\n    - float: weighted pairwise DPO accuracy in [0, 1].\n    \"\"\"\n    import numpy as np\n\n    t = np.asarray(token_level_scores)\n    m = np.asarray(response_mask)\n    acc_arr = np.asarray(acc)\n\n    if t.ndim != 3:\n        raise ValueError(\"token_level_scores must be a 3D tensor of shape (G, S, L).\")\n\n    G, S, L = t.shape\n\n    # Prepare mask to broadcast to (G, S, L)\n    if m.ndim == 3:\n        if m.shape == t.shape:\n            m_b = m\n        else:\n            try:\n                m_b = np.broadcast_to(m, t.shape).copy()\n            except Exception:\n                m_b = np.ones((G, S, L), dtype=t.dtype)\n    elif m.ndim == 2:\n        # Expecting (G, L)\n        if m.shape == (G, L):\n            m_b = m[:, None, :]\n        else:\n            # Try (G, S) as a per-sample mask (less common)\n            if m.shape == (G, S):\n                m_b = m[:, :, None]\n            else:\n                m_b = np.ones((G, S, L), dtype=t.dtype)\n    else:\n        m_b = np.ones((G, S, L), dtype=t.dtype)\n\n    # If shapes still mismatch, attempt a safe broadcast or fallback\n    if m_b.shape != t.shape:\n        try:\n            m_b = np.broadcast_to(m, t.shape).copy()\n        except Exception:\n            m_b = np.ones((G, S, L), dtype=t.dtype)\n\n    # Compute sequence-level scores by masking and averaging over the token axis\n    denom = m_b.sum(axis=2, keepdims=True)\n    denom = np.where(denom == 0, 1.0, denom)  # avoid division by zero\n    seq_scores = (t * m_b).sum(axis=2) / denom[..., 0]  # shape: (G, S)\n\n    # Ensure acc_arr has shape (G, S)\n    if acc_arr.ndim == 1:\n        if acc_arr.size == G:\n            acc_arr = acc_arr.reshape(G, 1)\n            acc_arr = np.repeat(acc_arr, S, axis=1)\n        else:\n            raise ValueError(\"acc must be shape (G, S) or (G,).\")\n    if acc_arr.ndim == 2:\n        if acc_arr.shape != (G, S):\n            acc_arr = acc_arr.reshape((G, -1))\n            if acc_arr.shape[1] != S:\n                raise ValueError(\"acc shape incompatible with token_level_scores.\")\n    else:\n        raise ValueError(\"acc must be 1D or 2D array.\")\n\n    # Respect n_samples: truncate if necessary\n    n = int(n_samples) if n_samples is not None else S\n    if n <= 0:\n        return 0.5\n    if S > n:\n        seq_scores = seq_scores[:, :n]\n        acc_arr = acc_arr[:, :n]\n        S = n\n\n    # Quick early exit: if all samples have the same accuracy, return 0.5\n    if np.all(acc_arr == acc_arr[0, 0]):\n        # All entries are identical; return 0.5\n        return 0.5\n\n    total_weight = 0.0\n    correct_weighted = 0.0\n\n    for g in range(G):\n        for i in range(S):\n            for j in range(i + 1, S):\n                diff_acc = float(acc_arr[g, i] - acc_arr[g, j])\n                if diff_acc == 0.0:\n                    continue  # skip pairs with no accuracy difference\n                diff_score = float(seq_scores[g, i] - seq_scores[g, j])\n                # directional consistency: sign(diff_score) should match sign(diff_acc)\n                correct = 1.0 if diff_score * diff_acc > 0.0 else 0.0\n                w = abs(diff_acc)\n                total_weight += w\n                correct_weighted += w * correct\n\n    if total_weight == 0.0:\n        return 0.5\n    return correct_weighted / total_weight"], "usage": {"prompt_tokens": 363, "completion_tokens": 5658, "total_tokens": 6021, "per_completion": [{"prompt_tokens": 363, "completion_tokens": 5658, "total_tokens": 6021}]}}
{"function_name": "compute_dpo_abs_accuracy", "overview": "Computes absolute accuracy metric", "function_signature": "def compute_dpo_abs_accuracy(token_level_scores, acc, response_mask, n_samples):", "input_parameters": "- `token_level_scores`: Token-level reward scores\n- `acc`: Accuracy labels\n- `response_mask`: Response mask", "detailed_description": "Converts aggregated token-level scores to predicted labels, compares with true labels to compute absolute accuracy.", "output": "Absolute classification accuracy", "func_implementation": "", "test_code_path": "code/test_code/test_dpo_abs_accuracy.py", "implementation_location": "code/recipe/prime/prime_core_algos.py:line 146-147", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_dpo_abs_accuracy`.\n\n**Function Overview:**\nComputes absolute accuracy metric\n\n**Function Signature:**\n```python\ndef compute_dpo_abs_accuracy(token_level_scores, acc, response_mask, n_samples):\n```\n\n**Input Parameters:**\n- `token_level_scores`: Token-level reward scores\n- `acc`: Accuracy labels\n- `response_mask`: Response mask\n\n**Detailed Description:**\nConverts aggregated token-level scores to predicted labels, compares with true labels to compute absolute accuracy.\n\n**Expected Output:**\nAbsolute classification accuracy\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def compute_dpo_abs_accuracy(token_level_scores, acc, response_mask, n_samples):\n    \"\"\"\n    Computes absolute accuracy from token-level scores.\n\n    Steps:\n    - Aggregate token-level scores per sample using the response_mask (if provided).\n    - Convert aggregated scores to a predicted label: 1 if the sum > 0, else 0.\n    - Compare predicted labels with true labels (acc) to compute accuracy.\n\n    Parameters:\n    - token_level_scores: array-like, shape (n_samples, n_tokens) or similar 2D structure\n                          containing token-level reward scores.\n    - acc: array-like of true labels, length >= n_samples.\n    - response_mask: array-like, same shape as token_level_scores or length n_tokens\n                     used to mask valid tokens. Can be None.\n    - n_samples: int, number of samples to evaluate.\n\n    Returns:\n    - float: absolute classification accuracy in [0.0, 1.0]\n    \"\"\"\n    import numpy as np\n\n    t = np.asarray(token_level_scores, dtype=float)\n    a = np.asarray(acc).astype(int)\n    if a.ndim != 1:\n        a = a.ravel()\n\n    mask = None if response_mask is None else np.asarray(response_mask)\n\n    # If 2D token scores: aggregate per sample\n    if t.ndim == 2:\n        if mask is None:\n            sums = t.sum(axis=1)\n        else:\n            # mask can be same shape as t or 1D with length n_tokens\n            if mask.shape == t.shape:\n                sums = (t * mask).sum(axis=1)\n            elif mask.ndim == 1 and mask.shape[0] == t.shape[1]:\n                sums = (t * mask).sum(axis=1)\n            else:\n                # Fallback broadcasting attempt\n                sums = (t * mask).sum(axis=1)\n\n        pred = (sums > 0).astype(int)\n\n        # Use up to n_samples and available labels\n        L = min(pred.shape[0], int(n_samples), a.shape[0])\n        if L <= 0:\n            return 0.0\n        accuracy = (pred[:L] == a[:L]).sum() / float(L)\n        return float(accuracy)\n\n    # If 1D token scores (single sample)\n    elif t.ndim == 1:\n        if mask is not None:\n            # Apply mask if possible\n            if mask.shape == t.shape or (mask.ndim == 1 and mask.shape[0] == t.shape[0]):\n                total = float((t * mask).sum())\n            else:\n                total = float((t * mask).sum())\n        else:\n            total = float(t.sum())\n        pred = int(total > 0)\n        label = int(a[0]) if a.size > 0 else 0\n        return float(pred == label)\n\n    else:\n        # Unsupported dimensionality\n        return 0.0"], "usage": {"prompt_tokens": 301, "completion_tokens": 3745, "total_tokens": 4046, "per_completion": [{"prompt_tokens": 301, "completion_tokens": 3745, "total_tokens": 4046}]}}
{"function_name": "compute_rloo_advantage_return", "overview": "Implements RLOO advantage estimation, supporting multiple reward source fusion", "function_signature": "def compute_rloo_advantage_return(data: verl.DataProto, response_mask: torch.Tensor, n_samples, config):", "input_parameters": "- `data`: Data protocol object containing DataProto.batch fields\n- `data.batch[\"rm_scores\"]`: Implicit reward scores\n- `data.batch[\"acc\"]`: Binary accuracy labels\n- `data.batch[\"prompts\"]`: Input prompt sequences\n- `data.batch[\"attention_mask\"]`: Attention mask\n- `response_mask`: Response mask indicating valid token positions\n- `n_samples`: Sample count per group, used for RLOO grouping computation\n- `config`: Reward weight configuration, hierarchical configuration object\n- `config.algorithm.reward_dpo_coef`: DPO reward weight coefficient, default 5.0\n- `config.algorithm.reward_gt_coef`: Accuracy reward weight coefficient, default 5.0", "detailed_description": "Function supports advantage computation with multi-source reward fusion. Reward sources include reward model scores and accuracy rewards.\nGroups multiple responses generated from the same prompt by n_samples intervals.\nUses RLOO to compute advantage function for each reward source and fuses multiple reward sources, computes cumulative return values after reward fusion and applies whitening.", "output": "- `advantages`: Whitened advantage function (torch.Tensor, shape: [batch_size, seq_len])\n- `returns`: Cumulative return values (torch.Tensor, shape: [batch_size, seq_len])", "func_implementation": "", "test_code_path": "code/test_code/test_rloo_advantage_return.py", "implementation_location": "code/recipe/prime/prime_core_algos.py:line 21-79", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_rloo_advantage_return`.\n\n**Function Overview:**\nImplements RLOO advantage estimation, supporting multiple reward source fusion\n\n**Function Signature:**\n```python\ndef compute_rloo_advantage_return(data: verl.DataProto, response_mask: torch.Tensor, n_samples, config):\n```\n\n**Input Parameters:**\n- `data`: Data protocol object containing DataProto.batch fields\n- `data.batch[\"rm_scores\"]`: Implicit reward scores\n- `data.batch[\"acc\"]`: Binary accuracy labels\n- `data.batch[\"prompts\"]`: Input prompt sequences\n- `data.batch[\"attention_mask\"]`: Attention mask\n- `response_mask`: Response mask indicating valid token positions\n- `n_samples`: Sample count per group, used for RLOO grouping computation\n- `config`: Reward weight configuration, hierarchical configuration object\n- `config.algorithm.reward_dpo_coef`: DPO reward weight coefficient, default 5.0\n- `config.algorithm.reward_gt_coef`: Accuracy reward weight coefficient, default 5.0\n\n**Detailed Description:**\nFunction supports advantage computation with multi-source reward fusion. Reward sources include reward model scores and accuracy rewards.\nGroups multiple responses generated from the same prompt by n_samples intervals.\nUses RLOO to compute advantage function for each reward source and fuses multiple reward sources, computes cumulative return values after reward fusion and applies whitening.\n\n**Expected Output:**\n- `advantages`: Whitened advantage function (torch.Tensor, shape: [batch_size, seq_len])\n- `returns`: Cumulative return values (torch.Tensor, shape: [batch_size, seq_len])\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def compute_rloo_advantage_return(data: verl.DataProto, response_mask: torch.Tensor, n_samples, config):\n    \"\"\"\n    Compute RLOO-based advantages and returns with multi-source reward fusion.\n\n    Steps:\n    - Fuse reward sources: reward model scores (rm_scores) and accuracy (acc) with coefficients\n      from config.algorithm.reward_dpo_coef and config.algorithm.reward_gt_coef.\n    - Group multiple responses generated from the same prompt by n_samples intervals.\n    - For each group, compute Leave-One-Out (LOO) rewards across samples, then compute\n      discounted cumulative returns from RLOO rewards.\n    - Flatten results back to the original batch order, apply whitening to advantages, and\n      mask invalid tokens with response_mask.\n    - Return:\n      - advantages: whitened advantage tensor of shape [batch_size, seq_len]\n      - returns: cumulative return values tensor of shape [batch_size, seq_len]\n\n    Note:\n    - If a group has only one sample, LOOC terms are zeros for that group.\n    - Gamma (discount) is retrieved from config.algorithm.gamma with a default of 0.99.\n    \"\"\"\n    # Access batch data\n    batch = data.batch\n    rm_scores = batch[\"rm_scores\"]  # shape: [N, L]\n    acc = batch.get(\"acc\", None)    # could be [N] or [N, 1] or [N, L]\n    prompts = batch.get(\"prompts\", None)  # unused here, kept for compatibility\n    attention_mask = batch.get(\"attention_mask\", None)  # unused here\n\n    device = rm_scores.device\n    N, L = rm_scores.shape[0], rm_scores.shape[1]\n\n    # Prepare accuracy rewards: broadcast to [N, L] if needed\n    if acc is None:\n        acc_tile = torch.zeros((N, L), dtype=rm_scores.dtype, device=device)\n    else:\n        if acc.ndim == 1:\n            acc_tile = acc.view(-1, 1).expand(-1, L).to(rm_scores.dtype)\n        elif acc.shape[1] == L:\n            acc_tile = acc.to(rm_scores.dtype)\n        else:\n            # Fallback: broadcast single value per sample\n            acc_tile = acc.view(-1, 1).expand(-1, L).to(rm_scores.dtype)\n\n    # Retrieve coefficients with sensible defaults\n    algo = getattr(config, \"algorithm\", None)\n    reward_dpo_coef = getattr(algo, \"reward_dpo_coef\", 5.0)\n    reward_gt_coef = getattr(algo, \"reward_gt_coef\", 5.0)\n    gamma = getattr(algo, \"gamma\", 0.99)\n\n    # Compute per-token rewards from multiple sources\n    reward_dpo = reward_dpo_coef * rm_scores\n    reward_gt = reward_gt_coef * acc_tile\n    total_r = reward_dpo + reward_gt  # shape: [N, L]\n\n    # Group responses by n_samples per prompt\n    group_size = int(n_samples) if (n_samples is not None and int(n_samples) > 0) else 1\n    group_size = max(1, group_size)\n    groups = N // group_size\n    trimmed_N = groups * group_size\n\n    if groups == 0:\n        # Not enough samples to form groups; return zeros\n        returns = torch.zeros((N, L), dtype=rm_scores.dtype, device=device)\n        advantages = torch.zeros((N, L), dtype=rm_scores.dtype, device=device)\n        # Apply response mask if provided\n        if response_mask is not None:\n            returns = returns * response_mask.to(returns.dtype)\n            adv_mask = response_mask.to(torch.bool)\n            advantages = advantages * adv_mask\n        return advantages, returns\n\n    # Reshape for group-wise LOOC\n    total_r_group = total_r[:trimmed_N].reshape(groups, group_size, L)  # [groups, group_size, L]\n    group_sum = total_r_group.sum(dim=1)  # [groups, L]\n\n    if group_size > 1:\n        loo_rewards = (group_sum[:, None, :] - total_r_group) / (group_size - 1)  # [groups, group_size, L]\n    else:\n        loo_rewards = torch.zeros((groups, 1, L), dtype=rm_scores.dtype, device=device)\n\n    # Compute discounted returns from LOOC rewards\n    cum = torch.zeros((groups, group_size), dtype=rm_scores.dtype, device=device)\n    ret = torch.empty((groups, group_size, L), dtype=rm_scores.dtype, device=device)\n\n    for t in range(L - 1, -1, -1):\n        cum = loo_rewards[:, :, t] + gamma * cum\n        ret[:, :, t] = cum\n\n    # Flatten back to [N, L], padding if needed\n    returns = torch.zeros((N, L), dtype=rm_scores.dtype, device=device)\n    returns[:trimmed_N] = ret.reshape(-1, L)\n\n    # Apply response_mask if provided\n    if response_mask is not None:\n        mask = response_mask.to(dtype=torch.bool)\n        # Ensure mask has the same shape as returns\n        if mask.shape != returns.shape:\n            # Try broadcasting if mask is [N] or [N, 1] types\n            if mask.dim() == 1 and mask.shape[0] == N:\n                mask = mask.view(N, 1).expand(N, L)\n            else:\n                # As a safe fallback, broadcast to [N, L]\n                mask = mask.to(dtype=torch.bool)\n                mask = mask.expand_as(returns).to(dtype=torch.bool)\n    else:\n        mask = torch.ones_like(returns, dtype=torch.bool)\n\n    # Zero out invalid positions\n    returns = returns * mask.to(returns.dtype)\n\n    # Whitening: compute mean/std on valid elements only\n    valid_mask = mask\n    valid_vals = returns[valid_mask]\n    if valid_vals.numel() > 0:\n        mean = valid_vals.mean()\n        std = valid_vals.std(unbiased=False)\n        if std.item() == 0:\n            std = torch.tensor(1.0, dtype=returns.dtype, device=device)\n    else:\n        mean = torch.tensor(0.0, dtype=returns.dtype, device=device)\n        std = torch.tensor(1.0, dtype=returns.dtype, device=device)\n\n    advantages = torch.zeros_like(returns)\n    if valid_vals.numel() > 0:\n        adv_vals = (valid_vals - mean) / (std + 1e-8)\n        advantages[valid_mask] = adv_vals\n\n    return advantages, returns"], "usage": {"prompt_tokens": 519, "completion_tokens": 6770, "total_tokens": 7289, "per_completion": [{"prompt_tokens": 519, "completion_tokens": 6770, "total_tokens": 7289}]}}
{"function_name": "RayPRIMETrainer.filter_and_downsample", "overview": "Implements intelligent sample filtering and priority downsampling for PRIME algorithm", "function_signature": "def filter_and_downsample(self, scores, batch: DataProto):", "input_parameters": "- `scores`: Verification score list, binary accuracy labels\n- `batch`: Data batch object\n- `config`: Filtering parameter configuration", "detailed_description": "Filters high-quality samples from oversampled candidate responses. Reconstructs verification scores into 2D matrix, performs dual filtering: accuracy filtering and length filtering.\nAccuracy filtering retains sample groups with average accuracy within configured range; length filtering removes sample groups exceeding maximum length threshold.\nSample groups meeting filtering conditions are retained with priority, ultimately retaining 1/oversample_factor proportion of samples. If all groups fail to meet filtering conditions, algorithm still retains corresponding proportion of samples to ensure training continues.", "output": "Filtered high-quality data batch", "func_implementation": "", "test_code_path": "code/test_code/test_filter_and_downsample.py", "implementation_location": "code/recipe/prime/prime_ray_trainer.py:line 540-572", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `RayPRIMETrainer.filter_and_downsample`.\n\n**Function Overview:**\nImplements intelligent sample filtering and priority downsampling for PRIME algorithm\n\n**Function Signature:**\n```python\ndef filter_and_downsample(self, scores, batch: DataProto):\n```\n\n**Input Parameters:**\n- `scores`: Verification score list, binary accuracy labels\n- `batch`: Data batch object\n- `config`: Filtering parameter configuration\n\n**Detailed Description:**\nFilters high-quality samples from oversampled candidate responses. Reconstructs verification scores into 2D matrix, performs dual filtering: accuracy filtering and length filtering.\nAccuracy filtering retains sample groups with average accuracy within configured range; length filtering removes sample groups exceeding maximum length threshold.\nSample groups meeting filtering conditions are retained with priority, ultimately retaining 1/oversample_factor proportion of samples. If all groups fail to meet filtering conditions, algorithm still retains corresponding proportion of samples to ensure training continues.\n\n**Expected Output:**\nFiltered high-quality data batch\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def filter_and_downsample(self, scores, batch: DataProto):\n    \"\"\"\n    Intelligent sample filtering and priority downsampling for PRIME-like behavior.\n\n    - Reconstructs scores into groups of size 'oversample_factor' (K).\n    - Performs accuracy filtering: keep groups whose average score is within [min_acc, max_acc].\n    - Performs length filtering: keep groups whose maximum sample length is <= max_len (if length info is available).\n    - Prioritize good groups, downsample to target fraction (total_samples // K).\n    - If not enough good groups, fill from remaining groups based on accuracy.\n    - Returns a new batch containing only the kept samples.\n    \"\"\"\n    import math\n    import numpy as np\n    import copy\n\n    # Extract configuration (support dict or object with attributes)\n    cfg = getattr(self, 'config', None)\n    if cfg is None:\n        min_acc = 0.0\n        max_acc = 1.0\n        max_len = 0\n        oversample_factor = 1\n    else:\n        if isinstance(cfg, dict):\n            min_acc = cfg.get('min_accuracy', 0.0)\n            max_acc = cfg.get('max_accuracy', 1.0)\n            max_len = cfg.get('max_length', 0)\n            oversample_factor = cfg.get('oversample_factor', 1)\n        else:\n            min_acc = getattr(cfg, 'min_accuracy', 0.0)\n            max_acc = getattr(cfg, 'max_accuracy', 1.0)\n            max_len = getattr(cfg, 'max_length', 0)\n            oversample_factor = getattr(cfg, 'oversample_factor', 1)\n\n    try:\n        K = max(1, int(oversample_factor))\n    except Exception:\n        K = 1\n\n    scores_arr = np.asarray(scores)\n    total = int(scores_arr.shape[0]) if scores_arr.ndim > 0 else 0\n    if total <= 0:\n        return batch\n\n    # Determine groups: each group has size K, handle remainder\n    if total < K:\n        group_size = total\n        n_groups = 1\n    else:\n        group_size = K\n        n_groups = total // K\n        if total % K != 0:\n            n_groups += 1  # last group with fewer items\n\n    # Build per-group stats\n    groups = []\n    for g in range(n_groups):\n        start = g * group_size\n        end = min(start + group_size, total)\n        if start >= end:\n            continue\n        acc = float(np.mean(scores_arr[start:end]))\n        groups.append({'start': start, 'end': end, 'acc': acc, 'size': end - start})\n\n    # Retrieve per-sample lengths if available\n    sample_lengths = None\n    for attr_name in ('lengths', 'sample_lengths', 'sample_lens', 'length'):\n        if hasattr(batch, attr_name):\n            val = getattr(batch, attr_name)\n            if isinstance(val, (list, tuple)):\n                if len(val) >= total:\n                    sample_lengths = list(val[:total])\n                elif len(val) == total:\n                    sample_lengths = list(val)\n            break\n\n    # If lengths not readily available, try per-sample probing via batch[i]\n    if sample_lengths is None:\n        lengths_tmp = []\n        usable_probe = True\n        for i in range(total):\n            l = None\n            try:\n                item = batch[i]\n            except Exception:\n                usable_probe = False\n                break\n            if hasattr(item, 'length'):\n                l = int(getattr(item, 'length'))\n            elif hasattr(item, 'len'):\n                l = int(getattr(item, 'len'))\n            elif isinstance(item, (str, bytes)):\n                l = len(item)\n            elif hasattr(item, 'shape'):\n                try:\n                    l = int(item.shape[0])\n                except Exception:\n                    l = None\n            if l is None:\n                usable_probe = False\n                break\n            lengths_tmp.append(int(l))\n        if usable_probe and len(lengths_tmp) == total:\n            sample_lengths = lengths_tmp\n\n    max_len_val = int(max_len) if max_len is not None else 0\n\n    # Length filter per group\n    len_ok = [True] * len(groups)\n    if sample_lengths is not None and max_len_val > 0:\n        for gi, g in enumerate(groups):\n            start = g['start']\n            end = g['end']\n            if end > start:\n                group_max = max(sample_lengths[start:end])\n                len_ok[gi] = (group_max <= max_len_val)\n            else:\n                len_ok[gi] = True\n\n    # Accuracy filter per group\n    good_group = []\n    for gi, g in enumerate(groups):\n        acc_ok = (g['acc'] >= min_acc) and (g['acc'] <= max_acc)\n        length_ok = len_ok[gi] if gi < len(len_ok) else True\n        good_group.append(acc_ok and length_ok)\n\n    # Build indices to keep\n    target = max(1, total // K)\n\n    accs = [g['acc'] for g in groups]\n    group_indices = list(range(len(groups)))\n    good_order = [i for i in group_indices if good_group[i]]\n    bad_order = [i for i in group_indices if not good_group[i]]\n    good_order.sort(key=lambda i: accs[i], reverse=True)\n    bad_order.sort(key=lambda i: accs[i], reverse=True)\n\n    kept = []\n    # Add full groups from good ones first\n    for gi in good_order:\n        if len(kept) >= target:\n            break\n        s = groups[gi]['start']\n        e = groups[gi]['end']\n        sz = e - s\n        if len(kept) + sz <= target:\n            kept.extend(range(s, e))\n        else:\n            remaining = target - len(kept)\n            if remaining > 0:\n                kept.extend(range(s, s + remaining))\n            break\n\n    # If still room, fill from bad groups\n    if len(kept) < target:\n        for gi in bad_order:\n            if len(kept) >= target:\n                break\n            s = groups[gi]['start']\n            e = groups[gi]['end']\n            sz = e - s\n            if len(kept) + sz <= target:\n                kept.extend(range(s, e))\n            else:\n                remaining = target - len(kept)\n                if remaining > 0:\n                    kept.extend(range(s, s + remaining))\n                break\n\n    if len(kept) == 0 and total > 0:\n        kept = [0]\n\n    kept = sorted(set(kept))\n\n    # If batch supports advanced indexing, use it\n    if hasattr(batch, '__getitem__'):\n        try:\n            new_batch = batch[kept]\n            return new_batch\n        except Exception:\n            pass\n\n    # Fallback: try to construct a new batch by copying and slicing known fields\n    try:\n        new_batch = copy.copy(batch)\n        for field in ('data', 'samples', 'items', 'batch'):\n            if hasattr(batch, field):\n                orig = getattr(batch, field)\n                if isinstance(orig, (list, tuple)):\n                    sliced = [orig[i] for i in kept if i < len(orig)]\n                    setattr(new_batch, field, sliced)\n                    break\n        return new_batch\n    except Exception:\n        return batch"], "usage": {"prompt_tokens": 388, "completion_tokens": 10351, "total_tokens": 10739, "per_completion": [{"prompt_tokens": 388, "completion_tokens": 10351, "total_tokens": 10739}]}}
