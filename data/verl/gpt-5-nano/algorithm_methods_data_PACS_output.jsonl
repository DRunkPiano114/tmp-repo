{"function_name": "compute_reward", "overview": "Computes \"reward\" for policy improvement, measuring log probability change of current policy relative to reference policy.", "function_signature": "def compute_reward(\nold_log_prob,\nlog_prob,\nresponse_mask,\nbeta=1.0,\nreward_method=\"1\",\n):", "input_parameters": "- `old_log_prob`: Reference policy token-level log probabilities, shape `[batch_size, response_length]`\n- `log_prob`: Current policy token-level log probabilities, shape `[batch_size, response_length]`\n- `response_mask`: Response portion mask, shape `[batch_size, response_length]`\n- `beta`: Reward scaling factor, controls aggressiveness of policy updates (default 1.0)\n- `reward_method`: Reward computation method, '1' or '2'", "detailed_description": "Function computes reward values in reinforcement learning based on specified method.\nMethod 1 computes sum of new-old log probability differences, measuring policy improvement magnitude.\nMethod 2 computes normalized sum of current log probabilities.\nIf an invalid method number is specified, function raises an exception.", "output": "- `reward`: Policy improvement reward, shape `[batch_size, 1]`", "func_implementation": "", "test_code_path": "code/test_code/test_compute_reward.py", "implementation_location": "code/src/pacs/pacs_core_algos.py:line 7-21", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_reward`.\n\n**Function Overview:**\nComputes \"reward\" for policy improvement, measuring log probability change of current policy relative to reference policy.\n\n**Function Signature:**\n```python\ndef compute_reward(\nold_log_prob,\nlog_prob,\nresponse_mask,\nbeta=1.0,\nreward_method=\"1\",\n):\n```\n\n**Input Parameters:**\n- `old_log_prob`: Reference policy token-level log probabilities, shape `[batch_size, response_length]`\n- `log_prob`: Current policy token-level log probabilities, shape `[batch_size, response_length]`\n- `response_mask`: Response portion mask, shape `[batch_size, response_length]`\n- `beta`: Reward scaling factor, controls aggressiveness of policy updates (default 1.0)\n- `reward_method`: Reward computation method, '1' or '2'\n\n**Detailed Description:**\nFunction computes reward values in reinforcement learning based on specified method.\nMethod 1 computes sum of new-old log probability differences, measuring policy improvement magnitude.\nMethod 2 computes normalized sum of current log probabilities.\nIf an invalid method number is specified, function raises an exception.\n\n**Expected Output:**\n- `reward`: Policy improvement reward, shape `[batch_size, 1]`\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def compute_reward(\n    old_log_prob,\n    log_prob,\n    response_mask,\n    beta=1.0,\n    reward_method=\"1\",\n):\n    \"\"\"\n    Compute policy improvement reward.\n\n    Args:\n        old_log_prob (Tensor or array): [batch_size, response_length] reference policy token-level log probs\n        log_prob (Tensor or array): [batch_size, response_length] current policy token-level log probs\n        response_mask (Tensor or array): [batch_size, response_length] mask for response tokens (1 for tokens to include, 0 otherwise)\n        beta (float): reward scaling factor\n        reward_method (str or int): \"1\" for sum of (log_prob - old_log_prob) over masked tokens,\n                                    \"2\" for normalized sum (average) of current log probabilities over masked tokens\n\n    Returns:\n        Tensor: reward of shape [batch_size, 1]\n    \"\"\"\n    import torch\n\n    # Convert inputs to tensors if they aren't already\n    old_log_prob = torch.as_tensor(old_log_prob)\n    log_prob = torch.as_tensor(log_prob)\n    response_mask = torch.as_tensor(response_mask)\n\n    if old_log_prob.shape != log_prob.shape or old_log_prob.shape != response_mask.shape:\n        raise ValueError(\n            f\"Input shapes must be identical, got {old_log_prob.shape}, {log_prob.shape}, {response_mask.shape}\"\n        )\n\n    # Ensure float type for computations\n    old_l = old_log_prob.to(dtype=log_prob.dtype)\n    new_l = log_prob.to(dtype=log_prob.dtype)\n    mask = response_mask.to(dtype=log_prob.dtype)\n\n    diff = new_l - old_l  # [batch, response_length]\n\n    method = str(reward_method)\n\n    if method == \"1\":\n        # Sum of new-old log probability differences over masked tokens\n        # Beta scales the reward\n        per_batch_sum = (diff * mask).sum(dim=1, keepdim=True)  # [batch, 1]\n        reward = beta * per_batch_sum\n    elif method == \"2\":\n        # Normalized sum (average) of current log probabilities over masked tokens\n        sum_log = (new_l * mask).sum(dim=1, keepdim=True)  # [batch, 1]\n        token_count = mask.sum(dim=1, keepdim=True)  # [batch, 1]\n        # Avoid division by zero\n        denom = token_count.clamp(min=1e-8)\n        avg_log = sum_log / denom\n        reward = beta * avg_log\n    else:\n        raise ValueError(f\"Invalid reward_method '{reward_method}'. Expected '1' or '2'.\")\n\n    return reward"], "usage": {"prompt_tokens": 440, "completion_tokens": 1896, "total_tokens": 2336, "per_completion": [{"prompt_tokens": 440, "completion_tokens": 1896, "total_tokens": 2336}]}}
{"function_name": "compute_weight", "overview": "Computes sample weights to handle class imbalance problem (uneven positive/negative sample ratio).", "function_signature": "def compute_weight(score, rollout_n, mode=\"question\"):", "input_parameters": "- `score`: Verifiable rewards (labels), shape `[batch_size, 1]`\n- `rollout_n`: Number of responses generated per prompt\n- `mode`: Weight computation mode, 'question'/'only_positive'/'only_negative'", "detailed_description": "Function assigns training weights to samples based on different modes. In \"question\" mode, function groups samples by rollout_n size, using class-balanced weights for groups with label imbalance to make positive and negative samples contribute equally to training. In \"only_positive\" mode, only positive samples (label 1) are assigned weight 1.0, negative samples get weight 0. In \"only_negative\" mode, the opposite applies: only negative samples (label 0) are assigned weight 1.0. If an invalid mode is specified, function raises an exception.", "output": "- `weight`: Sample weights, shape `[batch_size, 1]`", "func_implementation": "", "test_code_path": "code/test_code/test_compute_weight.py", "implementation_location": "code/src/pacs/pacs_core_algos.py:line 59-86", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_weight`.\n\n**Function Overview:**\nComputes sample weights to handle class imbalance problem (uneven positive/negative sample ratio).\n\n**Function Signature:**\n```python\ndef compute_weight(score, rollout_n, mode=\"question\"):\n```\n\n**Input Parameters:**\n- `score`: Verifiable rewards (labels), shape `[batch_size, 1]`\n- `rollout_n`: Number of responses generated per prompt\n- `mode`: Weight computation mode, 'question'/'only_positive'/'only_negative'\n\n**Detailed Description:**\nFunction assigns training weights to samples based on different modes. In \"question\" mode, function groups samples by rollout_n size, using class-balanced weights for groups with label imbalance to make positive and negative samples contribute equally to training. In \"only_positive\" mode, only positive samples (label 1) are assigned weight 1.0, negative samples get weight 0. In \"only_negative\" mode, the opposite applies: only negative samples (label 0) are assigned weight 1.0. If an invalid mode is specified, function raises an exception.\n\n**Expected Output:**\n- `weight`: Sample weights, shape `[batch_size, 1]`\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def compute_weight(score, rollout_n, mode=\"question\"):\n    \"\"\"\n    Compute sample weights to handle class imbalance.\n\n    Args:\n        score: array-like of shape [batch_size, 1] or [batch_size], containing labels (0 or 1).\n        rollout_n: array-like of shape [batch_size], integers indicating group size per prompt.\n        mode: one of \"question\" | \"only_positive\" | \"only_negative\"\n\n    Returns:\n        numpy.ndarray of shape [batch_size, 1] with computed weights.\n    \"\"\"\n    import numpy as np\n    from collections import defaultdict\n\n    s = np.asarray(score).reshape(-1)\n    rn = np.asarray(rollout_n).reshape(-1)\n\n    if s.shape[0] != rn.shape[0]:\n        raise ValueError(\"score and rollout_n must have the same length.\")\n\n    N = s.shape[0]\n    weights = np.zeros((N, 1), dtype=float)\n\n    if mode == \"question\":\n        # Group indices by rollout_n value\n        groups = defaultdict(list)\n        for idx, val in enumerate(rn):\n            groups[val].append(idx)\n\n        for idxs in groups.values():\n            if len(idxs) == 0:\n                continue\n            group_scores = s[idxs]\n            pos_mask = group_scores >= 0.5  # treat >=0.5 as positive\n            N_pos = int(np.sum(pos_mask))\n            N_group = len(idxs)\n            N_neg = N_group - N_pos\n\n            if N_pos > 0 and N_neg > 0:\n                w_pos = N_group / (2.0 * N_pos)\n                w_neg = N_group / (2.0 * N_neg)\n                for j, idx in enumerate(idxs):\n                    weights[idx, 0] = w_pos if pos_mask[j] else w_neg\n            else:\n                # All samples in this group have the same label; assign neutral weight\n                for idx in idxs:\n                    weights[idx, 0] = 1.0\n\n    elif mode == \"only_positive\":\n        # Positive samples get weight 1.0, negative samples get 0.0\n        weights = (s >= 0.5).astype(float).reshape(-1, 1)\n\n    elif mode == \"only_negative\":\n        # Negative samples get weight 1.0, positive samples get 0.0\n        weights = (s < 0.5).astype(float).reshape(-1, 1)\n\n    else:\n        raise ValueError(f\"Invalid mode: {mode}\")\n\n    return weights"], "usage": {"prompt_tokens": 436, "completion_tokens": 3799, "total_tokens": 4235, "per_completion": [{"prompt_tokens": 436, "completion_tokens": 3799, "total_tokens": 4235}]}}
{"function_name": "compute_pacs_loss", "overview": "Computes PACS's core loss function, using advantage function as logit and verifiable rewards as labels with binary cross-entropy loss.", "function_signature": "def compute_pacs_loss(\nprompts,\nold_log_prob,\nlog_prob,\ntoken_level_scores,\nresponse_mask,\nrollout_n,\nalgorithm_config=None,\n):", "input_parameters": "- `prompts`: Input prompts, shape `[batch_size, prompt_length]`\n- `old_log_prob`: Reference policy log probabilities, shape `[batch_size, response_length]`\n- `log_prob`: Current policy log probabilities, shape `[batch_size, response_length]`\n- `token_level_scores`: Verifiable rewards (labels), shape `[batch_size, response_length]`\n- `response_mask`: Response mask, shape `[batch_size, response_length]`\n- `rollout_n`: Number of responses generated per prompt\n- `algorithm_config`: Algorithm configuration object containing `beta`, `reward_method`, `adv_estimator`, `use_weight`, `weight_mode`, etc.", "detailed_description": "Function needs to verify that every rollout_n samples correspond to the same prompt, then compute reward values and advantage values (based on specified advantage estimator). Finally, function sums token-level scores and uses binary cross-entropy loss with advantage values as supervision signal and scores as predicted values to compute loss. If sample weighting is enabled, balanced weights are computed based on score labels to handle sample imbalance.", "output": "- `pacs_loss`: PACS loss value, scalar", "func_implementation": "", "test_code_path": "code/test_code/test_compute_pacs_loss.py", "implementation_location": "code/src/pacs/pacs_core_algos.py:line 89-138", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_pacs_loss`.\n\n**Function Overview:**\nComputes PACS's core loss function, using advantage function as logit and verifiable rewards as labels with binary cross-entropy loss.\n\n**Function Signature:**\n```python\ndef compute_pacs_loss(\nprompts,\nold_log_prob,\nlog_prob,\ntoken_level_scores,\nresponse_mask,\nrollout_n,\nalgorithm_config=None,\n):\n```\n\n**Input Parameters:**\n- `prompts`: Input prompts, shape `[batch_size, prompt_length]`\n- `old_log_prob`: Reference policy log probabilities, shape `[batch_size, response_length]`\n- `log_prob`: Current policy log probabilities, shape `[batch_size, response_length]`\n- `token_level_scores`: Verifiable rewards (labels), shape `[batch_size, response_length]`\n- `response_mask`: Response mask, shape `[batch_size, response_length]`\n- `rollout_n`: Number of responses generated per prompt\n- `algorithm_config`: Algorithm configuration object containing `beta`, `reward_method`, `adv_estimator`, `use_weight`, `weight_mode`, etc.\n\n**Detailed Description:**\nFunction needs to verify that every rollout_n samples correspond to the same prompt, then compute reward values and advantage values (based on specified advantage estimator). Finally, function sums token-level scores and uses binary cross-entropy loss with advantage values as supervision signal and scores as predicted values to compute loss. If sample weighting is enabled, balanced weights are computed based on score labels to handle sample imbalance.\n\n**Expected Output:**\n- `pacs_loss`: PACS loss value, scalar\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def compute_pacs_loss(\n    prompts,\n    old_log_prob,\n    log_prob,\n    token_level_scores,\n    response_mask,\n    rollout_n,\n    algorithm_config=None,\n):\n    \"\"\"\n    Compute PACS loss.\n\n    - Verifies that rollout_n samples correspond to the same prompt.\n    - Computes per-sample rewards by summing token-level scores (masked by response_mask).\n    - Computes an advantage value per sample (as the log-probability difference between current and old policy, masked by response_mask).\n    - Uses BCE with logits where the logits are the per-sample advantages and the targets are the per-sample rewards (clamped to [0,1]).\n    - Optionally applies per-sample weighting to handle imbalance.\n\n    Returns:\n        pacs_loss (torch.Tensor): scalar loss value\n    \"\"\"\n    import torch\n\n    # Ensure tensors\n    prompts = torch.as_tensor(prompts)\n    old_log_prob = torch.as_tensor(old_log_prob)\n    log_prob = torch.as_tensor(log_prob)\n    token_level_scores = torch.as_tensor(token_level_scores)\n    response_mask = torch.as_tensor(response_mask)\n\n    batch_size, prompt_length = prompts.shape\n\n    # Validate rollout grouping: every rollout_n samples should correspond to the same prompt\n    if rollout_n is None or rollout_n <= 0:\n        raise ValueError(\"rollout_n must be a positive integer.\")\n    if rollout_n > 1:\n        if batch_size % rollout_n != 0:\n            raise ValueError(\"Batch size must be divisible by rollout_n when rollout_n > 1.\")\n        group_n = batch_size // rollout_n\n        prompts_reshaped = prompts.view(group_n, rollout_n, prompt_length)\n        # Reference prompt for each group\n        ref = prompts_reshaped[:, 0, :].unsqueeze(1)  # shape [group_n, 1, prompt_length]\n        # Check all rollout samples in a group match the reference\n        if not torch.all(prompts_reshaped.eq(ref).all()):\n            raise ValueError(\"All rollout_n samples must correspond to the same prompt.\")\n    else:\n        group_n = batch_size  # not really used, but keep consistent\n\n    # Compute rewards: sum token-level scores over the response tokens, masked\n    # token_level_scores: [batch_size, response_length]\n    # response_mask: [batch_size, response_length]\n    rewards_per_token = token_level_scores * response_mask\n    total_reward_per_sample = rewards_per_token.sum(dim=-1)  # [batch_size]\n\n    # Compute advantages (logit for BCE)\n    # We'll use the masked sum of log_prob differences as the per-sample advantage\n    log_diff = (log_prob - old_log_prob) * response_mask  # [batch_size, response_length]\n    adv_per_sample = log_diff.sum(dim=-1)  # [batch_size]\n\n    # Labels for BCE: total rewards per sample, clamped to [0, 1] to be a valid target\n    labels = total_reward_per_sample.clamp(0.0, 1.0)\n\n    # BCE with logits: input is adv_per_sample (logits), target is labels\n    bce_loss_fn = torch.nn.BCEWithLogitsLoss(reduction='none')\n    per_sample_loss = bce_loss_fn(adv_per_sample, labels)\n\n    # Optional per-sample weighting to handle class imbalance\n    if algorithm_config is not None and getattr(algorithm_config, \"use_weight\", False):\n        # Simple balanced weighting based on label distribution\n        weight_mode = getattr(algorithm_config, \"weight_mode\", \"balanced\")\n        # Only one mode implemented: balanced\n        if weight_mode == \"balanced\":\n            # Treat labels > 0.5 as positive, <= 0.5 as negative\n            pos_mask = (labels > 0.5)\n            n_pos = pos_mask.sum().float()\n            n_total = labels.numel()\n            n_neg = n_total - n_pos\n\n            if n_pos > 0 and n_neg > 0:\n                w_pos = n_total / (2.0 * n_pos)\n                w_neg = n_total / (2.0 * n_neg)\n                weights = torch.where(pos_mask, w_pos, w_neg)\n            else:\n                weights = torch.ones_like(labels)\n            per_sample_loss = per_sample_loss * weights\n        else:\n            # For unknown weight_mode, default to no weighting\n            pass\n\n    pacs_loss = per_sample_loss.mean()\n\n    return pacs_loss"], "usage": {"prompt_tokens": 515, "completion_tokens": 4672, "total_tokens": 5187, "per_completion": [{"prompt_tokens": 515, "completion_tokens": 4672, "total_tokens": 5187}]}}
