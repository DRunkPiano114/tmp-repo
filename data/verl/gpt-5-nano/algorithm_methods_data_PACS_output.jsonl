{"function_name": "compute_reward", "overview": "Computes \"reward\" for policy improvement, measuring log probability change of current policy relative to reference policy.", "function_signature": "def compute_reward(\nold_log_prob,\nlog_prob,\nresponse_mask,\nbeta=1.0,\nreward_method=\"1\",\n):", "input_parameters": "- `old_log_prob`: Reference policy token-level log probabilities, shape `[batch_size, response_length]`\n- `log_prob`: Current policy token-level log probabilities, shape `[batch_size, response_length]`\n- `response_mask`: Response portion mask, shape `[batch_size, response_length]`\n- `beta`: Reward scaling factor, controls aggressiveness of policy updates (default 1.0)\n- `reward_method`: Reward computation method, '1' or '2'", "detailed_description": "Function computes reward values in reinforcement learning based on specified method.\nMethod 1 computes sum of new-old log probability differences, measuring policy improvement magnitude.\nMethod 2 computes normalized sum of current log probabilities.\nIf an invalid method number is specified, function raises an exception.", "output": "- `reward`: Policy improvement reward, shape `[batch_size, 1]`", "func_implementation": "", "test_code_path": "code/test_code/test_compute_reward.py", "implementation_location": "code/src/pacs/pacs_core_algos.py:line 7-21", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_reward`.\n\n**Function Overview:**\nComputes \"reward\" for policy improvement, measuring log probability change of current policy relative to reference policy.\n\n**Function Signature:**\n```python\ndef compute_reward(\nold_log_prob,\nlog_prob,\nresponse_mask,\nbeta=1.0,\nreward_method=\"1\",\n):\n```\n\n**Input Parameters:**\n- `old_log_prob`: Reference policy token-level log probabilities, shape `[batch_size, response_length]`\n- `log_prob`: Current policy token-level log probabilities, shape `[batch_size, response_length]`\n- `response_mask`: Response portion mask, shape `[batch_size, response_length]`\n- `beta`: Reward scaling factor, controls aggressiveness of policy updates (default 1.0)\n- `reward_method`: Reward computation method, '1' or '2'\n\n**Detailed Description:**\nFunction computes reward values in reinforcement learning based on specified method.\nMethod 1 computes sum of new-old log probability differences, measuring policy improvement magnitude.\nMethod 2 computes normalized sum of current log probabilities.\nIf an invalid method number is specified, function raises an exception.\n\n**Expected Output:**\n- `reward`: Policy improvement reward, shape `[batch_size, 1]`\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def compute_reward(\n    old_log_prob,\n    log_prob,\n    response_mask,\n    beta=1.0,\n    reward_method=\"1\",\n):\n    \"\"\"\n    Computes policy improvement reward.\n\n    Args:\n        old_log_prob (Tensor): Reference policy token-level log probabilities, shape [batch_size, response_length]\n        log_prob (Tensor): Current policy token-level log probabilities, shape [batch_size, response_length]\n        response_mask (Tensor): Response portion mask, shape [batch_size, response_length]\n        beta (float, optional): Reward scaling factor. Defaults to 1.0\n        reward_method (str, optional): Reward computation method, '1' or '2'. Defaults to '1'\n\n    Returns:\n        Tensor: Reward values, shape [batch_size, 1]\n    \"\"\"\n    import torch\n\n    if reward_method not in {\"1\", \"2\"}:\n        raise ValueError(\n            f\"Invalid reward_method: {reward_method}. Expected '1' or '2'.\"\n        )\n\n    # Ensure we operate with a consistent dtype for the mask\n    mask = response_mask.to(log_prob.dtype)\n\n    if reward_method == \"1\":\n        # Method 1: sum of (new_log_prob - old_log_prob) over response tokens\n        diff = (log_prob - old_log_prob) * mask  # [B, L]\n        reward = diff.sum(dim=1, keepdim=True)  # [B, 1]\n        reward = reward * beta\n        return reward\n    else:\n        # Method 2: normalized sum of current log probabilities over response tokens\n        sum_log = (log_prob * mask).sum(dim=1, keepdim=True)  # [B, 1]\n        token_count = mask.sum(dim=1, keepdim=True)  # [B, 1]\n        normalized = sum_log / (token_count + 1e-8)\n        reward = normalized * beta\n        return reward"], "usage": {"prompt_tokens": 440, "completion_tokens": 1697, "total_tokens": 2137, "per_completion": [{"prompt_tokens": 440, "completion_tokens": 1697, "total_tokens": 2137}]}}
{"function_name": "compute_weight", "overview": "Computes sample weights to handle class imbalance problem (uneven positive/negative sample ratio).", "function_signature": "def compute_weight(score, rollout_n, mode=\"question\"):", "input_parameters": "- `score`: Verifiable rewards (labels), shape `[batch_size, 1]`\n- `rollout_n`: Number of responses generated per prompt\n- `mode`: Weight computation mode, 'question'/'only_positive'/'only_negative'", "detailed_description": "Function assigns training weights to samples based on different modes. In \"question\" mode, function groups samples by rollout_n size, using class-balanced weights for groups with label imbalance to make positive and negative samples contribute equally to training. In \"only_positive\" mode, only positive samples (label 1) are assigned weight 1.0, negative samples get weight 0. In \"only_negative\" mode, the opposite applies: only negative samples (label 0) are assigned weight 1.0. If an invalid mode is specified, function raises an exception.", "output": "- `weight`: Sample weights, shape `[batch_size, 1]`", "func_implementation": "", "test_code_path": "code/test_code/test_compute_weight.py", "implementation_location": "code/src/pacs/pacs_core_algos.py:line 59-86", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_weight`.\n\n**Function Overview:**\nComputes sample weights to handle class imbalance problem (uneven positive/negative sample ratio).\n\n**Function Signature:**\n```python\ndef compute_weight(score, rollout_n, mode=\"question\"):\n```\n\n**Input Parameters:**\n- `score`: Verifiable rewards (labels), shape `[batch_size, 1]`\n- `rollout_n`: Number of responses generated per prompt\n- `mode`: Weight computation mode, 'question'/'only_positive'/'only_negative'\n\n**Detailed Description:**\nFunction assigns training weights to samples based on different modes. In \"question\" mode, function groups samples by rollout_n size, using class-balanced weights for groups with label imbalance to make positive and negative samples contribute equally to training. In \"only_positive\" mode, only positive samples (label 1) are assigned weight 1.0, negative samples get weight 0. In \"only_negative\" mode, the opposite applies: only negative samples (label 0) are assigned weight 1.0. If an invalid mode is specified, function raises an exception.\n\n**Expected Output:**\n- `weight`: Sample weights, shape `[batch_size, 1]`\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def compute_weight(score, rollout_n, mode=\"question\"):\n    \"\"\"\n    Compute per-sample weights to handle class imbalance.\n\n    Modes:\n    - \"question\": balance positives/negatives within groups of samples that share the same rollout_n.\n    - \"only_positive\": weight = 1.0 for positive samples (score == 1), 0.0 otherwise.\n    - \"only_negative\": weight = 1.0 for negative samples (score == 0), 0.0 otherwise.\n\n    Returns:\n        weight: Tensor of shape [batch_size, 1] with dtype matching score, on the same device.\n    \"\"\"\n    import torch\n\n    if not isinstance(score, torch.Tensor):\n        score = torch.tensor(score)\n\n    # Ensure score has shape [batch_size, 1]\n    if score.dim() == 1:\n        score = score.view(-1, 1)\n    elif score.dim() != 2 or score.size(1) != 1:\n        score = score.view(-1, 1)\n\n    batch_size = score.size(0)\n    device = score.device\n    dtype = score.dtype\n\n    # Validate mode\n    valid_modes = {\"question\", \"only_positive\", \"only_negative\"}\n    if mode not in valid_modes:\n        raise ValueError(f\"Invalid mode: {mode}. Supported modes: {valid_modes}\")\n\n    if mode == \"only_positive\":\n        weight = (score == 1).float()\n        return weight\n\n    if mode == \"only_negative\":\n        weight = (score == 0).float()\n        return weight\n\n    # mode == \"question\": group by rollout_n and balance within each group\n    # Prepare group_ids with same length as batch\n    if isinstance(rollout_n, torch.Tensor):\n        rn = rollout_n.view(-1)\n        if rn.numel() != batch_size:\n            if rn.numel() == 1:\n                rn = rn.expand(batch_size)\n            else:\n                raise ValueError(\"rollout_n length does not match batch size of score\")\n        group_ids = rn.to(torch.long)\n    else:\n        # scalar rollout_n\n        group_ids = torch.full((batch_size,), int(rollout_n), dtype=torch.long, device=device)\n\n    unique_ids = torch.unique(group_ids)\n\n    weight = torch.zeros((batch_size, 1), dtype=dtype, device=device)\n\n    for gid in unique_ids:\n        idx = (group_ids == gid).nonzero(as_tuple=False).squeeze(-1)\n        if idx.numel() == 0:\n            continue\n\n        group_scores = score[idx].view(-1)  # [group_size]\n\n        # Count positives and negatives within the group\n        p = int((group_scores == 1).sum().item())\n        n = int((group_scores == 0).sum().item())\n        total = p + n\n\n        if total > 0:\n            w_pos = float(n) / float(total)\n            w_neg = float(p) / float(total)\n        else:\n            w_pos, w_neg = 0.0, 0.0\n\n        # Handle edge cases where one class is missing\n        if p == 0 and n > 0:\n            w_neg = 1.0\n        if n == 0 and p > 0:\n            w_pos = 1.0\n\n        # Build weights for the group: pos -> w_pos, neg -> w_neg\n        weights_group = torch.where(group_scores == 1,\n                                    torch.tensor(w_pos, dtype=dtype, device=device),\n                                    torch.tensor(w_neg, dtype=dtype, device=device))\n        weight[idx, 0] = weights_group\n\n    return weight"], "usage": {"prompt_tokens": 436, "completion_tokens": 4374, "total_tokens": 4810, "per_completion": [{"prompt_tokens": 436, "completion_tokens": 4374, "total_tokens": 4810}]}}
{"function_name": "compute_pacs_loss", "overview": "Computes PACS's core loss function, using advantage function as logit and verifiable rewards as labels with binary cross-entropy loss.", "function_signature": "def compute_pacs_loss(\nprompts,\nold_log_prob,\nlog_prob,\ntoken_level_scores,\nresponse_mask,\nrollout_n,\nalgorithm_config=None,\n):", "input_parameters": "- `prompts`: Input prompts, shape `[batch_size, prompt_length]`\n- `old_log_prob`: Reference policy log probabilities, shape `[batch_size, response_length]`\n- `log_prob`: Current policy log probabilities, shape `[batch_size, response_length]`\n- `token_level_scores`: Verifiable rewards (labels), shape `[batch_size, response_length]`\n- `response_mask`: Response mask, shape `[batch_size, response_length]`\n- `rollout_n`: Number of responses generated per prompt\n- `algorithm_config`: Algorithm configuration object containing `beta`, `reward_method`, `adv_estimator`, `use_weight`, `weight_mode`, etc.", "detailed_description": "Function needs to verify that every rollout_n samples correspond to the same prompt, then compute reward values and advantage values (based on specified advantage estimator). Finally, function sums token-level scores and uses binary cross-entropy loss with advantage values as supervision signal and scores as predicted values to compute loss. If sample weighting is enabled, balanced weights are computed based on score labels to handle sample imbalance.", "output": "- `pacs_loss`: PACS loss value, scalar", "func_implementation": "", "test_code_path": "code/test_code/test_compute_pacs_loss.py", "implementation_location": "code/src/pacs/pacs_core_algos.py:line 89-138", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_pacs_loss`.\n\n**Function Overview:**\nComputes PACS's core loss function, using advantage function as logit and verifiable rewards as labels with binary cross-entropy loss.\n\n**Function Signature:**\n```python\ndef compute_pacs_loss(\nprompts,\nold_log_prob,\nlog_prob,\ntoken_level_scores,\nresponse_mask,\nrollout_n,\nalgorithm_config=None,\n):\n```\n\n**Input Parameters:**\n- `prompts`: Input prompts, shape `[batch_size, prompt_length]`\n- `old_log_prob`: Reference policy log probabilities, shape `[batch_size, response_length]`\n- `log_prob`: Current policy log probabilities, shape `[batch_size, response_length]`\n- `token_level_scores`: Verifiable rewards (labels), shape `[batch_size, response_length]`\n- `response_mask`: Response mask, shape `[batch_size, response_length]`\n- `rollout_n`: Number of responses generated per prompt\n- `algorithm_config`: Algorithm configuration object containing `beta`, `reward_method`, `adv_estimator`, `use_weight`, `weight_mode`, etc.\n\n**Detailed Description:**\nFunction needs to verify that every rollout_n samples correspond to the same prompt, then compute reward values and advantage values (based on specified advantage estimator). Finally, function sums token-level scores and uses binary cross-entropy loss with advantage values as supervision signal and scores as predicted values to compute loss. If sample weighting is enabled, balanced weights are computed based on score labels to handle sample imbalance.\n\n**Expected Output:**\n- `pacs_loss`: PACS loss value, scalar\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def compute_pacs_loss(\n    prompts,\n    old_log_prob,\n    log_prob,\n    token_level_scores,\n    response_mask,\n    rollout_n,\n    algorithm_config=None,\n):\n    \"\"\"\n    Compute PACS loss for a batch of prompt-rollout samples.\n\n    Assumptions:\n    - Each prompt is rolled out rollout_n times. All samples within the same rollout group\n      must correspond to the same prompt.\n    - token_level_scores provides per-token verifiable rewards; response_mask indicates\n      which tokens are valid for the corresponding samples.\n    - The loss uses BCE with logits where:\n        input = summed advantage (per-sample), target = summed rewards (per-sample)\n        Advantage is computed per-token as log_prob - old_log_prob.\n    - If weighting is enabled via algorithm_config, per-sample weights are applied to the loss.\n\n    Parameters:\n    - prompts: Tensor [batch_size, prompt_length]\n    - old_log_prob: Tensor [batch_size, response_length]\n    - log_prob: Tensor [batch_size, response_length]\n    - token_level_scores: Tensor [batch_size, response_length]\n    - response_mask: Tensor [batch_size, response_length]\n    - rollout_n: int\n    - algorithm_config: object or dict containing optional fields:\n        * adv_estimator: str (default 'log_ratio')\n        * use_weight: bool (default False)\n        * weight_mode: str (e.g., 'balanced', 'inverse_frequency') (default 'balanced')\n    Returns:\n    - pacs_loss: scalar tensor\n    \"\"\"\n\n    import torch\n    import torch.nn as nn\n\n    # Helper to read config attributes from dict or object\n    def _get_config_attr(cfg, name, default=None):\n        if cfg is None:\n            return default\n        if isinstance(cfg, dict):\n            return cfg.get(name, default)\n        return getattr(cfg, name, default)\n\n    # Basic validation and rollout consistency check\n    if rollout_n <= 0:\n        raise ValueError(\"rollout_n must be a positive integer.\")\n\n    batch_size = prompts.size(0)\n    if batch_size % rollout_n != 0:\n        raise ValueError(\"batch_size must be divisible by rollout_n to form rollout groups.\")\n\n    group_cnt = batch_size // rollout_n\n    # Verify that every group of rollout_n samples corresponds to the same prompt\n    for g in range(group_cnt):\n        base_idx = g * rollout_n\n        base_prompt = prompts[base_idx]\n        for r in range(1, rollout_n):\n            curr_prompt = prompts[base_idx + r]\n            if not torch.equal(base_prompt, curr_prompt):\n                raise ValueError(\n                    f\"Inconsistent prompts within rollout group {g}: \"\n                    f\"samples {base_idx} and {base_idx + r} do not match.\"\n                )\n\n    # Advantage estimator configuration (default to 'log_ratio')\n    adv_estimator = _get_config_attr(algorithm_config, \"adv_estimator\", \"log_ratio\")\n    # For this implementation, we support a simple 'log_ratio' estimator: adv = log_prob - old_log_prob\n    if adv_estimator not in {\"log_ratio\", \"log_ratio_eps\"}:\n        # Fallback to log_ratio for unknown values\n        adv_estimator = \"log_ratio\"\n\n    # Compute per-token advantage\n    # adv_per_token shape: [batch_size, response_length]\n    adv_per_token = log_prob - old_log_prob\n    # If additional estimators are added later, they can be integrated here\n\n    # Compute per-sample reward by summing token-level scores with the token mask\n    # reward_per_sample shape: [batch_size]\n    token_mask = response_mask\n    reward_per_sample = torch.sum(token_level_scores * token_mask, dim=1)\n\n    # Targets for BCE loss: clamp rewards to [0, 1]\n    targets = torch.clamp(reward_per_sample, min=0.0, max=1.0)\n\n    # Compute per-sample total advantage by summing over tokens with the mask\n    # adv_sum_per_sample shape: [batch_size]\n    adv_sum_per_sample = torch.sum(adv_per_token * token_mask, dim=1)\n\n    # Compute optional sample weights to handle imbalance\n    weights = None\n    use_weight = _get_config_attr(algorithm_config, \"use_weight\", False)\n    if use_weight:\n        weight_mode = _get_config_attr(algorithm_config, \"weight_mode\", \"balanced\")\n        # Simple balanced weighting based on binary threshold of targets\n        if weight_mode == \"balanced\":\n            # Create binary labels from targets for weighting\n            labels_bin = (targets > 0.5).float()\n            pos_ratio = labels_bin.mean().item()\n            neg_ratio = 1.0 - pos_ratio\n            # Avoid division by zero\n            pos_w = 1.0 / max(pos_ratio, 1e-6)\n            neg_w = 1.0 / max(neg_ratio, 1e-6)\n            weights = labels_bin * pos_w + (1.0 - labels_bin) * neg_w\n            # Normalize weights to maintain stable scale\n            weights = weights / (weights.mean().item() + 1e-6)\n        elif weight_mode == \"inverse_frequency\":\n            thresh = 0.5\n            labels_bin = (targets > thresh).float()\n            pos_count = labels_bin.sum().item()\n            total = labels_bin.numel()\n            neg_count = total - pos_count\n            pos_w = 1.0 / max(pos_count, 1e-6)\n            neg_w = 1.0 / max(neg_count, 1e-6)\n            weights = labels_bin * pos_w + (1.0 - labels_bin) * neg_w\n            weights = weights / (weights.mean().item() + 1e-6)\n        else:\n            # Default to uniform weights if unknown mode\n            weights = torch.ones(batch_size, device=targets.device, dtype=targets.dtype)\n    else:\n        weights = torch.ones(batch_size, device=targets.device, dtype=targets.dtype)\n\n    # BCE with logits expects input logits and target in [0,1]\n    bce_loss = nn.BCEWithLogitsLoss(reduction=\"none\")\n    per_sample_loss = bce_loss(adv_sum_per_sample, targets)\n\n    # Apply weights and aggregate to scalar loss\n    if weights is not None:\n        weighted_loss = per_sample_loss * weights\n        pacs_loss = weighted_loss.sum() / (weights.sum() + 1e-6)\n    else:\n        pacs_loss = per_sample_loss.mean()\n\n    return pacs_loss"], "usage": {"prompt_tokens": 515, "completion_tokens": 6308, "total_tokens": 6823, "per_completion": [{"prompt_tokens": 515, "completion_tokens": 6308, "total_tokens": 6823}]}}
