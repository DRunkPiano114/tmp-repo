{"function_name": "ProcessRewardModelWorker._forward_micro_batch", "overview": "Computes token-level scores from the Process Reward Model (PRM) and performs approximate weighting for min-form credit assignment based on configuration.", "function_signature": "def _forward_micro_batch(self, micro_batch):", "input_parameters": "- `micro_batch`: Micro-batch data containing `input_ids`, `attention_mask`, `position_ids`, `responses`, `reward_mask`, etc.\n- `response_length`: Response sequence length (inferred from batch tensors)\n- `temperature`: Temperature for approximate min-form (when `reward_model.credit_assignment` is a float)", "detailed_description": "Performs forward pass on the Process Reward Model (PRM), computing token-level scores via softmax to obtain the positive-negative class probability difference. Uses masking to retain only valid scores from the response portion.\nIf depadding or sequence parallelism optimization is enabled, padding must be removed before computation and restored afterward.\nCredit assignment weighting executes different strategies based on configuration mode: min-form, strict min-form, or gamma-decay. Lower-scoring tokens should receive higher weights.", "output": "- `rm_score`: Token-level process reward scores (shape `[batch_size, response_length]`)", "func_implementation": "", "test_code_path": "code/test_code/test_forward_micro_batch.py", "implementation_location": "code/verl/workers/fsdp_workers.py:line 1462-1539", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `ProcessRewardModelWorker._forward_micro_batch`.\n\n**Function Overview:**\nComputes token-level scores from the Process Reward Model (PRM) and performs approximate weighting for min-form credit assignment based on configuration.\n\n**Function Signature:**\n```python\ndef _forward_micro_batch(self, micro_batch):\n```\n\n**Input Parameters:**\n- `micro_batch`: Micro-batch data containing `input_ids`, `attention_mask`, `position_ids`, `responses`, `reward_mask`, etc.\n- `response_length`: Response sequence length (inferred from batch tensors)\n- `temperature`: Temperature for approximate min-form (when `reward_model.credit_assignment` is a float)\n\n**Detailed Description:**\nPerforms forward pass on the Process Reward Model (PRM), computing token-level scores via softmax to obtain the positive-negative class probability difference. Uses masking to retain only valid scores from the response portion.\nIf depadding or sequence parallelism optimization is enabled, padding must be removed before computation and restored afterward.\nCredit assignment weighting executes different strategies based on configuration mode: min-form, strict min-form, or gamma-decay. Lower-scoring tokens should receive higher weights.\n\n**Expected Output:**\n- `rm_score`: Token-level process reward scores (shape `[batch_size, response_length]`)\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def _forward_micro_batch(self, micro_batch):\n    \"\"\"\n    Compute token-level process reward scores (rm_score) for a micro-batch from the Process Reward Model (PRM),\n    with optional approximate min-form / gamma-decay credit assignment weighting.\n\n    Returns:\n        rm_score: Tensor of shape [batch_size, response_length]\n    \"\"\"\n    import torch\n    import torch.nn.functional as F\n\n    # Locate PRM model (Process Reward Model)\n    prm = getattr(self, \"prm\", None)\n    if prm is None:\n        prm = getattr(self, \"reward_model\", None)\n    if prm is None:\n        raise RuntimeError(\"Process Reward Model (PRM) not found on the worker.\")\n\n    # Extract batch inputs for PRM forward pass\n    input_ids = micro_batch.get(\"input_ids\", None)\n    attention_mask = micro_batch.get(\"attention_mask\", None)\n    position_ids = micro_batch.get(\"position_ids\", None)\n    responses = micro_batch.get(\"responses\", None)\n\n    # Forward through PRM; be tolerant to a few possible signature variations\n    try:\n        prm_outputs = prm(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            responses=responses,\n        )\n    except TypeError:\n        # Fallback if PRM does not accept 'responses'\n        prm_outputs = prm(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n        )\n\n    # If PRM returns a tuple (e.g., (logits, other_state)), take the logits\n    if isinstance(prm_outputs, tuple) or isinstance(prm_outputs, list):\n        rm_logits = prm_outputs[0]\n    else:\n        rm_logits = prm_outputs\n\n    # If logits come as a dict (some models), extract expected key\n    if isinstance(rm_logits, dict):\n        # Try common keys\n        for key in (\"logits\", \"token_logits\", \"scores\"):\n            if key in rm_logits:\n                rm_logits = rm_logits[key]\n                break\n\n    # Ensure tensor\n    if not isinstance(rm_logits, torch.Tensor):\n        raise RuntimeError(\"PRM forward did not return a tensor for logits.\")\n\n    # Compute token-level positive-negative difference via softmax over two classes when available\n    # Expect rm_logits to have last dimension = 2 for a 2-class per-token output\n    if rm_logits.dim() >= 3 and rm_logits.size(-1) == 2:\n        probs = F.softmax(rm_logits, dim=-1)\n        # Assuming class 1 is the \"positive\" class\n        diff = probs[..., 1] - probs[..., 0]  # shape: [batch, seq_len]\n    elif rm_logits.size(-1) == 1:\n        # Fallback: single logit per token (positive logit); use sigmoid to get probability\n        probs = torch.sigmoid(rm_logits)\n        diff = 2.0 * probs - 1.0\n        diff = diff.squeeze(-1)\n    else:\n        # General multi-class case: use max-min probability gap as a proxy\n        probs = F.softmax(rm_logits, dim=-1)\n        diff = probs.max(dim=-1)[0] - probs.min(dim=-1)[0]\n\n    # diff now has shape [batch_size, seq_len] (or possibly with extra dims if inputs\n    # had more dims; normalize to [B, S])\n    if diff.dim() > 2:\n        # Collapse to [B, S] by selecting the last two dims if necessary\n        diff = diff.view(diff.size(0), diff.size(-2))\n\n    # Determine response length and the response region within each sequence\n    seq_len = diff.size(-1)\n    response_length = micro_batch.get(\"response_length\", seq_len)\n    if response_length is None:\n        response_length = seq_len\n    # Ensure non-negative and within bounds\n    response_length = max(0, min(int(response_length), seq_len))\n    start_idx = seq_len - response_length\n    if start_idx < 0:\n        start_idx = 0\n\n    # Extract the response portion scores\n    rm_score = diff[:, start_idx:]  # [B, response_length]\n\n    # Apply masking to keep only valid tokens from the response portion\n    # 1) Attention mask (non-padding)\n    if attention_mask is not None:\n        # If attention_mask is [B, S], mask the response region\n        attn_for_resp = attention_mask[:, start_idx:]\n        if attn_for_resp.dtype != torch.float32:\n            attn_for_resp = attn_for_resp.to(dtype=torch.float32)\n        rm_score = rm_score * attn_for_resp\n\n    # 2) Reward mask if provided (explicit validity mask for tokens)\n    reward_mask = micro_batch.get(\"reward_mask\", None)\n    if reward_mask is not None:\n        # Align to response region\n        if reward_mask.dim() == 2:\n            mask_for_resp = reward_mask[:, start_idx:]\n        elif reward_mask.dim() == 1 and reward_mask.numel() == seq_len:\n            mask_for_resp = reward_mask[start_idx:]\n        else:\n            mask_for_resp = reward_mask\n        if mask_for_resp.dtype != torch.float32:\n            mask_for_resp = mask_for_resp.to(dtype=torch.float32)\n        # Broadcast if necessary\n        if mask_for_resp.size(-1) != rm_score.size(-1):\n            # Attempt to broadcast along batch dimension\n            mask_for_resp = mask_for_resp.view(mask_for_resp.size(0), -1)\n            if mask_for_resp.size(-1) != rm_score.size(-1):\n                # As a fallback, create a mask of ones for matching length\n                mask_for_resp = torch.ones_like(rm_score)\n        rm_score = rm_score * mask_for_resp\n\n    # Optional: Depadding / sequence-parallel optimization handling\n    depadding_enabled = False\n    if hasattr(self, \"config\") and getattr(self.config, \"depadding\", False):\n        depadding_enabled = True\n    if micro_batch.get(\"depadding_enabled\", False):\n        depadding_enabled = True\n\n    if depadding_enabled:\n        # If padding exists, ensure we zero-out scores for padded tokens in the response region\n        if attention_mask is not None:\n            attn_resp = attention_mask[:, start_idx:]\n            rm_score = rm_score * attn_resp.to(dtype=rm_score.dtype)\n\n    # Credit assignment weighting (min-form / strict min-form / gamma-decay)\n    credit_assignment = None\n    if hasattr(self, \"reward_model\"):\n        credit_assignment = getattr(self.reward_model, \"credit_assignment\", None)\n\n    # Helper: ensure rm_score remains [B, response_length]\n    if rm_score.dim() != 2 or rm_score.size(1) != int(response_length):\n        rm_score = rm_score.view(rm_score.size(0), -1)\n\n    # If credit assignment is a numeric temperature, or specified modes, apply weighting\n    temperature = None\n    mode = None\n\n    if isinstance(credit_assignment, (float, int)):\n        temperature = float(credit_assignment)\n        mode = \"gamma_decay\"  # treated as gamma-decay style with explicit temperature\n    elif isinstance(credit_assignment, str):\n        s = credit_assignment.lower()\n        if s in (\"min-form\", \"min_form\"):\n            mode = \"min_form\"\n        elif s in (\"strict_min_form\", \"strict-min-form\", \"strictminform\"):\n            mode = \"strict_min_form\"\n        elif s in (\"gamma_decay\", \"gamma-decay\"):\n            mode = \"gamma_decay\"\n        else:\n            mode = None\n    elif credit_assignment is None:\n        mode = None\n\n    if mode is not None:\n        # For min-form and strict min-form, we assume a per-token weighting across the response tokens\n        # Use a temperature if provided; otherwise default to 1.0\n        if temperature is None:\n            if mode == \"gamma_decay\":\n                temperature = 1.0\n            else:\n                temperature = 1.0\n        if mode == \"min_form\" or mode == \"gamma_decay\" or (isinstance(credit_assignment, (float, int))):\n            # Compute weights emphasizing lower-scoring tokens\n            # weight_i \\propto exp(-diff_i / temperature)\n            w = torch.softmax(-rm_score / (temperature if temperature != 0 else 1.0), dim=-1)\n            rm_score = rm_score * w\n        elif mode == \"strict_min_form\":\n            # Only keep tokens with minimum score within the response\n            min_vals, _ = rm_score.min(dim=-1, keepdim=True)\n            strict_mask = (rm_score <= min_vals).float()\n            rm_score = rm_score * strict_mask\n        else:\n            # Fallback: no change\n            pass\n\n    # Ensure final shape is [batch_size, response_length]\n    if rm_score.dim() == 3:\n        rm_score = rm_score.view(rm_score.size(0), rm_score.size(1))\n\n    return rm_score"], "usage": {"prompt_tokens": 450, "completion_tokens": 6717, "total_tokens": 7167, "per_completion": [{"prompt_tokens": 450, "completion_tokens": 6717, "total_tokens": 7167}]}}
{"function_name": "ProcessRewardModelWorker.compute_rm_score", "overview": "Batch-constructs PRM inputs (splitting by steps and appending newlines after each step), performs inference with (optional) variable-length sharding/backfilling, and aggregates to obtain batch-level token-level `rm_scores`.", "function_signature": "def compute_rm_score(self, data:DataProto):", "input_parameters": "- `data`: `DataProto` containing `responses`, `reward_mask`, etc.;", "detailed_description": "Splits response sequences by steps to construct input format for the process reward model.\nIn dynamic batch mode, reorganizes data into multiple mini-batches based on total token count limit and records original order; in fixed batch mode, evenly splits by sample count.\nCalls forward function for each mini-batch to compute token-level scores, concatenating results from all batches. In dynamic batch mode, restores results to original order based on recorded indices.", "output": "- `DataProto`: `batch['rm_scores']` with shape `[batch_size, response_length]`", "func_implementation": "", "test_code_path": "code/test_code/test_compute_rm_score.py", "implementation_location": "code/verl/workers/fsdp_workers.py:line 1541-1594", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `ProcessRewardModelWorker.compute_rm_score`.\n\n**Function Overview:**\nBatch-constructs PRM inputs (splitting by steps and appending newlines after each step), performs inference with (optional) variable-length sharding/backfilling, and aggregates to obtain batch-level token-level `rm_scores`.\n\n**Function Signature:**\n```python\ndef compute_rm_score(self, data:DataProto):\n```\n\n**Input Parameters:**\n- `data`: `DataProto` containing `responses`, `reward_mask`, etc.;\n\n**Detailed Description:**\nSplits response sequences by steps to construct input format for the process reward model.\nIn dynamic batch mode, reorganizes data into multiple mini-batches based on total token count limit and records original order; in fixed batch mode, evenly splits by sample count.\nCalls forward function for each mini-batch to compute token-level scores, concatenating results from all batches. In dynamic batch mode, restores results to original order based on recorded indices.\n\n**Expected Output:**\n- `DataProto`: `batch['rm_scores']` with shape `[batch_size, response_length]`\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def compute_rm_score(self, data: DataProto):\n    \"\"\"\n    Compute token-level reward-model scores (rm_scores) for a batch of responses.\n\n    Process:\n    - Split each response into steps (lines separated by newline), and construct\n      PRM inputs by appending a newline after each step.\n    - If dynamic batch mode is enabled, partition inputs into mini-batches by total\n      token count (with a cap), recording original order for later restoration.\n    - If fixed batch mode, partition inputs into mini-batches by a fixed sample count.\n    - Run the forward pass for each mini-batch to obtain token-level scores.\n    - Concatenate results in original order and pad rows to the maximum response length.\n    - Store the final rm_scores into data.batch['rm_scores'] (or a suitable fallback).\n\n    The function returns the input DataProto (data) with rm_scores populated.\n    \"\"\"\n    # Helper to count tokens in a response (rough estimate based on whitespace)\n    def _count_tokens(resp) -> int:\n        if isinstance(resp, str):\n            # simple token count approximation\n            return max(1, len(resp.split()))\n        elif isinstance(resp, (list, tuple)):\n            return max(1, len(resp))\n        else:\n            return 0\n\n    # Helper to build PRM input from a response by steps (lines)\n    def _build_prm_input_from_response(resp) -> str:\n        if resp is None:\n            return \"\"\n        # Split by newline into steps, ignore empty steps\n        steps = [line.strip() for line in str(resp).split('\\n') if line.strip() != \"\"]\n        # Append a newline after each step\n        return ''.join([step + '\\n' for step in steps])\n\n    # Extract responses from DataProto (with fallbacks)\n    responses = []\n    if hasattr(data, 'responses') and data.responses is not None:\n        responses = list(data.responses)\n    elif hasattr(data, 'batch') and isinstance(data.batch, dict) and 'responses' in data.batch:\n        responses = list(data.batch['responses'])\n    else:\n        responses = []\n\n    batch_size = len(responses)\n\n    # If no responses, ensure rm_scores is at least an empty structure\n    if batch_size == 0:\n        all_rm_scores = []\n        # Attach to data in a robust way\n        try:\n            if isinstance(getattr(data, 'batch', None), dict):\n                data.batch['rm_scores'] = all_rm_scores\n            else:\n                setattr(data, 'rm_scores', all_rm_scores)\n        except Exception:\n            pass\n        return data\n\n    # Build PRM inputs for each response\n    prm_inputs = [_build_prm_input_from_response(r) for r in responses]\n\n    # Compute max response length for padding (token-based)\n    resp_lengths = [_count_tokens(r) for r in responses]\n    max_resp_len = max(resp_lengths) if resp_lengths else 0\n\n    # Determine batching strategy\n    dynamic_batch = getattr(self, 'rm_dynamic_batch', False) or getattr(self, 'dynamic_batch', False)\n    rm_batch_token_limit = getattr(self, 'rm_batch_token_limit', 1024)  # default token limit per dynamic batch\n    fixed_batch_size = getattr(self, 'fixed_batch_size', 8)  # default mini-batch size for fixed batching\n\n    batches_info = []  # list of tuples: (indices, batch_inputs)\n    if dynamic_batch:\n        # Greedy dynamic batching by token budget\n        current_indices = []\n        current_inputs = []\n        current_tokens = 0\n\n        for idx, s in enumerate(prm_inputs):\n            tok = max(1, len(s.split()))\n            if current_tokens + tok > rm_batch_token_limit and current_indices:\n                batches_info.append((current_indices, current_inputs))\n                current_indices = []\n                current_inputs = []\n                current_tokens = 0\n            current_indices.append(idx)\n            current_inputs.append(s)\n            current_tokens += tok\n\n        if current_indices:\n            batches_info.append((current_indices, current_inputs))\n    else:\n        # Fixed-size mini-batches (even split by sample count)\n        for i in range(0, batch_size, max(1, fixed_batch_size)):\n            idxs = list(range(i, min(i + fixed_batch_size, batch_size)))\n            batch_inputs = [prm_inputs[j] for j in idxs]\n            batches_info.append((idxs, batch_inputs))\n\n    # Prepare storage for all rm_scores in original order\n    all_rm_scores = [None] * batch_size\n\n    # Iterate over batches and run forward\n    for batch_indices, batch_inputs in batches_info:\n        # Prepare forward input payload (best-effort structure; adapt as needed)\n        forward_input = {\n            'inputs': batch_inputs\n        }\n\n        # If the forward method expects additional fields, include the relevant subset\n        if isinstance(data, DataProto):\n            # Try to pass through corresponding responses/reward_mask if available\n            sub_responses = [responses[i] for i in batch_indices]\n            forward_input['responses'] = sub_responses\n            # Optional: pass reward_mask if present and aligned\n            reward_mask = None\n            if hasattr(data, 'reward_mask') and data.reward_mask is not None:\n                try:\n                    reward_mask = [data.reward_mask[i] for i in batch_indices]\n                except Exception:\n                    reward_mask = None\n            if reward_mask is not None:\n                forward_input['reward_mask'] = reward_mask\n\n        # Call forward (the actual model inference)\n        forward_out = self.forward(forward_input)\n\n        # Extract rm_scores from forward_out\n        batch_rm = None\n        if isinstance(forward_out, dict):\n            # Common keys that might hold token-level scores\n            for key in ('rm_scores', 'token_scores', 'scores'):\n                if key in forward_out:\n                    batch_rm = forward_out[key]\n                    break\n        elif isinstance(forward_out, (list, tuple)):\n            batch_rm = forward_out\n\n        # Normalize batch_rm to a list of rows\n        if batch_rm is None:\n            # Fallback: generate zero scores with max_resp_len per sample\n            for t, idx in enumerate(batch_indices):\n                resp_len = max_resp_len\n                all_rm_scores[idx] = [0.0] * resp_len\n            continue\n\n        # If batch_rm is a single row but batch size > 1, try to broadcast\n        if isinstance(batch_rm, (list, tuple)):\n            if len(batch_rm) == len(batch_indices):\n                # Each element corresponds to a sample\n                for local_pos, global_idx in enumerate(batch_indices):\n                    row = batch_rm[local_pos]\n                    # Ensure row is a list\n                    if not isinstance(row, (list, tuple)):\n                        row = [row]\n                    row = list(row)\n                    # Pad to max_resp_len\n                    if len(row) < max_resp_len:\n                        row += [0.0] * (max_resp_len - len(row))\n                    all_rm_scores[global_idx] = row\n            else:\n                # If shapes don't line up, attempt to treat batch_rm as a 2D array-like\n                try:\n                    for local_pos, global_idx in enumerate(batch_indices):\n                        row = batch_rm[local_pos]\n                        if not isinstance(row, (list, tuple)):\n                            row = [row]\n                        row = list(row)\n                        if len(row) < max_resp_len:\n                            row += [0.0] * (max_resp_len - len(row))\n                        all_rm_scores[global_idx] = row\n                except Exception:\n                    # Fallback: zeros\n                    for global_idx in batch_indices:\n                        all_rm_scores[global_idx] = [0.0] * max_resp_len\n        else:\n            # batch_rm is a tensor-like; convert to list of rows\n            try:\n                rm_list = list(batch_rm)\n                if len(rm_list) != len(batch_indices):\n                    # attempt to pad/truncate\n                    rm_list = rm_list[: len(batch_indices)]\n                for local_pos, global_idx in enumerate(batch_indices):\n                    row = rm_list[local_pos]\r\n                    if not isinstance(row, (list, tuple)):\n                        row = [row]\n                    row = list(row)\n                    if len(row) < max_resp_len:\n                        row += [0.0] * (max_resp_len - len(row))\n                    all_rm_scores[global_idx] = row\n            except Exception:\n                for global_idx in batch_indices:\n                    all_rm_scores[global_idx] = [0.0] * max_resp_len\n\n    # Final sanity: ensure every row is present; fill missing with zeros\n    for i in range(batch_size):\n        if all_rm_scores[i] is None:\n            all_rm_scores[i] = [0.0] * max_resp_len\n\n    # Attach rm_scores to DataProto in a robust way\n    try:\n        if isinstance(getattr(data, 'batch', None), dict):\n            data.batch['rm_scores'] = all_rm_scores\n        else:\n            setattr(data, 'rm_scores', all_rm_scores)\n    except Exception:\n        # As a last resort, try a direct attribute\n        try:\n            data.rm_scores = all_rm_scores\n        except Exception:\n            pass\n\n    return data"], "usage": {"prompt_tokens": 416, "completion_tokens": 7453, "total_tokens": 7869, "per_completion": [{"prompt_tokens": 416, "completion_tokens": 7453, "total_tokens": 7869}]}}
{"function_name": "compute_return", "overview": "Computes token-level return sequences based on `method`", "function_signature": "def compute_return(token_level_rewards, eos_mask, method='sum', gamma=1.0):", "input_parameters": "- `token_level_rewards`: `[batch_size, response_length]`\n- `eos_mask`: `[batch_size, response_length]`\n- `method`: `'sum'|'min'`\n- `gamma`: Discount factor (applicable in `'sum'` mode)", "detailed_description": "Computes token-level return sequences based on method, using masking to ensure only valid positions are processed.\n\"method\" includes \"sum\" and \"min\".", "output": "- `returns`: `[batch_size, response_length]`", "func_implementation": "", "test_code_path": "code/test_code/test_compute_return.py", "implementation_location": "code/verl/trainer/ppo/core_algos.py:line 70-88", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_return`.\n\n**Function Overview:**\nComputes token-level return sequences based on `method`\n\n**Function Signature:**\n```python\ndef compute_return(token_level_rewards, eos_mask, method='sum', gamma=1.0):\n```\n\n**Input Parameters:**\n- `token_level_rewards`: `[batch_size, response_length]`\n- `eos_mask`: `[batch_size, response_length]`\n- `method`: `'sum'|'min'`\n- `gamma`: Discount factor (applicable in `'sum'` mode)\n\n**Detailed Description:**\nComputes token-level return sequences based on method, using masking to ensure only valid positions are processed.\n\"method\" includes \"sum\" and \"min\".\n\n**Expected Output:**\n- `returns`: `[batch_size, response_length]`\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def compute_return(token_level_rewards, eos_mask, method='sum', gamma=1.0):\n    \"\"\"\n    Compute token-level return sequences based on the specified method.\n\n    Args:\n        token_level_rewards: Tensor of shape [batch_size, response_length]\n        eos_mask: Tensor of shape [batch_size, response_length], values >0 indicate valid tokens, 0 indicates padding/ignored\n        method: 'sum' or 'min'\n        gamma: Discount factor used only in 'sum' mode\n\n    Returns:\n        Tensor of shape [batch_size, response_length] containing the computed returns\n    \"\"\"\n    import torch\n\n    rewards = token_level_rewards.float()\n    mask = eos_mask.float()\n    B, L = rewards.shape\n    returns = torch.zeros_like(rewards)\n\n    if method == 'sum':\n        # Compute discounted sum of future rewards for each position\n        running = torch.zeros(B, dtype=rewards.dtype, device=rewards.device)\n        for t in range(L - 1, -1, -1):\n            r = rewards[:, t]\n            m = mask[:, t]\n            running = r + gamma * running\n            returns[:, t] = running * m\n        return returns\n\n    elif method == 'min':\n        # For each sequence in the batch, compute for each position t:\n        # min_{k in [t, last]} (sum_{i=t..k} gamma^{i-t} * r_i)\n        # where last is the last valid index according to the mask\n        for b in range(B):\n            r_b = rewards[b]\n            m_b = mask[b]\n\n            # Determine last valid index\n            valid_idx = (m_b > 0).nonzero(as_tuple=False)\n            if valid_idx.numel() == 0:\n                # No valid tokens for this sample\n                returns[b] = torch.zeros(L, dtype=rewards.dtype, device=rewards.device)\n                continue\n            last = int(valid_idx.max().item())\n\n            for t in range(L):\n                if t > last:\n                    returns[b, t] = 0.0\n                    continue\n\n                S = 0.0      # accumulated discounted sum from t to current k\n                disc = 1.0   # gamma^{k-t}\n                min_ret = float('inf')\n                k = t\n                while k <= last:\n                    if m_b[k] > 0:\n                        S += disc * float(r_b[k].item())\n                        disc *= float(gamma)\n                        if S < min_ret:\n                            min_ret = S\n                    else:\n                        break\n                    k += 1\n\n                if min_ret == float('inf'):\n                    min_ret = 0.0\n                returns[b, t] = min_ret\n        return returns\n\n    else:\n        raise ValueError(\"Unsupported method: choose 'sum' or 'min'\")"], "usage": {"prompt_tokens": 354, "completion_tokens": 4455, "total_tokens": 4809, "per_completion": [{"prompt_tokens": 354, "completion_tokens": 4455, "total_tokens": 4809}]}}
