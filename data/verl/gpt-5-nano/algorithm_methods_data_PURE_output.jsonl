{"function_name": "ProcessRewardModelWorker._forward_micro_batch", "overview": "Computes token-level scores from the Process Reward Model (PRM) and performs approximate weighting for min-form credit assignment based on configuration.", "function_signature": "def _forward_micro_batch(self, micro_batch):", "input_parameters": "- `micro_batch`: Micro-batch data containing `input_ids`, `attention_mask`, `position_ids`, `responses`, `reward_mask`, etc.\n- `response_length`: Response sequence length (inferred from batch tensors)\n- `temperature`: Temperature for approximate min-form (when `reward_model.credit_assignment` is a float)", "detailed_description": "Performs forward pass on the Process Reward Model (PRM), computing token-level scores via softmax to obtain the positive-negative class probability difference. Uses masking to retain only valid scores from the response portion.\nIf depadding or sequence parallelism optimization is enabled, padding must be removed before computation and restored afterward.\nCredit assignment weighting executes different strategies based on configuration mode: min-form, strict min-form, or gamma-decay. Lower-scoring tokens should receive higher weights.", "output": "- `rm_score`: Token-level process reward scores (shape `[batch_size, response_length]`)", "func_implementation": "", "test_code_path": "code/test_code/test_forward_micro_batch.py", "implementation_location": "code/verl/workers/fsdp_workers.py:line 1462-1539", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `ProcessRewardModelWorker._forward_micro_batch`.\n\n**Function Overview:**\nComputes token-level scores from the Process Reward Model (PRM) and performs approximate weighting for min-form credit assignment based on configuration.\n\n**Function Signature:**\n```python\ndef _forward_micro_batch(self, micro_batch):\n```\n\n**Input Parameters:**\n- `micro_batch`: Micro-batch data containing `input_ids`, `attention_mask`, `position_ids`, `responses`, `reward_mask`, etc.\n- `response_length`: Response sequence length (inferred from batch tensors)\n- `temperature`: Temperature for approximate min-form (when `reward_model.credit_assignment` is a float)\n\n**Detailed Description:**\nPerforms forward pass on the Process Reward Model (PRM), computing token-level scores via softmax to obtain the positive-negative class probability difference. Uses masking to retain only valid scores from the response portion.\nIf depadding or sequence parallelism optimization is enabled, padding must be removed before computation and restored afterward.\nCredit assignment weighting executes different strategies based on configuration mode: min-form, strict min-form, or gamma-decay. Lower-scoring tokens should receive higher weights.\n\n**Expected Output:**\n- `rm_score`: Token-level process reward scores (shape `[batch_size, response_length]`)\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def _forward_micro_batch(self, micro_batch):\n    import torch\n\n    # Extract basic batch information\n    input_ids = micro_batch.get(\"input_ids\", None)\n    responses = micro_batch.get(\"responses\", None)\n    reward_mask = micro_batch.get(\"reward_mask\", None)\n    attention_mask = micro_batch.get(\"attention_mask\", None)\n\n    batch_size = 0\n    resp_len = 0\n    device = None\n    if input_ids is not None:\n        batch_size = input_ids.size(0)\n        device = input_ids.device\n    elif responses is not None:\n        batch_size = responses.size(0)\n        device = responses.device\n    else:\n        # No data to process\n        return torch.zeros((0, 0), device=\"cpu\")\n\n    if responses is not None:\n        resp_len = responses.size(1)\n\n    # Initialize rm_score with zeros; shape [batch_size, response_length]\n    rm_score = torch.zeros((batch_size, resp_len), device=device, dtype=torch.float32)\n\n    # PRM (Process Reward Model) forward\n    prm = getattr(self, \"prm\", None)\n    if prm is None:\n        return rm_score\n\n    logits = None\n    # Try a few common invocation patterns for the PRM\n    try:\n        if attention_mask is not None:\n            logits = prm(responses, attention_mask=attention_mask)\n        else:\n            logits = prm(responses)\n    except TypeError:\n        try:\n            # Some implementations expect an explicit reward mask or extra arg\n            logits = prm(responses, reward_mask)\n        except Exception:\n            logits = None\n    except Exception:\n        logits = None\n\n    if logits is None:\n        # If we still can't run PRM, return zeros\n        return rm_score\n\n    # Compute token-level scores as difference between positive and negative class probabilities\n    rm_score_full = None\n    try:\n        if logits.dim() == 3:\n            # Expect shape [B, L, 2] or [B, 2, L]\n            if logits.size(-1) == 2:\n                probs = torch.softmax(logits, dim=-1)  # [B, L, 2]\n                rm_score_full = probs[..., 1] - probs[..., 0]\n            elif logits.size(1) == 2:\n                probs = torch.softmax(logits, dim=1)  # [B, 2, L]\n                rm_score_full = probs[:, 1, :] - probs[:, 0, :]\n            else:\n                probs = torch.softmax(logits, dim=-1)\n                rm_score_full = probs[..., 1] - probs[..., 0]\n        elif logits.dim() == 2 and logits.size(-1) == 2:\n            probs = torch.softmax(logits, dim=-1)\n            rm_score_full = probs[:, 1] - probs[:, 0]\n        else:\n            # Unsupported logits shape\n            return rm_score\n    except Exception:\n        return rm_score\n\n    # Ensure shape [B, L] and align with response length\n    if rm_score_full.dim() == 1:\n        rm_score_full = rm_score_full.unsqueeze(1)\n    if rm_score_full.size(0) != batch_size:\n        rm_score_full = rm_score_full[:batch_size, :]\n    if rm_score_full.size(1) != resp_len:\n        rm_score_full = rm_score_full[:, :resp_len]\n\n    # Apply reward_mask to keep only valid scores from the response portion\n    if reward_mask is not None:\n        mask = reward_mask.to(rm_score_full.dtype)\n        if mask.dim() == 2:\n            rm_score_full = rm_score_full * mask\n        else:\n            rm_score_full = rm_score_full * mask[:, :rm_score_full.shape[1]]\n\n    rm_score = rm_score_full\n\n    # Credit assignment weighting\n    credit_assignment = micro_batch.get(\"credit_assignment\", None)\n    temperature = micro_batch.get(\"temperature\", None)\n\n    if isinstance(credit_assignment, (float, int)):\n        # Temperature-based approximate min-form\n        temp = float(credit_assignment)\n        min_v = rm_score.min(dim=1, keepdim=True)[0]\n        max_v = rm_score.max(dim=1, keepdim=True)[0]\n        denom = (max_v - min_v).clamp(min=1e-8)\n        norm = (rm_score - min_v) / denom  # [B, L] in [0, 1]\n        inv = 1.0 - norm\n        weights = torch.softmax(inv / (temp if temp > 0 else 1.0), dim=1)\n        rm_score = rm_score * weights\n    elif credit_assignment is not None:\n        mode = str(credit_assignment)\n        if mode == \"min-form\":\n            min_v = rm_score.min(dim=1, keepdim=True)[0]\n            max_v = rm_score.max(dim=1, keepdim=True)[0]\n            denom = (max_v - min_v).clamp(min=1e-8)\n            norm = (rm_score - min_v) / denom\n            weights = 1.0 - norm  # lower scores get higher weights\n            rm_score = rm_score * weights\n        elif mode == \"strict-min-form\":\n            min_idx = rm_score.argmin(dim=1, keepdim=True)\n            weights = torch.zeros_like(rm_score)\n            weights.scatter_(1, min_idx, 1.0)\n            rm_score = rm_score * weights\n        elif mode == \"gamma-decay\":\n            gamma = float(micro_batch.get(\"gamma\", 1.0))\n            B, L = rm_score.shape\n            pos = torch.arange(L, device=device).unsqueeze(0).expand(B, -1).float()\n            decay = torch.exp(-gamma * (L - 1 - pos))\n            min_v = rm_score.min(dim=1, keepdim=True)[0]\n            max_v = rm_score.max(dim=1, keepdim=True)[0]\n            denom = (max_v - min_v).clamp(min=1e-8)\n            norm = (rm_score - min_v) / denom\n            weights = decay * norm\n            rm_score = rm_score * weights\n        else:\n            # Unknown mode: no additional weighting\n            pass\n\n    return rm_score"], "usage": {"prompt_tokens": 450, "completion_tokens": 8626, "total_tokens": 9076, "per_completion": [{"prompt_tokens": 450, "completion_tokens": 8626, "total_tokens": 9076}]}}
{"function_name": "ProcessRewardModelWorker.compute_rm_score", "overview": "Batch-constructs PRM inputs (splitting by steps and appending newlines after each step), performs inference with (optional) variable-length sharding/backfilling, and aggregates to obtain batch-level token-level `rm_scores`.", "function_signature": "def compute_rm_score(self, data:DataProto):", "input_parameters": "- `data`: `DataProto` containing `responses`, `reward_mask`, etc.;", "detailed_description": "Splits response sequences by steps to construct input format for the process reward model.\nIn dynamic batch mode, reorganizes data into multiple mini-batches based on total token count limit and records original order; in fixed batch mode, evenly splits by sample count.\nCalls forward function for each mini-batch to compute token-level scores, concatenating results from all batches. In dynamic batch mode, restores results to original order based on recorded indices.", "output": "- `DataProto`: `batch['rm_scores']` with shape `[batch_size, response_length]`", "func_implementation": "", "test_code_path": "code/test_code/test_compute_rm_score.py", "implementation_location": "code/verl/workers/fsdp_workers.py:line 1541-1594", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `ProcessRewardModelWorker.compute_rm_score`.\n\n**Function Overview:**\nBatch-constructs PRM inputs (splitting by steps and appending newlines after each step), performs inference with (optional) variable-length sharding/backfilling, and aggregates to obtain batch-level token-level `rm_scores`.\n\n**Function Signature:**\n```python\ndef compute_rm_score(self, data:DataProto):\n```\n\n**Input Parameters:**\n- `data`: `DataProto` containing `responses`, `reward_mask`, etc.;\n\n**Detailed Description:**\nSplits response sequences by steps to construct input format for the process reward model.\nIn dynamic batch mode, reorganizes data into multiple mini-batches based on total token count limit and records original order; in fixed batch mode, evenly splits by sample count.\nCalls forward function for each mini-batch to compute token-level scores, concatenating results from all batches. In dynamic batch mode, restores results to original order based on recorded indices.\n\n**Expected Output:**\n- `DataProto`: `batch['rm_scores']` with shape `[batch_size, response_length]`\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def compute_rm_score(self, data: DataProto):\n    \"\"\"\n    Build PRM inputs by splitting responses into steps, batch them (dynamic or fixed),\n    run the reward-model forward passes, and collect per-token rm_scores aligned to\n    each response length. The resulting rm_scores are stored back into data.batch['rm_scores'].\n\n    Assumptions:\n    - data.responses: list of response strings (length = batch_size)\n    - data.reward_mask (optional): mask to apply on rm_scores with same shape as output\n    - A forward method is available on self to run the PRM, named either forward(...) or rm_forward(...)\n      and should accept a list of input_texts and return a tensor of shape [batch_size, seq_len]\n      containing token-level scores.\n    - We approximate token lengths using a simple whitespace-based token count for batching.\n    - The rm_scores output shape is [batch_size, max_response_length] where\n      max_response_length = max token-length among responses in the batch.\n    \"\"\"\n    # Guard: ensure data contains responses\n    responses = getattr(data, 'responses', None)\n    if responses is None:\n        # Nothing to compute\n        if not hasattr(data, 'batch') or data.batch is None:\n            data.batch = {}\n        data.batch['rm_scores'] = []\n        return\n\n    # Ensure batch container exists\n    if not hasattr(data, 'batch') or data.batch is None:\n        data.batch = {}\n\n    # Helper: simple token length estimator (space-based)\n    def estimate_tokens(text: str) -> int:\n        if text is None:\n            return 0\n        # strip and split on whitespace to approximate tokens\n        return len(text.split())\n\n    # Prepare per-sample PRM input text by splitting response into steps (lines),\n    # then appending a newline after each step.\n    input_texts = []\n    resp_token_lengths = []\n    max_resp_len = 0\n\n    for resp in responses:\n        if resp is None:\n            resp = \"\"\n        # Split by steps: lines separated by newline are treated as steps\n        steps = [line for line in str(resp).split(\"\\n\") if line.strip() != \"\"]\n        # Build input by appending newline after each step\n        prm_input = \"\"\n        for step in steps:\n            prm_input += step.rstrip() + \"\\n\"\n        # If there are no steps, at least a newline to keep non-empty input\n        if prm_input == \"\":\n            prm_input = \"\\n\"\n        input_texts.append(prm_input)\n\n        # Estimate token length of the response (to align rm_scores later)\n        tlen = estimate_tokens(str(resp))\n        resp_token_lengths.append(tlen)\n        if tlen > max_resp_len:\n            max_resp_len = tlen\n\n    batch_size = len(responses)\n    if batch_size == 0:\n        data.batch['rm_scores'] = []\n        return\n\n    # Initialize output container: [batch_size, max_resp_len]\n    # Use torch if available for tensor operations; fall back to Python lists.\n    use_torch = False\n    try:\n        import torch\n        use_torch = True\n        rm_scores = torch.zeros((batch_size, max_resp_len), dtype=torch.float32)\n    except Exception:\n        rm_scores = [[0.0 for _ in range(max_resp_len)] for _ in range(batch_size)]\n\n    # Determine batching strategy: dynamic vs fixed\n    dynamic_batch = False\n    # Try to detect common attribute names\n    if hasattr(self, 'rm_dynamic_batch') and isinstance(getattr(self, 'rm_dynamic_batch'), bool):\n        dynamic_batch = getattr(self, 'rm_dynamic_batch')\n    elif hasattr(self, 'config') and hasattr(self.config, 'rm_dynamic_batch'):\n        dynamic_batch = bool(self.config.rm_dynamic_batch)\n\n    # Token limit for dynamic batching\n    batch_tokens_limit = None\n    if hasattr(self, 'rm_batch_token_limit'):\n        batch_tokens_limit = int(self.rm_batch_token_limit)\n    elif hasattr(self, 'config') and hasattr(self.config, 'rm_batch_token_limit'):\n        batch_tokens_limit = int(self.config.rm_batch_token_limit)\n    # Fallback default\n    if batch_tokens_limit is None or batch_tokens_limit <= 0:\n        batch_tokens_limit = 2048  # reasonable default\n\n    # Fixed batch size for non-dynamic mode\n    fixed_batch_size = None\n    if hasattr(self, 'rm_fixed_batch_size') and isinstance(self.rm_fixed_batch_size, int):\n        fixed_batch_size = self.rm_fixed_batch_size\n    elif hasattr(self, 'config') and hasattr(self.config, 'rm_fixed_batch_size'):\n        fixed_batch_size = int(self.config.rm_fixed_batch_size)\n    if fixed_batch_size is None or fixed_batch_size <= 0:\n        fixed_batch_size = max(1, batch_size // 4)  # sensible default\n\n    # Helper: forward through PRM\n    def _forward_batch(batch_inputs):\n        # Prefer explicit rm_forward, then fallback to forward\n        if hasattr(self, 'rm_forward'):\n            return self.rm_forward(batch_inputs)\n        elif hasattr(self, 'forward'):\n            return self.forward(batch_inputs)\n        else:\n            raise RuntimeError(\"No forward method found on ProcessRewardModelWorker (rm_forward/forward).\")\n\n    # Build batches and run forward per batch\n    if dynamic_batch:\n        # Dynamic batching: accumulate samples until token limit exceeded\n        current_inputs = []\n        current_indices = []\n        current_token_sum = 0\n\n        def flush_current_batch():\n            nonlocal current_inputs, current_indices, current_token_sum\n            if not current_indices:\n                return\n            # Run forward on current batch\n            try:\n                scores_batch = _forward_batch(current_inputs)\n            except Exception as e:\n                raise RuntimeError(f\"Error during PRM forward in dynamic batch: {e}\")\n\n            # scores_batch shape: [batch_size_in_batch, seq_len]\n            # Fill rm_scores for each corresponding sample\n            for j, idx in enumerate(current_indices):\n                # length of the response in tokens (estimated)\n                resp_len = resp_token_lengths[idx]\n                if resp_len <= 0:\n                    continue\n                # Retrieve j-th scores\n                scores_j = scores_batch[j]\n                # Ensure we can index into scores_j\n                if getattr(scores_batch, \"shape\", None) is not None:\n                    # torch tensor\n                    scores_j_len = int(scores_j.shape[0])\n                    take_n = min(scores_j_len, resp_len)\n                    if take_n > 0:\n                        if use_torch:\n                            rm_slice = scores_j[:take_n].detach().cpu()\n                            rm_scores[idx, :take_n] = rm_slice\n                        else:\n                            rm_scores[idx][:take_n] = scores_j[:take_n]\n                else:\n                    # Assume list-like\n                    scores_j_list = list(scores_j)\n                    take_n = min(len(scores_j_list), resp_len)\n                    if take_n > 0:\n                        rm_scores[idx, :take_n] = scores_j_list[:take_n]\n            # reset\n            current_inputs = []\n            current_indices = []\n            current_token_sum = 0\n\n        for i in range(batch_size):\n            tlen = resp_token_lengths[i]\n            if tlen <= 0:\n                # No tokens in response; leave zeros\n                continue\n            # If adding this sample would exceed the token limit, flush first\n            if current_token_sum + tlen > batch_tokens_limit and current_indices:\n                flush_current_batch()\n            # Add to current batch\n            current_indices.append(i)\n            current_inputs.append(input_texts[i])\n            current_token_sum += tlen\n\n        # Flush remaining\n        if current_indices:\n            flush_current_batch()\n    else:\n        # Fixed batch mode: evenly split by sample count\n        total = batch_size\n        batch_size_num = max(1, fixed_batch_size)\n        num_batches = (total + batch_size_num - 1) // batch_size_num\n\n        for b in range(num_batches):\n            start = b * batch_size_num\n            end = min(total, (b + 1) * batch_size_num)\n            batch_inputs = input_texts[start:end]\n            if not batch_inputs:\n                continue\n            try:\n                scores_batch = _forward_batch(batch_inputs)\n            except Exception as e:\n                raise RuntimeError(f\"Error during PRM forward in fixed batch: {e}\")\n\n            # Fill rm_scores for this batch\n            for j, idx in enumerate(range(start, end)):\n                resp_len = resp_token_lengths[idx]\n                if resp_len <= 0:\n                    continue\n                scores_j = scores_batch[j]\n                if getattr(scores_batch, \"shape\", None) is not None:\n                    scores_j_len = int(scores_j.shape[0])\n                    take_n = min(scores_j_len, resp_len)\n                    if take_n > 0:\n                        if use_torch:\n                            rm_slice = scores_j[:take_n].detach().cpu()\n                            rm_scores[idx, :take_n] = rm_slice\n                        else:\n                            rm_scores[idx, :take_n] = scores_j[:take_n]\n                else:\n                    scores_j_list = list(scores_j)\n                    take_n = min(len(scores_j_list), resp_len)\n                    if take_n > 0:\n                        rm_scores[idx, :take_n] = scores_j_list[:take_n]\n\n    # Apply reward_mask if provided\n    reward_mask = getattr(data, 'reward_mask', None)\n    if reward_mask is not None:\n        try:\n            if use_torch and isinstance(rm_scores, torch.Tensor):\n                mask_tensor = reward_mask\n                if isinstance(mask_tensor, torch.Tensor) and mask_tensor.shape == rm_scores.shape:\n                    rm_scores = rm_scores * mask_tensor\n                else:\n                    # Broadcast if possible\n                    rm_scores = rm_scores * torch.as_tensor(mask_tensor, dtype=rm_scores.dtype)\n            else:\n                # numpy-like masking\n                import numpy as np\n                rm_scores_np = rm_scores if isinstance(rm_scores, (np.ndarray, list)) else rm_scores.cpu().numpy()\n                mask_np = reward_mask if isinstance(reward_mask, (np.ndarray, list)) else reward_mask.cpu().numpy()\n                rm_scores_np = (rm_scores_np * mask_np).tolist() if not isinstance(rm_scores_np, list) else rm_scores_np\n                rm_scores = rm_scores_np\n        except Exception:\n            # If masking fails for any reason, skip masking to avoid breaking the flow\n            pass\n\n    # Move to desired format for DataProto: store as numpy array if possible\n    try:\n        import numpy as np\n        if use_torch and isinstance(rm_scores, torch.Tensor):\n            rm_scores_np = rm_scores.detach().cpu().numpy()\n        else:\n            rm_scores_np = np.array(rm_scores)\n    except Exception:\n        rm_scores_np = rm_scores  # fallback\n\n    data.batch['rm_scores'] = rm_scores_np"], "usage": {"prompt_tokens": 416, "completion_tokens": 7767, "total_tokens": 8183, "per_completion": [{"prompt_tokens": 416, "completion_tokens": 7767, "total_tokens": 8183}]}}
{"function_name": "compute_return", "overview": "Computes token-level return sequences based on `method`", "function_signature": "def compute_return(token_level_rewards, eos_mask, method='sum', gamma=1.0):", "input_parameters": "- `token_level_rewards`: `[batch_size, response_length]`\n- `eos_mask`: `[batch_size, response_length]`\n- `method`: `'sum'|'min'`\n- `gamma`: Discount factor (applicable in `'sum'` mode)", "detailed_description": "Computes token-level return sequences based on method, using masking to ensure only valid positions are processed.\n\"method\" includes \"sum\" and \"min\".", "output": "- `returns`: `[batch_size, response_length]`", "func_implementation": "", "test_code_path": "code/test_code/test_compute_return.py", "implementation_location": "code/verl/trainer/ppo/core_algos.py:line 70-88", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_return`.\n\n**Function Overview:**\nComputes token-level return sequences based on `method`\n\n**Function Signature:**\n```python\ndef compute_return(token_level_rewards, eos_mask, method='sum', gamma=1.0):\n```\n\n**Input Parameters:**\n- `token_level_rewards`: `[batch_size, response_length]`\n- `eos_mask`: `[batch_size, response_length]`\n- `method`: `'sum'|'min'`\n- `gamma`: Discount factor (applicable in `'sum'` mode)\n\n**Detailed Description:**\nComputes token-level return sequences based on method, using masking to ensure only valid positions are processed.\n\"method\" includes \"sum\" and \"min\".\n\n**Expected Output:**\n- `returns`: `[batch_size, response_length]`\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def compute_return(token_level_rewards, eos_mask, method='sum', gamma=1.0):\n    \"\"\"\n    Compute token-level return sequences based on the specified method.\n\n    - token_level_rewards: tensor of shape [batch_size, response_length]\n    - eos_mask: tensor of shape [batch_size, response_length], should be 1 for valid tokens, 0 otherwise\n    - method: 'sum' or 'min'\n    - gamma: discount factor (applies only for 'sum')\n\n    Returns:\n    - returns: tensor of shape [batch_size, response_length]\n    \"\"\"\n    import torch\n\n    # Ensure inputs are tensors\n    if not isinstance(token_level_rewards, torch.Tensor):\n        token_level_rewards = torch.tensor(token_level_rewards)\n    if not isinstance(eos_mask, torch.Tensor):\n        eos_mask = torch.tensor(eos_mask)\n\n    # Cast mask to the same dtype for proper arithmetic and apply mask\n    rewards = token_level_rewards * eos_mask.to(token_level_rewards.dtype)\n\n    batch_size, seq_len = rewards.shape\n    gamma_tensor = torch.tensor(gamma, dtype=rewards.dtype, device=rewards.device)\n\n    if method == 'sum':\n        # Compute discounted sum from the end: returns[:, t] = sum_{k=t}^{L-1} gamma^{k-t} * rewards[:, k]\n        returns = torch.zeros_like(rewards)\n        acc = torch.zeros(batch_size, dtype=rewards.dtype, device=rewards.device)\n        for t in range(seq_len - 1, -1, -1):\n            acc = rewards[:, t] + gamma_tensor * acc\n            returns[:, t] = acc\n        return returns\n\n    elif method == 'min':\n        # Compute per-time-step minimum over future discounted rewards:\n        # A_t = min_{k>=t} rewards[:, k] * gamma^{k-t}\n        returns = torch.zeros_like(rewards)\n        if seq_len == 0:\n            return returns\n        # Initialize with the last position\n        A_next = rewards[:, seq_len - 1]\n        returns[:, seq_len - 1] = A_next\n        for t in range(seq_len - 2, -1, -1):\n            A_curr = torch.minimum(rewards[:, t], gamma_tensor * A_next)\n            returns[:, t] = A_curr\n            A_next = A_curr\n        return returns\n\n    else:\n        raise ValueError(\"Unsupported method: choose 'sum' or 'min'\")"], "usage": {"prompt_tokens": 354, "completion_tokens": 4411, "total_tokens": 4765, "per_completion": [{"prompt_tokens": 354, "completion_tokens": 4411, "total_tokens": 4765}]}}
