{"function_name": "RewardModelForCritic.build_revision_messages", "overview": "Builds prompt messages for generating revised code. Combines original problem, initial solution, and critique into chat template format.", "function_signature": "def build_revision_messages(self, init_messages, critique):", "input_parameters": "- `init_messages`: Initial dialogue history list containing problem (user role) and initial solution (assistant role)\n- `critique`: Model-generated critique text string", "detailed_description": "Function receives initial dialogue history and critique text, validates that initial dialogue history is list type and last message role is assistant, calls related functions for content cleaning (keeping only code portion) and updating, then converts processed plain text critique with no leading/trailing whitespace and revision instruction \"Please finalize your answer accordingly using the same format.\" into new message with user role appended to end of dialogue.", "output": "- Complete dialogue message list (List[Dict]) containing original problem, cleaned initial code, and revision instruction", "func_implementation": "", "test_code_path": "code/test_code/test_build_revision_messages.py", "implementation_location": "code/ctrl/rl/critic_rm.py:line 136-150", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `RewardModelForCritic.build_revision_messages`.\n\n**Function Overview:**\nBuilds prompt messages for generating revised code. Combines original problem, initial solution, and critique into chat template format.\n\n**Function Signature:**\n```python\ndef build_revision_messages(self, init_messages, critique):\n```\n\n**Input Parameters:**\n- `init_messages`: Initial dialogue history list containing problem (user role) and initial solution (assistant role)\n- `critique`: Model-generated critique text string\n\n**Detailed Description:**\nFunction receives initial dialogue history and critique text, validates that initial dialogue history is list type and last message role is assistant, calls related functions for content cleaning (keeping only code portion) and updating, then converts processed plain text critique with no leading/trailing whitespace and revision instruction \"Please finalize your answer accordingly using the same format.\" into new message with user role appended to end of dialogue.\n\n**Expected Output:**\n- Complete dialogue message list (List[Dict]) containing original problem, cleaned initial code, and revision instruction\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def build_revision_messages(self, init_messages, critique):\n    \"\"\"\n    Build a revision prompt sequence for code revision.\n\n    - Validates input: init_messages must be a list and the last message must be from the 'assistant'.\n    - Extracts code portion from the initial solution (last assistant message).\n    - Cleans/normalizes the extracted code.\n    - Creates a new assistant message with the cleaned code.\n    - Creates a new user message containing the critique and a revision instruction.\n\n    Returns:\n        List[Dict]: A complete dialogue list containing:\n            1) the original problem message,\n            2) the cleaned initial code as an assistant message,\n            3) a new user message with the critique and revision instruction.\n    \"\"\"\n    import re\n\n    # Validate input type\n    if not isinstance(init_messages, list):\n        raise TypeError(\"init_messages must be a list of message dicts\")\n\n    if len(init_messages) == 0:\n        raise ValueError(\"init_messages must contain at least one message (the problem).\")\n\n    # Ensure the last message is from the assistant (initial solution)\n    last_msg = init_messages[-1]\n    if not isinstance(last_msg, dict) or last_msg.get('role') != 'assistant':\n        raise ValueError(\"init_messages last message must be from the 'assistant' role (initial solution).\")\n\n    # Original problem message\n    problem_msg = init_messages[0]\n    problem_copy = {\n        'role': problem_msg.get('role', 'user'),\n        'content': problem_msg.get('content', '')\n    }\n\n    # Extract code portion from the initial solution\n    original_solution = last_msg.get('content', '')\n\n    def _extract_code_portion(text):\n        if not text:\n            return ''\n        # 1) Fenced code blocks\n        blocks = re.findall(r'```(?:[a-zA-Z0-9_+-]*\\n)?(.*?)```', text, flags=re.S)\n        if blocks:\n            return \"\\n\".join(block.strip() for block in blocks if block.strip())\n\n        # 2) Indented code blocks (4 spaces)\n        lines = text.splitlines()\n        code_segments = []\n        current = []\n        for line in lines:\n            if line.startswith('    '):\n                current.append(line[4:])\n            else:\n                if current:\n                    code_segments.append(\"\\n\".join(current))\n                    current = []\n        if current:\n            code_segments.append(\"\\n\".join(current))\n\n        if code_segments:\n            return \"\\n\\n\".join(code_segments)\n\n        # Fallback: no code detected\n        return ''\n\n    code_text = _extract_code_portion(original_solution)\n\n    # Normalize/clean code content\n    def _normalize_code(code):\n        if not code:\n            return ''\n        lines = code.splitlines()\n        # Trim trailing spaces\n        lines = [ln.rstrip() for ln in lines]\n        # Remove leading/trailing empty lines\n        while lines and lines[0].strip() == '':\n            lines.pop(0)\n        while lines and lines[-1].strip() == '':\n            lines.pop()\n        return \"\\n\".join(lines)\n\n    cleaned_code = _normalize_code(code_text)\n\n    # Build the new assistant message with cleaned code\n    code_message = {'role': 'assistant', 'content': cleaned_code}\n\n    # Process critique text\n    critique_text = critique if isinstance(critique, str) else str(critique)\n    critique_clean = critique_text.strip()\n\n    revision_instruction = \"Please finalize your answer accordingly using the same format.\"\n\n    # Compose user message content: critique plus revision instruction\n    if critique_clean:\n        user_message_content = critique_clean + \"\\n\\n\" + revision_instruction\n    else:\n        user_message_content = revision_instruction\n\n    user_message = {'role': 'user', 'content': user_message_content}\n\n    return [problem_copy, code_message, user_message]"], "usage": {"prompt_tokens": 398, "completion_tokens": 4483, "total_tokens": 4881, "per_completion": [{"prompt_tokens": 398, "completion_tokens": 4483, "total_tokens": 4881}]}}
{"function_name": "RewardModelForCritic.get_reward_all", "overview": "Concurrently executes sandbox tests for multiple critique-revision pairs, computing verifiable rewards. Improves execution efficiency through asynchronous I/O mechanism, supporting large-scale parallel evaluation.", "function_signature": "async def get_reward_all(\nself, critiques: List[str], revisions: List[str], samples: List[Any]\n) -> List[float]:", "input_parameters": "- `critiques`: Critique text list, each element corresponding to a generated critique\n- `revisions`: Revised code list, one-to-one correspondence with critiques\n- `samples`: Test sample list containing test cases and problem metadata", "detailed_description": "Function calls underlying reward computation method for each triplet (sample, critique, revision), evaluating improvement quality of revised version relative to original sample, returning numerical reward scores. All computation tasks execute asynchronously and concurrently, maximizing processing throughput while ensuring resource control, with default maximum concurrency set to MAX_REQUESTS (256), requiring progress bar (\"Generating rewards\") for real-time completion feedback.", "output": "- Float list, each element representing reward value for corresponding critique-revision pair, range [0.0, 1.0]", "func_implementation": "", "test_code_path": "code/test_code/test_get_reward_all.py", "implementation_location": "code/ctrl/rl/critic_rm.py:line 198-209", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `RewardModelForCritic.get_reward_all`.\n\n**Function Overview:**\nConcurrently executes sandbox tests for multiple critique-revision pairs, computing verifiable rewards. Improves execution efficiency through asynchronous I/O mechanism, supporting large-scale parallel evaluation.\n\n**Function Signature:**\n```python\nasync def get_reward_all(\nself, critiques: List[str], revisions: List[str], samples: List[Any]\n) -> List[float]:\n```\n\n**Input Parameters:**\n- `critiques`: Critique text list, each element corresponding to a generated critique\n- `revisions`: Revised code list, one-to-one correspondence with critiques\n- `samples`: Test sample list containing test cases and problem metadata\n\n**Detailed Description:**\nFunction calls underlying reward computation method for each triplet (sample, critique, revision), evaluating improvement quality of revised version relative to original sample, returning numerical reward scores. All computation tasks execute asynchronously and concurrently, maximizing processing throughput while ensuring resource control, with default maximum concurrency set to MAX_REQUESTS (256), requiring progress bar (\"Generating rewards\") for real-time completion feedback.\n\n**Expected Output:**\n- Float list, each element representing reward value for corresponding critique-revision pair, range [0.0, 1.0]\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["async def get_reward_all(\n    self, critiques: list[str], revisions: list[str], samples: list[object]\n) -> list[float]:\n    \"\"\"\n    Concurrently compute rewards for each (sample, critique, revision) triplet.\n\n    - Uses a concurrency limit (default 256) to control resource usage.\n    - Attempts to call a reward computation method on self in a best-effort manner.\n    - Provides a real-time progress indicator if available (refreshes with a \"Generating rewards\" label).\n\n    Returns:\n        A list of float rewards in [0.0, 1.0], aligned with input triplets.\n    \"\"\"\n    import asyncio\n    import inspect\n\n    # Normalize inputs and ensure consistent lengths\n    if critiques is None:\n        critiques = []\n    if revisions is None:\n        revisions = []\n    if samples is None:\n        samples = []\n\n    min_len = min(len(critiques), len(revisions), len(samples))\n    critiques = critiques[:min_len]\n    revisions = revisions[:min_len]\n    samples = samples[:min_len]\n\n    n = len(critiques)\n    if n == 0:\n        return []\n\n    # Maximum concurrency (default to 256 if not defined)\n    max_concurrency = getattr(self, \"MAX_REQUESTS\", 256) or 256\n    sem = asyncio.Semaphore(max_concurrency)\n\n    # Progress bar support (optional)\n    progress_bar = None\n    use_tqdm_async = False\n    try:\n        from tqdm.asyncio import tqdm as tqdm_async\n        progress_bar = tqdm_async(total=n, desc=\"Generating rewards\")\n        use_tqdm_async = True\n    except Exception:\n        progress_bar = None\n\n    results = [0.0] * n\n\n    # Determine a candidate reward method to call on self\n    reward_method = None\n    for name in (\"get_reward\", \"_get_reward\", \"compute_reward\", \"_compute_reward\", \"reward_for_pair\"):\n        m = getattr(self, name, None)\n        if m is not None:\n            reward_method = m\n            break\n\n    # Lightweight progress fallback if tqdm is unavailable\n    completed = [0]\n    progress_lock = asyncio.Lock()\n    progress_interval = max(1, n // 10)  # report every ~10%\n\n    async def compute_one(i: int) -> float:\n        s = samples[i]\n        c = critiques[i]\n        r = revisions[i]\n\n        reward = 0.0\n        if reward_method is None:\n            reward = 0.0\n        else:\n            try:\n                async with sem:\n                    if asyncio.iscoroutinefunction(reward_method):\n                        reward = await reward_method(s, c, r)\n                    else:\n                        maybe = reward_method(s, c, r)\n                        if inspect.isawaitable(maybe):\n                            reward = await maybe\n                        else:\n                            reward = maybe\n            except Exception:\n                reward = 0.0\n\n        # Normalize reward to float and clamp within [0.0, 1.0]\n        try:\n            reward = float(reward)\n        except Exception:\n            reward = 0.0\n\n        if reward < 0.0:\n            reward = 0.0\n        if reward > 1.0:\n            reward = 1.0\n\n        results[i] = reward\n\n        # Update progress\n        if progress_bar is not None:\n            try:\n                progress_bar.update(1)\n            except Exception:\n                pass\n        else:\n            async with progress_lock:\n                completed[0] += 1\n                if completed[0] % progress_interval == 0 or completed[0] == n:\n                    print(f\"Generating rewards: {completed[0]}/{n}\")\n\n        return reward\n\n    # Launch all tasks with controlled concurrency\n    tasks = [asyncio.create_task(compute_one(i)) for i in range(n)]\n    await asyncio.gather(*tasks)\n\n    # Close progress bar if used\n    if progress_bar is not None:\n        try:\n            progress_bar.close()\n        except Exception:\n            pass\n\n    return results"], "usage": {"prompt_tokens": 443, "completion_tokens": 5286, "total_tokens": 5729, "per_completion": [{"prompt_tokens": 443, "completion_tokens": 5286, "total_tokens": 5729}]}}
{"function_name": "sanitize", "overview": "Extracts pure code blocks from mixed text containing code and explanations, removes explanatory text, preserves markdown format code block structure.", "function_signature": "def sanitize(text):", "input_parameters": "- `text`: Mixed text string containing code blocks", "detailed_description": "Function uses regular expressions to identify and extract markdown format code blocks, matching code blocks wrapped in triple backticks through regex, recognizing cases with or without python identifier. Matching process is case-insensitive. After successful match, extracts content of first code block and reformats it to standard markdown format ````python\\n{code}\\n``` for return. If no code block found in text or code block is empty, directly returns original text.", "output": "- Formatted code block string (````python\\n{code}\\n```) or original text (when no code block)\nIn `build_revision_messages` function, used to clean model-generated initial solution, remove explanatory text, keep only code portion, ensuring conciseness of revision prompt.", "func_implementation": "", "test_code_path": "code/test_code/test_sanitize_desanitize.py", "implementation_location": "code/ctrl/rl/critic_rm.py:line 76-84", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `sanitize`.\n\n**Function Overview:**\nExtracts pure code blocks from mixed text containing code and explanations, removes explanatory text, preserves markdown format code block structure.\n\n**Function Signature:**\n```python\ndef sanitize(text):\n```\n\n**Input Parameters:**\n- `text`: Mixed text string containing code blocks\n\n**Detailed Description:**\nFunction uses regular expressions to identify and extract markdown format code blocks, matching code blocks wrapped in triple backticks through regex, recognizing cases with or without python identifier. Matching process is case-insensitive. After successful match, extracts content of first code block and reformats it to standard markdown format ````python\\n{code}\\n``` for return. If no code block found in text or code block is empty, directly returns original text.\n\n**Expected Output:**\n- Formatted code block string (````python\\n{code}\\n```) or original text (when no code block)\nIn `build_revision_messages` function, used to clean model-generated initial solution, remove explanatory text, keep only code portion, ensuring conciseness of revision prompt.\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def sanitize(text):\n    \"\"\"\n    Extracts the first markdown code block from mixed text and returns it\n    in a standardized Markdown code fence using Python as the language.\n    If no code block is found or the extracted code is empty, returns the\n    original text unchanged.\n    \"\"\"\n    import re\n\n    if not isinstance(text, str):\n        return text\n\n    # Pattern to find a markdown code block fenced with ```\n    # Opening fence may have an optional python qualifier (e.g., ```python, ```python3, etc.)\n    pattern = re.compile(r'```(?:\\s*)(?:python[^\\n]*)?\\n(.*?)\\n```', re.IGNORECASE | re.DOTALL)\n\n    match = pattern.search(text)\n    if not match:\n        # No code block found\n        return text\n\n    code_block = match.group(1)\n    if code_block is None:\n        return text\n\n    # Normalize content by trimming leading/trailing whitespace/newlines\n    code_content = code_block.strip()\n    if code_content == \"\":\n        # Empty code block, return original text\n        return text\n\n    # Return as a standard python code block\n    return \"```python\\n\" + code_content + \"\\n```\""], "usage": {"prompt_tokens": 409, "completion_tokens": 2926, "total_tokens": 3335, "per_completion": [{"prompt_tokens": 409, "completion_tokens": 2926, "total_tokens": 3335}]}}
{"function_name": "desanitize", "overview": "Removes markdown code block markers (triple backticks) from code text, extracts pure code string.", "function_signature": "def desanitize(text):", "input_parameters": "- `text`: Text with or without markdown code block markers", "detailed_description": "Function uses regular expression to match code blocks wrapped in triple backticks, recognizing cases with or without python identifier. After successful match, extracts content inside code block, returns after removing leading/trailing whitespace. If no code block format found in text, directly returns original text.", "output": "- Pure code string with markdown markers removed\nIn `get_sandbox_response` function, used to extract pure code from revision text, remove markdown format markers, prepare code for sandbox execution.", "func_implementation": "", "test_code_path": "code/test_code/test_sanitize_desanitize.py", "implementation_location": "code/ctrl/rl/critic_rm.py:line 66-73", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `desanitize`.\n\n**Function Overview:**\nRemoves markdown code block markers (triple backticks) from code text, extracts pure code string.\n\n**Function Signature:**\n```python\ndef desanitize(text):\n```\n\n**Input Parameters:**\n- `text`: Text with or without markdown code block markers\n\n**Detailed Description:**\nFunction uses regular expression to match code blocks wrapped in triple backticks, recognizing cases with or without python identifier. After successful match, extracts content inside code block, returns after removing leading/trailing whitespace. If no code block format found in text, directly returns original text.\n\n**Expected Output:**\n- Pure code string with markdown markers removed\nIn `get_sandbox_response` function, used to extract pure code from revision text, remove markdown format markers, prepare code for sandbox execution.\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def desanitize(text):\n    \"\"\"\n    Remove markdown fenced code block markers from the provided text and return the pure code.\n    - Looks for a fenced code block starting with triple backticks, optionally followed by a language\n      spec (e.g., ```python or ``` python), then captures the content between the opening and closing\n      backticks.\n    - Returns the extracted code with leading/trailing whitespace removed.\n    - If no fenced code block is found, returns the original text unchanged.\n    \"\"\"\n    if not isinstance(text, str):\n        # If input is not a string, there's nothing to desanitize; return as is.\n        return text\n\n    import re\n\n    # Pattern explanation:\n    # - ```(?:\\s*\\w+)?\\s*\\n  -> opening backticks, optionally followed by whitespace and a word (language),\n    #                          then a newline.\n    # - (.*?)                 -> lazily capture the code content\n    # - \\n```                  -> closing backticks on its own line\n    # re.S                   -> make . match newline characters as well\n    pattern = re.compile(r\"```(?:\\s*\\w+)?\\s*\\n(.*?)\\n```\", re.S)\n\n    match = pattern.search(text)\n    if match:\n        code_block = match.group(1)\n        return code_block.strip()\n\n    # No code block found; return original text unchanged\n    return text"], "usage": {"prompt_tokens": 356, "completion_tokens": 1919, "total_tokens": 2275, "per_completion": [{"prompt_tokens": 356, "completion_tokens": 1919, "total_tokens": 2275}]}}
{"function_name": "normalize_code", "overview": "Normalizes Python code through Abstract Syntax Tree (AST) level transformation, unifies variable naming, improves cache hit rate.", "function_signature": "def normalize_code(code_str):", "input_parameters": "- `code_str`: Python code string to be normalized", "detailed_description": "Function uniformly renames variables in Python code to standard form (v_0, v_1, ...) through AST parsing, enabling functionally identical but differently named code to be recognized as equivalent. If code has syntax errors, function returns original code without transformation.", "output": "- Normalized code string, or original code (when parsing fails)\nDuring training, model may generate multiple semantically identical but differently named code versions. For example, `x = 1; return x` and `y = 1; return y` are functionally completely equivalent. Through normalization, both are converted to `v_0 = 1; return v_0`, using same cache key, avoiding repeated sandbox execution, significantly improving training efficiency.", "func_implementation": "", "test_code_path": "code/test_code/test_normalize_code.py", "implementation_location": "code/ctrl/rl/critic_rm.py:line 35-63", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `normalize_code`.\n\n**Function Overview:**\nNormalizes Python code through Abstract Syntax Tree (AST) level transformation, unifies variable naming, improves cache hit rate.\n\n**Function Signature:**\n```python\ndef normalize_code(code_str):\n```\n\n**Input Parameters:**\n- `code_str`: Python code string to be normalized\n\n**Detailed Description:**\nFunction uniformly renames variables in Python code to standard form (v_0, v_1, ...) through AST parsing, enabling functionally identical but differently named code to be recognized as equivalent. If code has syntax errors, function returns original code without transformation.\n\n**Expected Output:**\n- Normalized code string, or original code (when parsing fails)\nDuring training, model may generate multiple semantically identical but differently named code versions. For example, `x = 1; return x` and `y = 1; return y` are functionally completely equivalent. Through normalization, both are converted to `v_0 = 1; return v_0`, using same cache key, avoiding repeated sandbox execution, significantly improving training efficiency.\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\ndef normalize_code(code_str):\n    \"\"\"\n    Normalize Python code by uniformly renaming local variables to a canonical form.\n    The transformation is done at the AST level:\n      - Each function (and lambda) gets a fresh scope.\n      - Function parameters (posonlyargs, args, kwonlyargs, vararg, kwarg) are renamed to v_*\n      - Whenever a new local variable is introduced (Name with Store context), it is assigned a\n        new canonical name (v_*), and all its usages within that scope are renamed accordingly.\n      - Names in Load/Del contexts are renamed if they map to a variable in the current or an\n        enclosing scope.\n    If the code cannot be parsed as valid Python, the original string is returned.\n\n    Note: This is a pragmatic normalization suitable for helping cache key stability in many\n    common code paths (especially simple functions). It may not cover every corner-case of Python's\n    scoping rules (e.g., complex interactions with globals/nonlocals or comprehensions). It aims\n    for robustness and a reasonable canonical form for typical code fragments.\n\n    Parameters:\n    - code_str: str, Python code to normalize\n\n    Returns:\n    - str: normalized code string, or the original code_str if parsing fails or normalization\n           cannot be safely applied\n    \"\"\"\n    import ast\n\n    try:\n        tree = ast.parse(code_str)\n    except SyntaxError:\n        # If code is not valid Python, return as-is per specification\n        return code_str\n\n    class NormalizeTransformer(ast.NodeTransformer):\n        def __init__(self):\n            super().__init__()\n            self.scopes = []      # stack of dicts: original_name -> new_name per scope\n            self.counter = 0      # global counter to produce deterministic v_0, v_1, ...\n\n        def push_scope(self):\n            self.scopes.append({})\n\n        def pop_scope(self):\n            if self.scopes:\n                self.scopes.pop()\n\n        def new_name(self):\n            name = f\"v_{self.counter}\"\n            self.counter += 1\n            return name\n\n        # Handle function definitions: create a new local scope and rename args\n        def visit_FunctionDef(self, node):\n            self.push_scope()\n\n            # Rename positional-only args (Python 3.8+)\n            for a in getattr(node.args, \"posonlyargs\", []):\n                new = self.new_name()\n                self.scopes[-1][a.arg] = new\n                a.arg = new\n\n            # Rename regular args\n            for a in node.args.args:\n                new = self.new_name()\n                self.scopes[-1][a.arg] = new\n                a.arg = new\n\n            # Rename keyword-only args\n            for a in getattr(node.args, \"kwonlyargs\", []):\n                new = self.new_name()\n                self.scopes[-1][a.arg] = new\n                a.arg = new\n\n            # Rename vararg and kwarg if present\n            if node.args.vararg:\n                new = self.new_name()\n                self.scopes[-1][node.args.vararg.arg] = new\n                node.args.vararg.arg = new\n\n            if node.args.kwarg:\n                new = self.new_name()\n                self.scopes[-1][node.args.kwarg.arg] = new\n                node.args.kwarg.arg = new\n\n            # Process body (will rename local variables inside this function)\n            self.generic_visit(node)\n            self.pop_scope()\n            return node\n\n        # Handle lambda: similar treatment as FunctionDef but for the lambda's scope\n        def visit_Lambda(self, node):\n            self.push_scope()\n\n            for a in getattr(node.args, \"posonlyargs\", []):\n                new = self.new_name()\n                self.scopes[-1][a.arg] = new\n                a.arg = new\n\n            for a in node.args.args:\n                new = self.new_name()\n                self.scopes[-1][a.arg] = new\n                a.arg = new\n\n            for a in getattr(node.args, \"kwonlyargs\", []):\n                new = self.new_name()\n                self.scopes[-1][a.arg] = new\n                a.arg = new\n\n            if node.args.vararg:\n                new = self.new_name()\n                self.scopes[-1][node.args.vararg.arg] = new\n                node.args.vararg.arg = new\n\n            if node.args.kwarg:\n                new = self.new_name()\n                self.scopes[-1][node.args.kwarg.arg] = new\n                node.args.kwarg.arg = new\n\n            # Visit the lambda body (expression)\n            self.visit(node.body)\n            self.pop_scope()\n            return node\n\n        # Rename identifiers according to the innermost scope first\n        def visit_Name(self, node):\n            # Look for an existing mapping from inner to outer scopes\n            for scope in reversed(self.scopes):\n                if node.id in scope:\n                    newname = scope[node.id]\n                    if node.id != newname:\n                        node.id = newname\n                    return node\n\n            # If this is a local variable being defined (Store context) and we are in a scope,\n            # create a new mapping for it and rename the occurrence.\n            if isinstance(node.ctx, ast.Store):\n                if self.scopes:\n                    newname = self.new_name()\n                    self.scopes[-1][node.id] = newname\n                    node.id = newname\n            return node\n\n    transformer = NormalizeTransformer()\n    transformed = transformer.visit(tree)\n    ast.fix_missing_locations(transformed)\n\n    try:\n        normalized_code = ast.unparse(transformed)\n    except Exception:\n        # If unparsing fails for any reason, return the original code\n        return code_str\n\n    return normalized_code\n```"], "usage": {"prompt_tokens": 412, "completion_tokens": 11859, "total_tokens": 12271, "per_completion": [{"prompt_tokens": 412, "completion_tokens": 11859, "total_tokens": 12271}]}}
