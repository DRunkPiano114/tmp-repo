{"function_name": "agg_loss", "overview": "Aggregates loss matrix to scalar, supporting multiple aggregation modes. DAPO recommends using `token-mean` mode, making each token's contribution to loss equal, avoiding long sequences dominating gradients.", "function_signature": "def agg_loss(loss_mat: torch.Tensor, loss_mask: torch.Tensor, loss_agg_mode: str):", "input_parameters": "- `loss_mat`: Loss matrix with shape `(batch_size, response_length)`\n- `loss_mask`: Mask tensor with shape `(batch_size, response_length)` (response part is 1, padding is 0)\n- `loss_agg_mode`: String specifying aggregation mode\n- `\"token-mean\"`: Average over all tokens (**DAPO recommended**)\n- `\"seq-mean-token-sum\"`: Sum tokens within sequences first, then average across sequences\n- `\"seq-mean-token-mean\"`: Average tokens within sequences first, then average across sequences\n- `\"seq-mean-token-sum-norm\"`: Sum sequences then normalize by sequence length", "detailed_description": "The core function is to aggregate a 2D loss matrix into a single scalar value, supporting the following four aggregation strategies: token-mean, seq-mean-token-sum, seq-mean-token-mean, seq-mean-token-sum-norm.", "output": "- Scalar loss value for backpropagation", "func_implementation": "", "test_code_path": "code/test_code/test_agg_loss.py", "implementation_location": "code/verl/trainer/ppo/core_algos.py:line 704-737", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `agg_loss`.\n\n**Function Overview:**\nAggregates loss matrix to scalar, supporting multiple aggregation modes. DAPO recommends using `token-mean` mode, making each token's contribution to loss equal, avoiding long sequences dominating gradients.\n\n**Function Signature:**\n```python\ndef agg_loss(loss_mat: torch.Tensor, loss_mask: torch.Tensor, loss_agg_mode: str):\n```\n\n**Input Parameters:**\n- `loss_mat`: Loss matrix with shape `(batch_size, response_length)`\n- `loss_mask`: Mask tensor with shape `(batch_size, response_length)` (response part is 1, padding is 0)\n- `loss_agg_mode`: String specifying aggregation mode\n- `\"token-mean\"`: Average over all tokens (**DAPO recommended**)\n- `\"seq-mean-token-sum\"`: Sum tokens within sequences first, then average across sequences\n- `\"seq-mean-token-mean\"`: Average tokens within sequences first, then average across sequences\n- `\"seq-mean-token-sum-norm\"`: Sum sequences then normalize by sequence length\n\n**Detailed Description:**\nThe core function is to aggregate a 2D loss matrix into a single scalar value, supporting the following four aggregation strategies: token-mean, seq-mean-token-sum, seq-mean-token-mean, seq-mean-token-sum-norm.\n\n**Expected Output:**\n- Scalar loss value for backpropagation\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def agg_loss(loss_mat: torch.Tensor, loss_mask: torch.Tensor, loss_agg_mode: str):\n    \"\"\"\n    Aggregate a 2D loss matrix to a scalar according to the specified mode.\n\n    Parameters:\n    - loss_mat: Tensor of shape (batch_size, response_length)\n    - loss_mask: Tensor of shape (batch_size, response_length), where 1 indicates a valid token and 0 a padding\n    - loss_agg_mode: Aggregation mode. One of:\n        - \"token-mean\": mean over all tokens (masked)\n        - \"seq-mean-token-sum\": sum per sequence, then mean across sequences\n        - \"seq-mean-token-mean\": mean per sequence (token-mean within sequence), then mean across sequences\n        - \"seq-mean-token-sum-norm\": per-sequence mean (sum divided by length), then mean across sequences\n\n    Returns:\n    - A scalar tensor representing the aggregated loss (0-dim tensor) suitable for backprop.\n    \"\"\"\n    # Ensure inputs are on the same device and dtype are compatible\n    # All computations are done in-place with PyTorch operations to preserve autograd\n\n    if loss_agg_mode == \"token-mean\":\n        numer = (loss_mat * loss_mask).sum()\n        denom = loss_mask.sum()\n        denom_item = denom.item()\n        if denom_item == 0.0:\n            return torch.tensor(0., device=loss_mat.device, dtype=loss_mat.dtype)\n        return numer / denom\n\n    elif loss_agg_mode == \"seq-mean-token-sum\":\n        per_row_sum = (loss_mat * loss_mask).sum(dim=1)\n        return per_row_sum.mean()\n\n    elif loss_agg_mode == \"seq-mean-token-mean\":\n        per_row_sum = (loss_mat * loss_mask).sum(dim=1)\n        per_row_len = loss_mask.sum(dim=1)\n        per_row_mean = torch.zeros_like(per_row_sum)\n        valid = per_row_len > 0\n        if valid.any():\n            per_row_mean[valid] = per_row_sum[valid] / per_row_len[valid]\n        return per_row_mean.mean()\n\n    elif loss_agg_mode == \"seq-mean-token-sum-norm\":\n        # Per-sequence mean (sum divided by length), then mean across sequences\n        per_row_sum = (loss_mat * loss_mask).sum(dim=1)\n        per_row_len = loss_mask.sum(dim=1)\n        per_row_mean = torch.zeros_like(per_row_sum)\n        valid = per_row_len > 0\n        if valid.any():\n            per_row_mean[valid] = per_row_sum[valid] / per_row_len[valid]\n        return per_row_mean.mean()\n\n    else:\n        raise ValueError(f\"Unsupported loss_agg_mode: {loss_agg_mode}\")"], "usage": {"prompt_tokens": 482, "completion_tokens": 4225, "total_tokens": 4707, "per_completion": [{"prompt_tokens": 482, "completion_tokens": 4225, "total_tokens": 4707}]}}
{"function_name": "compute_policy_loss_vanilla", "overview": "Computes PPO policy gradient loss, supporting decoupled clipping (DAPO's core innovation). Through `clip_ratio_low` and `clip_ratio_high` parameters, allows asymmetric clipping for positive and negative advantages, implementing the \"Clip-Higher\" mechanism.", "function_signature": "def compute_policy_loss_vanilla(\nold_log_prob: torch.Tensor,\nlog_prob: torch.Tensor,\nadvantages: torch.Tensor,\nresponse_mask: torch.Tensor,\nloss_agg_mode: str = \"token-mean\",\nconfig: Optional[DictConfig | AlgoConfig] = None,\nrollout_log_probs: torch.Tensor | None = None,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:", "input_parameters": "- `old_log_prob`: Old policy log probabilities with shape `(batch_size, response_length)`\n- `log_prob`: Current policy log probabilities with shape `(batch_size, response_length)`\n- `advantages`: Advantage estimates with shape `(batch_size, response_length)`\n- `response_mask`: Response mask with shape `(batch_size, response_length)`\n- `loss_agg_mode`: Loss aggregation mode (default is token-mean)\n- `config`: Actor configuration object, containing:\n- `clip_ratio`: Default clipping parameter (backward compatibility)\n- `clip_ratio_low`: Lower bound clipping parameter (**DAPO new**)\n- `clip_ratio_high`: Upper bound clipping parameter (**DAPO new**)\n- `clip_ratio_c`: Dual clipping lower bound (handles extreme negative advantages, default 3.0)\n- `rollout_log_probs`: Optional, Rollout policy log probabilities (for off-policy training)", "detailed_description": "The function implements enhanced PPO policy loss computation, including standard PPO clipped loss calculation, dual-clip PPO, and truncated importance sampling (optional), ultimately calling related functions to implement loss aggregation.\nStandard PPO clipping mechanism supports asymmetric clipping, requiring computation of unclipped original policy gradient loss and asymmetrically clipped loss, taking the maximum of both as pessimistic update strategy. When advantage value is negative, apply dual clipping to more conservatively reduce the probability of that action. If truncated importance sampling is enabled and rollout policy log probabilities are provided, the function computes the importance ratio of old policy relative to rollout policy, clips it to the configured upper limit, then multiplies it into the policy gradient loss.", "output": "Returns four scalar tensors:\n- `pg_loss`: Policy gradient loss\n- `pg_clipfrac`: Clipping fraction (proportion of clipped tokens, for monitoring training dynamics)\n- `ppo_kl`: Approximate KL divergence\n- `pg_clipfrac_lower`: Lower bound clipping fraction (proportion of tokens where dual clipping takes effect)", "func_implementation": "", "test_code_path": "code/test_code/test_compute_policy_loss_vanilla.py", "implementation_location": "code/verl/trainer/ppo/core_algos.py:line 816-902", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_policy_loss_vanilla`.\n\n**Function Overview:**\nComputes PPO policy gradient loss, supporting decoupled clipping (DAPO's core innovation). Through `clip_ratio_low` and `clip_ratio_high` parameters, allows asymmetric clipping for positive and negative advantages, implementing the \"Clip-Higher\" mechanism.\n\n**Function Signature:**\n```python\ndef compute_policy_loss_vanilla(\nold_log_prob: torch.Tensor,\nlog_prob: torch.Tensor,\nadvantages: torch.Tensor,\nresponse_mask: torch.Tensor,\nloss_agg_mode: str = \"token-mean\",\nconfig: Optional[DictConfig | AlgoConfig] = None,\nrollout_log_probs: torch.Tensor | None = None,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n```\n\n**Input Parameters:**\n- `old_log_prob`: Old policy log probabilities with shape `(batch_size, response_length)`\n- `log_prob`: Current policy log probabilities with shape `(batch_size, response_length)`\n- `advantages`: Advantage estimates with shape `(batch_size, response_length)`\n- `response_mask`: Response mask with shape `(batch_size, response_length)`\n- `loss_agg_mode`: Loss aggregation mode (default is token-mean)\n- `config`: Actor configuration object, containing:\n- `clip_ratio`: Default clipping parameter (backward compatibility)\n- `clip_ratio_low`: Lower bound clipping parameter (**DAPO new**)\n- `clip_ratio_high`: Upper bound clipping parameter (**DAPO new**)\n- `clip_ratio_c`: Dual clipping lower bound (handles extreme negative advantages, default 3.0)\n- `rollout_log_probs`: Optional, Rollout policy log probabilities (for off-policy training)\n\n**Detailed Description:**\nThe function implements enhanced PPO policy loss computation, including standard PPO clipped loss calculation, dual-clip PPO, and truncated importance sampling (optional), ultimately calling related functions to implement loss aggregation.\nStandard PPO clipping mechanism supports asymmetric clipping, requiring computation of unclipped original policy gradient loss and asymmetrically clipped loss, taking the maximum of both as pessimistic update strategy. When advantage value is negative, apply dual clipping to more conservatively reduce the probability of that action. If truncated importance sampling is enabled and rollout policy log probabilities are provided, the function computes the importance ratio of old policy relative to rollout policy, clips it to the configured upper limit, then multiplies it into the policy gradient loss.\n\n**Expected Output:**\nReturns four scalar tensors:\n- `pg_loss`: Policy gradient loss\n- `pg_clipfrac`: Clipping fraction (proportion of clipped tokens, for monitoring training dynamics)\n- `ppo_kl`: Approximate KL divergence\n- `pg_clipfrac_lower`: Lower bound clipping fraction (proportion of tokens where dual clipping takes effect)\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def compute_policy_loss_vanilla(\n    old_log_prob: torch.Tensor,\n    log_prob: torch.Tensor,\n    advantages: torch.Tensor,\n    response_mask: torch.Tensor,\n    loss_agg_mode: str = \"token-mean\",\n    config: Optional[DictConfig | AlgoConfig] = None,\n    rollout_log_probs: torch.Tensor | None = None,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Compute PPO-style policy loss with decoupled clipping (DAPO) support.\n\n    Features:\n    - Standard PPO clipped loss with asymmetric clipping (clip_ratio_low / clip_ratio_high).\n    - Dual clipping for negative advantages using clip_ratio_c (lower bound).\n    - Optional truncated importance sampling when rollout_log_probs is provided.\n    - Loss aggregation modes (token-mean by default; supports 'mean' as fallback).\n    - Returns: (pg_loss, pg_clipfrac, ppo_kl, pg_clipfrac_lower)\n\n    Notes:\n    - All computations respect response_mask.\n    - Assumes per-token shapes: (batch_size, response_length)\n    - config may be None; attributes accessed via getattr to allow optional configuration.\n    \"\"\"\n    # Basic dtype handling and mask\n    device = old_log_prob.device\n    dtype = old_log_prob.dtype\n\n    # Ensure masks are same dtype as logits for arithmetic\n    mask = response_mask.to(dtype)\n\n    # Clip configuration (supporting both legacy and new parameters)\n    clip_ratio = getattr(config, \"clip_ratio\", 0.2) if config is not None else 0.2\n    clip_ratio_low = getattr(config, \"clip_ratio_low\", None) if config is not None else None\n    clip_ratio_high = getattr(config, \"clip_ratio_high\", None) if config is not None else None\n    clip_ratio_c = getattr(config, \"clip_ratio_c\", 3.0) if config is not None else 3.0\n\n    # Determine clipping bounds\n    if (clip_ratio_low is None) and (clip_ratio_high is None):\n        lower = 1.0 - clip_ratio\n        upper = 1.0 + clip_ratio\n    else:\n        # Use provided asymmetric clipping if any\n        low_delta = clip_ratio_low if clip_ratio_low is not None else clip_ratio\n        high_delta = clip_ratio_high if clip_ratio_high is not None else clip_ratio\n        lower = 1.0 - low_delta\n        upper = 1.0 + high_delta\n\n    # Dual clipping lower bound for negative advantages\n    lower_dual = 1.0 / float(clip_ratio_c)\n\n    # Compute probability ratio (current / old)\n    ratio = torch.exp(log_prob - old_log_prob)  # shape: (B, L)\n\n    # Track where advantages are positive/negative for asymmetric clipping\n    is_pos_adv = advantages > 0\n\n    # Prepare ratio_clipped with asymmetric clipping\n    ratio_clipped = ratio.clone()\n\n    # For positive advantages, apply standard asymmetric clipping [lower, upper]\n    if ratio_clipped[is_pos_adv].numel() > 0:\n        ratio_clipped_pos = ratio[is_pos_adv].clamp(min=lower, max=upper)\n        ratio_clipped[is_pos_adv] = ratio_clipped_pos\n\n    # For negative advantages, apply dual clipping (lower bound only)\n    if ratio_clipped[~is_pos_adv].numel() > 0:\n        ratio_clipped_neg = ratio[~is_pos_adv].clamp(min=lower_dual)\n        ratio_clipped[~is_pos_adv] = ratio_clipped_neg\n\n    # Unclipped and clipped policy gradient per-token losses\n    # Note: We minimize the negative of the objective, hence the negative signs\n    unclipped_pg = -ratio * advantages\n    clipped_pg = -ratio_clipped * advantages\n\n    # Final per-token GRADIENT loss using \"pessimistic\" update as max(unclipped, clipped)\n    # This follows the description's \"maximum of both\" strategy (pessimistic update).\n    pg_loss_per_token = torch.maximum(unclipped_pg, clipped_pg)\n\n    # Optional: Truncated Importance Sampling using rollout_policy\n    if rollout_log_probs is not None:\n        # Ratio of old policy to rollout policy\n        ratio_rollout = torch.exp(old_log_prob - rollout_log_probs)\n        # Clip rollout ratio to upper bound (reuse the same upper bound as before)\n        ratio_rollout_clipped = torch.clamp(ratio_rollout, max=upper)\n        pg_loss_per_token = pg_loss_per_token * ratio_rollout_clipped\n\n    # Aggregation helper\n    def aggregate_with_mask(x: torch.Tensor, m: torch.Tensor, mode: str) -> torch.Tensor:\n        x = x * m\n        if mode == \"token-mean\":\n            denom = m.sum().clamp(min=1.0)\n            return (x.sum()) / denom\n        elif mode == \"mean\":\n            denom = m.numel()\n            return (x.sum()) / denom\n        else:\n            # Fallback to token-mean behavior for unknown modes\n            denom = m.sum().clamp(min=1.0)\n            return (x.sum()) / denom\n\n    # Final policy gradient loss\n    pg_loss = aggregate_with_mask(pg_loss_per_token, mask, loss_agg_mode)\n\n    # Clip fraction statistics\n    denom_for_frac = mask.sum().clamp(min=1.0)\n\n    # Tokens clipped relative to the ratio_clipped decision\n    clipped_mask_pos = (ratio > upper) | (ratio < lower)\n    clipped_mask_neg = (ratio < lower_dual)\n    clip_token_mask = torch.zeros_like(advantages, dtype=dtype)\n    clip_token_mask[is_pos_adv] = clipped_mask_pos[is_pos_adv].to(dtype)\n    clip_token_mask[~is_pos_adv] = clipped_mask_neg[~is_pos_adv].to(dtype)\n\n    pg_clipfrac = (clip_token_mask * mask).sum() / denom_for_frac\n\n    # Dual clipping fraction: how often dual clipping took effect (negative adv and ratio below lower_dual)\n    dual_clip_mask = (~is_pos_adv) & (ratio < lower_dual)\n    pg_clipfrac_lower = (dual_clip_mask.to(dtype) * mask).sum() / denom_for_frac\n\n    # Approximate KL divergence: KL(pi_old || pi_new) â‰ˆ E_pi_old[log(pi_old) - log(pi_new)]\n    kl_per_token = old_log_prob - log_prob\n    ppo_kl = (kl_per_token * mask).sum() / denom_for_frac\n\n    return pg_loss, pg_clipfrac, ppo_kl, pg_clipfrac_lower"], "usage": {"prompt_tokens": 753, "completion_tokens": 9017, "total_tokens": 9770, "per_completion": [{"prompt_tokens": 753, "completion_tokens": 9017, "total_tokens": 9770}]}}
{"function_name": "DAPORewardManager.__call__", "overview": "DAPO's reward manager, responsible for computing task rewards and applying overlong penalty. This is one of the key components distinguishing DAPO from other methods, preventing the model from generating meaningless lengthy outputs through a linear penalty mechanism.", "function_signature": "def __call__(self, data: DataProto, return_dict: bool = False):", "input_parameters": "- `data`: DataProto object containing:\n- `batch[\"prompts\"]`: Prompt token IDs with shape `(bs, prompt_len)`\n- `batch[\"responses\"]`: Response token IDs with shape `(bs, response_len)`\n- `batch[\"attention_mask\"]`: Attention mask with shape `(bs, total_len)`\n- `non_tensor_batch[\"reward_model\"][\"ground_truth\"]`: Ground truth answers\n- `non_tensor_batch[\"data_source\"]`: Data source (e.g., \"gsm8k\", \"math\")\n- `return_dict`: Boolean, whether to return dictionary format (including additional information like accuracy, overlong penalty, etc.)", "detailed_description": "The function is responsible for computing reward scores for generated responses. First checks if there are pre-computed rewards; if so, returns directly; otherwise processes each response individually, decoding token sequences into text and computing task rewards based on ground truth answers (needs to call related functional functions). Computed scores are placed at the last valid token position in the response sequence. If length control is enabled, the function applies additional negative reward penalty for overlong responses (linear penalty mechanism). Throughout the process, detailed information for some samples is output for debugging verification, ultimately returning complete reward tensor or dictionary containing additional evaluation metrics.", "output": "- If `return_dict=False`: Returns sparse reward tensor with shape `(batch_size, response_length)` (only last valid token position is non-zero)\n- If `return_dict=True`: Returns dictionary containing:\n- `\"reward_tensor\"`: Reward tensor as above\n- `\"reward_extra_info\"`: Dictionary containing `\"acc\"` (accuracy list), `\"overlong_reward\"` (overlong penalty list), `\"overlong\"` (boolean list indicating if overlong), etc.", "func_implementation": "", "test_code_path": "code/test_code/test_dapo_reward_manager.py", "implementation_location": "code/verl/workers/reward_manager/dapo.py:line 53-149", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `DAPORewardManager.__call__`.\n\n**Function Overview:**\nDAPO's reward manager, responsible for computing task rewards and applying overlong penalty. This is one of the key components distinguishing DAPO from other methods, preventing the model from generating meaningless lengthy outputs through a linear penalty mechanism.\n\n**Function Signature:**\n```python\ndef __call__(self, data: DataProto, return_dict: bool = False):\n```\n\n**Input Parameters:**\n- `data`: DataProto object containing:\n- `batch[\"prompts\"]`: Prompt token IDs with shape `(bs, prompt_len)`\n- `batch[\"responses\"]`: Response token IDs with shape `(bs, response_len)`\n- `batch[\"attention_mask\"]`: Attention mask with shape `(bs, total_len)`\n- `non_tensor_batch[\"reward_model\"][\"ground_truth\"]`: Ground truth answers\n- `non_tensor_batch[\"data_source\"]`: Data source (e.g., \"gsm8k\", \"math\")\n- `return_dict`: Boolean, whether to return dictionary format (including additional information like accuracy, overlong penalty, etc.)\n\n**Detailed Description:**\nThe function is responsible for computing reward scores for generated responses. First checks if there are pre-computed rewards; if so, returns directly; otherwise processes each response individually, decoding token sequences into text and computing task rewards based on ground truth answers (needs to call related functional functions). Computed scores are placed at the last valid token position in the response sequence. If length control is enabled, the function applies additional negative reward penalty for overlong responses (linear penalty mechanism). Throughout the process, detailed information for some samples is output for debugging verification, ultimately returning complete reward tensor or dictionary containing additional evaluation metrics.\n\n**Expected Output:**\n- If `return_dict=False`: Returns sparse reward tensor with shape `(batch_size, response_length)` (only last valid token position is non-zero)\n- If `return_dict=True`: Returns dictionary containing:\n- `\"reward_tensor\"`: Reward tensor as above\n- `\"reward_extra_info\"`: Dictionary containing `\"acc\"` (accuracy list), `\"overlong_reward\"` (overlong penalty list), `\"overlong\"` (boolean list indicating if overlong), etc.\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def __call__(self, data: DataProto, return_dict: bool = False):\n    \"\"\"\n    DAPO Reward Manager: compute per-token rewards for generated responses with an\n    overlong (linear) penalty. Rewards are placed at the last valid token position of\n    each response. If precomputed rewards exist, they are returned directly.\n\n    Returns:\n        - If return_dict is False: a tensor of shape (batch_size, response_len) with\n          non-zero values only at the last valid token position for each sample.\n        - If return_dict is True: a dict with keys:\n            {\"reward_tensor\": <tensor>, \"reward_extra_info\": {...}}\n          where reward_extra_info contains \"acc\", \"overlong_reward\", \"overlong\", and\n          optionally \"debug_infos\".\n    \"\"\"\n    import torch\n\n    # Retrieve batch data (expected structure per description)\n    batch = getattr(data, \"batch\", None)\n    non_tensor_batch = getattr(data, \"non_tensor_batch\", {})\n\n    # Quick path: use pre-computed rewards if available\n    precomputed = getattr(self, \"_precomputed_reward_tensor\", None)\n    if precomputed is not None:\n        reward_tensor = precomputed\n        if return_dict:\n            reward_extra_info = {\n                \"acc\": [],\n                \"overlong_reward\": [],\n                \"overlong\": [],\n            }\n            return {\n                \"reward_tensor\": reward_tensor,\n                \"reward_extra_info\": reward_extra_info,\n            }\n        else:\n            return reward_tensor\n\n    if batch is None:\n        # If there is no batch, return an empty tensor gracefully\n        empty_tensor = torch.zeros((0, 0), dtype=torch.float32)\n        if return_dict:\n            return {\"reward_tensor\": empty_tensor, \"reward_extra_info\": {\"acc\": [], \"overlong_reward\": [], \"overlong\": []}}\n        else:\n            return empty_tensor\n\n    # Basic batch sizes\n    prompts = batch[\"prompts\"]\n    responses = batch[\"responses\"]\n    batch_size = int(prompts.shape[0])\n    resp_len = int(responses.shape[1])\n\n    # Device-appropriate tensor for rewards\n    device = prompts.device if isinstance(prompts, torch.Tensor) else \"cpu\"\n    reward_tensor = torch.zeros((batch_size, resp_len), dtype=torch.float32, device=device)\n\n    # Optional hyperparameters with safe defaults\n    max_response_len = getattr(self, \"max_response_len\", 128)\n    overlong_penalty_coef = getattr(self, \"overlong_penalty_coef\", 0.1)\n\n    # Helper to decode tokens to text in a robust way\n    def _decode_tokens(token_seq):\n        # token_seq can be a torch.Tensor or a Python list\n        if isinstance(token_seq, torch.Tensor):\n            toks = token_seq.tolist()\n        else:\n            toks = list(token_seq)\n        # Try tokenizer if available\n        if hasattr(self, \"tokenizer\") and getattr(self, \"tokenizer\") is not None:\n            try:\n                return self.tokenizer.decode(toks, skip_special_tokens=True)\n            except Exception:\n                pass\n        # Fallback: join as strings\n        try:\n            return \" \".join(str(t) for t in toks)\n        except Exception:\n            return str(toks)\n\n    # Accumulators for extra info\n    acc_list = []\n    overlong_reward_list = []\n    overlong_bool_list = []\n    debug_infos = []\n\n    # Ground truth retrieval helper (robust to missing keys)\n    ground_truths = None\n    if isinstance(non_tensor_batch, dict):\n        reward_model = non_tensor_batch.get(\"reward_model\", {})\n        if isinstance(reward_model, dict):\n            ground_truths = reward_model.get(\"ground_truth\", None)\n\n    # Iterate over batch samples\n    for i in range(batch_size):\n        # Last valid token index in the response (non-zero token)\n        resp_seq = responses[i]\n        if isinstance(resp_seq, torch.Tensor):\n            L = resp_seq.shape[0]\n            last_valid = L - 1\n            while last_valid >= 0 and int(resp_seq[last_valid].item()) == 0:\n                last_valid -= 1\n            if last_valid < 0:\n                last_valid = 0\n            last_valid_idx = last_valid\n            pred_tokens = resp_seq[: last_valid_idx + 1]\n        else:\n            # Assume list-like\n            L = len(resp_seq)\n            last_valid = L - 1\n            while last_valid >= 0 and int(resp_seq[last_valid]) == 0:\n                last_valid -= 1\n            if last_valid < 0:\n                last_valid = 0\n            last_valid_idx = last_valid\n            pred_tokens = resp_seq[: last_valid_idx + 1]\n\n        pred_text = _decode_tokens(pred_tokens)\n\n        # Ground truth for this sample\n        ground_truth = None\n        if ground_truths is not None:\n            if isinstance(ground_truths, (list, tuple)):\n                if i < len(ground_truths):\n                    ground_truth = ground_truths[i]\n            else:\n                ground_truth = ground_truths\n\n        # Data source (optional, for potential specialized scoring)\n        data_sources = None\n        if isinstance(non_tensor_batch, dict) and \"data_source\" in non_tensor_batch:\n            ds = non_tensor_batch[\"data_source\"]\n            if isinstance(ds, (list, tuple)):\n                if i < len(ds):\n                    data_sources = ds[i]\n            else:\n                data_sources = ds\n\n        # Accuracy indicator (simple substring match with ground truth)\n        acc_i = 0.0\n        if isinstance(ground_truth, str) and isinstance(pred_text, str):\n            gt_lower = ground_truth.lower().strip()\n            pred_lower = pred_text.lower().strip()\n            if gt_lower and (gt_lower in pred_lower):\n                acc_i = 1.0\n        acc_list.append(acc_i)\n\n        # Overlong detection and linear penalty\n        # Determine actual response length for this sample\n        if isinstance(responses[i], torch.Tensor):\n            actual_len = int(responses[i].shape[0])\n        else:\n            actual_len = int(len(responses[i]))\n        overlong = actual_len > max_response_len\n        overlong_bool_list.append(bool(overlong))\n        penalty = -float(max(0, actual_len - max_response_len)) * float(overlong_penalty_coef) if overlong else 0.0\n        overlong_reward_list.append(penalty)\n\n        # Compute base task reward from ground truth and prediction\n        task_reward = 0.0\n        if isinstance(ground_truth, str) and isinstance(pred_text, str):\n            gt_lower = ground_truth.lower()\n            pred_lower = pred_text.lower()\n            task_reward = 1.0 if (gt_lower in pred_lower) else 0.0\n\n        final_reward = float(task_reward) + float(penalty)\n\n        # Place the final reward at the last valid token position\n        reward_tensor[i, last_valid_idx] = final_reward\n\n        # Optional debug info for a few samples\n        if i < 3:\n            debug_infos.append({\n                \"idx\": i,\n                \"pred_text\": pred_text,\n                \"ground_truth\": ground_truth,\n                \"acc\": acc_i,\n                \"overlong\": bool(overlong),\n                \"actual_len\": actual_len,\n                \"max_allowed\": max_response_len,\n                \"penalty\": penalty,\n                \"final_reward\": final_reward,\n            })\n\n    # Cache for potential reuse\n    self._precomputed_reward_tensor = reward_tensor\n\n    reward_extra_info = {\n        \"acc\": acc_list,\n        \"overlong_reward\": overlong_reward_list,\n        \"overlong\": overlong_bool_list,\n    }\n    if debug_infos:\n        reward_extra_info[\"debug_infos\"] = debug_infos\n\n    if return_dict:\n        return {\"reward_tensor\": reward_tensor, \"reward_extra_info\": reward_extra_info}\n    else:\n        return reward_tensor"], "usage": {"prompt_tokens": 638, "completion_tokens": 6633, "total_tokens": 7271, "per_completion": [{"prompt_tokens": 638, "completion_tokens": 6633, "total_tokens": 7271}]}}
