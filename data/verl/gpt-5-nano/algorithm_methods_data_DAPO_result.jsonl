{"function_name": "agg_loss", "overview": "Aggregates loss matrix to scalar, supporting multiple aggregation modes. DAPO recommends using `token-mean` mode, making each token's contribution to loss equal, avoiding long sequences dominating gradients.", "function_signature": "def agg_loss(loss_mat: torch.Tensor, loss_mask: torch.Tensor, loss_agg_mode: str):", "input_parameters": "- `loss_mat`: Loss matrix with shape `(batch_size, response_length)`\n- `loss_mask`: Mask tensor with shape `(batch_size, response_length)` (response part is 1, padding is 0)\n- `loss_agg_mode`: String specifying aggregation mode\n- `\"token-mean\"`: Average over all tokens (**DAPO recommended**)\n- `\"seq-mean-token-sum\"`: Sum tokens within sequences first, then average across sequences\n- `\"seq-mean-token-mean\"`: Average tokens within sequences first, then average across sequences\n- `\"seq-mean-token-sum-norm\"`: Sum sequences then normalize by sequence length", "detailed_description": "The core function is to aggregate a 2D loss matrix into a single scalar value, supporting the following four aggregation strategies: token-mean, seq-mean-token-sum, seq-mean-token-mean, seq-mean-token-sum-norm.", "output": "- Scalar loss value for backpropagation", "func_implementation": "", "test_code_path": "code/test_code/test_agg_loss.py", "implementation_location": "code/verl/trainer/ppo/core_algos.py:line 704-737", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `agg_loss`.\n\n**Function Overview:**\nAggregates loss matrix to scalar, supporting multiple aggregation modes. DAPO recommends using `token-mean` mode, making each token's contribution to loss equal, avoiding long sequences dominating gradients.\n\n**Function Signature:**\n```python\ndef agg_loss(loss_mat: torch.Tensor, loss_mask: torch.Tensor, loss_agg_mode: str):\n```\n\n**Input Parameters:**\n- `loss_mat`: Loss matrix with shape `(batch_size, response_length)`\n- `loss_mask`: Mask tensor with shape `(batch_size, response_length)` (response part is 1, padding is 0)\n- `loss_agg_mode`: String specifying aggregation mode\n- `\"token-mean\"`: Average over all tokens (**DAPO recommended**)\n- `\"seq-mean-token-sum\"`: Sum tokens within sequences first, then average across sequences\n- `\"seq-mean-token-mean\"`: Average tokens within sequences first, then average across sequences\n- `\"seq-mean-token-sum-norm\"`: Sum sequences then normalize by sequence length\n\n**Detailed Description:**\nThe core function is to aggregate a 2D loss matrix into a single scalar value, supporting the following four aggregation strategies: token-mean, seq-mean-token-sum, seq-mean-token-mean, seq-mean-token-sum-norm.\n\n**Expected Output:**\n- Scalar loss value for backpropagation\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def agg_loss(loss_mat: torch.Tensor, loss_mask: torch.Tensor, loss_agg_mode: str):\n    \"\"\"\n    Aggregate a 2D loss matrix into a scalar according to the specified mode.\n\n    Modes:\n    - \"token-mean\": average over all valid tokens across the batch\n    - \"seq-mean-token-sum\": for each sequence, sum its valid token losses, then average across sequences\n    - \"seq-mean-token-mean\": for each sequence, mean its valid token losses, then average across sequences\n    - \"seq-mean-token-sum-norm\": for each sequence, mean its valid token losses, then sum these means across sequences\n\n    Inputs:\n    - loss_mat: Tensor of shape (batch_size, response_length)\n    - loss_mask: Tensor of shape (batch_size, response_length), where 1 indicates valid token, 0 padding\n    - loss_agg_mode: aggregation mode string\n\n    Returns:\n    - A scalar tensor representing the aggregated loss (supports backpropagation)\n    \"\"\"\n    if loss_mat.dim() != 2 or loss_mask.dim() != 2:\n        raise ValueError(\"loss_mat and loss_mask must be 2D tensors with shape (batch_size, response_length)\")\n\n    if loss_mat.shape != loss_mask.shape:\n        raise ValueError(\"loss_mat and loss_mask must have the same shape\")\n\n    # Apply mask to compute per-sequence sums and per-token masked values\n    masked = loss_mat * loss_mask  # (B, T)\n    per_seq_sum = masked.sum(dim=1)  # (B,)\n    per_seq_len = loss_mask.sum(dim=1)  # (B,)\n\n    if loss_agg_mode == \"token-mean\":\n        total_tokens = per_seq_len.sum()\n        if total_tokens == 0:\n            return torch.tensor(0.0, dtype=loss_mat.dtype, device=loss_mat.device)\n        return masked.sum() / total_tokens\n\n    elif loss_agg_mode == \"seq-mean-token-sum\":\n        return per_seq_sum.mean()\n\n    elif loss_agg_mode == \"seq-mean-token-mean\":\n        per_seq_len_safe = per_seq_len.clamp(min=1)\n        per_seq_mean = per_seq_sum / per_seq_len_safe\n        return per_seq_mean.mean()\n\n    elif loss_agg_mode == \"seq-mean-token-sum-norm\":\n        per_seq_len_safe = per_seq_len.clamp(min=1)\n        per_seq_mean = per_seq_sum / per_seq_len_safe\n        return per_seq_mean.sum()\n\n    else:\n        raise ValueError(f\"Unknown loss_agg_mode: {loss_agg_mode}\")"], "usage": {"prompt_tokens": 482, "completion_tokens": 3271, "total_tokens": 3753, "per_completion": [{"prompt_tokens": 482, "completion_tokens": 3271, "total_tokens": 3753}]}, "results": [false], "pass_ratios": [0.0]}
{"function_name": "compute_policy_loss_vanilla", "overview": "Computes PPO policy gradient loss, supporting decoupled clipping (DAPO's core innovation). Through `clip_ratio_low` and `clip_ratio_high` parameters, allows asymmetric clipping for positive and negative advantages, implementing the \"Clip-Higher\" mechanism.", "function_signature": "def compute_policy_loss_vanilla(\nold_log_prob: torch.Tensor,\nlog_prob: torch.Tensor,\nadvantages: torch.Tensor,\nresponse_mask: torch.Tensor,\nloss_agg_mode: str = \"token-mean\",\nconfig: Optional[DictConfig | AlgoConfig] = None,\nrollout_log_probs: torch.Tensor | None = None,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:", "input_parameters": "- `old_log_prob`: Old policy log probabilities with shape `(batch_size, response_length)`\n- `log_prob`: Current policy log probabilities with shape `(batch_size, response_length)`\n- `advantages`: Advantage estimates with shape `(batch_size, response_length)`\n- `response_mask`: Response mask with shape `(batch_size, response_length)`\n- `loss_agg_mode`: Loss aggregation mode (default is token-mean)\n- `config`: Actor configuration object, containing:\n- `clip_ratio`: Default clipping parameter (backward compatibility)\n- `clip_ratio_low`: Lower bound clipping parameter (**DAPO new**)\n- `clip_ratio_high`: Upper bound clipping parameter (**DAPO new**)\n- `clip_ratio_c`: Dual clipping lower bound (handles extreme negative advantages, default 3.0)\n- `rollout_log_probs`: Optional, Rollout policy log probabilities (for off-policy training)", "detailed_description": "The function implements enhanced PPO policy loss computation, including standard PPO clipped loss calculation, dual-clip PPO, and truncated importance sampling (optional), ultimately calling related functions to implement loss aggregation.\nStandard PPO clipping mechanism supports asymmetric clipping, requiring computation of unclipped original policy gradient loss and asymmetrically clipped loss, taking the maximum of both as pessimistic update strategy. When advantage value is negative, apply dual clipping to more conservatively reduce the probability of that action. If truncated importance sampling is enabled and rollout policy log probabilities are provided, the function computes the importance ratio of old policy relative to rollout policy, clips it to the configured upper limit, then multiplies it into the policy gradient loss.", "output": "Returns four scalar tensors:\n- `pg_loss`: Policy gradient loss\n- `pg_clipfrac`: Clipping fraction (proportion of clipped tokens, for monitoring training dynamics)\n- `ppo_kl`: Approximate KL divergence\n- `pg_clipfrac_lower`: Lower bound clipping fraction (proportion of tokens where dual clipping takes effect)", "func_implementation": "", "test_code_path": "code/test_code/test_compute_policy_loss_vanilla.py", "implementation_location": "code/verl/trainer/ppo/core_algos.py:line 816-902", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_policy_loss_vanilla`.\n\n**Function Overview:**\nComputes PPO policy gradient loss, supporting decoupled clipping (DAPO's core innovation). Through `clip_ratio_low` and `clip_ratio_high` parameters, allows asymmetric clipping for positive and negative advantages, implementing the \"Clip-Higher\" mechanism.\n\n**Function Signature:**\n```python\ndef compute_policy_loss_vanilla(\nold_log_prob: torch.Tensor,\nlog_prob: torch.Tensor,\nadvantages: torch.Tensor,\nresponse_mask: torch.Tensor,\nloss_agg_mode: str = \"token-mean\",\nconfig: Optional[DictConfig | AlgoConfig] = None,\nrollout_log_probs: torch.Tensor | None = None,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n```\n\n**Input Parameters:**\n- `old_log_prob`: Old policy log probabilities with shape `(batch_size, response_length)`\n- `log_prob`: Current policy log probabilities with shape `(batch_size, response_length)`\n- `advantages`: Advantage estimates with shape `(batch_size, response_length)`\n- `response_mask`: Response mask with shape `(batch_size, response_length)`\n- `loss_agg_mode`: Loss aggregation mode (default is token-mean)\n- `config`: Actor configuration object, containing:\n- `clip_ratio`: Default clipping parameter (backward compatibility)\n- `clip_ratio_low`: Lower bound clipping parameter (**DAPO new**)\n- `clip_ratio_high`: Upper bound clipping parameter (**DAPO new**)\n- `clip_ratio_c`: Dual clipping lower bound (handles extreme negative advantages, default 3.0)\n- `rollout_log_probs`: Optional, Rollout policy log probabilities (for off-policy training)\n\n**Detailed Description:**\nThe function implements enhanced PPO policy loss computation, including standard PPO clipped loss calculation, dual-clip PPO, and truncated importance sampling (optional), ultimately calling related functions to implement loss aggregation.\nStandard PPO clipping mechanism supports asymmetric clipping, requiring computation of unclipped original policy gradient loss and asymmetrically clipped loss, taking the maximum of both as pessimistic update strategy. When advantage value is negative, apply dual clipping to more conservatively reduce the probability of that action. If truncated importance sampling is enabled and rollout policy log probabilities are provided, the function computes the importance ratio of old policy relative to rollout policy, clips it to the configured upper limit, then multiplies it into the policy gradient loss.\n\n**Expected Output:**\nReturns four scalar tensors:\n- `pg_loss`: Policy gradient loss\n- `pg_clipfrac`: Clipping fraction (proportion of clipped tokens, for monitoring training dynamics)\n- `ppo_kl`: Approximate KL divergence\n- `pg_clipfrac_lower`: Lower bound clipping fraction (proportion of tokens where dual clipping takes effect)\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def compute_policy_loss_vanilla(\n    old_log_prob: torch.Tensor,\n    log_prob: torch.Tensor,\n    advantages: torch.Tensor,\n    response_mask: torch.Tensor,\n    loss_agg_mode: str = \"token-mean\",\n    config: Optional[DictConfig | AlgoConfig] = None,\n    rollout_log_probs: torch.Tensor | None = None,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Compute PPO-style policy loss with optional decoupled (asymmetric) clipping (DAPO)\n    and optional truncated importance sampling (off-policy weighting).\n\n    Returns:\n        pg_loss: scalar policy gradient loss (to be minimized)\n        pg_clipfrac: fraction of tokens clipped by the clipping bounds\n        ppo_kl: approximate KL divergence = mean(old_log_prob - log_prob) over masked tokens\n        pg_clipfrac_lower: fraction of tokens where dual clipping (lower bound) took effect\n    \"\"\"\n    # Ensure tensors are the expected shapes\n    # Shapes: (batch_size, response_length)\n    mask = response_mask.bool()\n    A = advantages\n    olp = old_log_prob\n    lp = log_prob\n\n    # Clip/clip bounds configuration\n    # If explicit decoupled bounds are provided, use them; otherwise fall back to symmetric PPO clip\n    clip_low = getattr(config, \"clip_ratio_low\", None)\n    clip_high = getattr(config, \"clip_ratio_high\", None)\n\n    if clip_low is not None and clip_high is not None:\n        lb = 1.0 - float(clip_low)\n        ub = 1.0 + float(clip_high)\n    else:\n        # Fallback to symmetric clipping using clip_ratio (default ~0.2)\n        clip_sym = getattr(config, \"clip_ratio\", 0.2) if config is not None else 0.2\n        lb = 1.0 - float(clip_sym)\n        ub = 1.0 + float(clip_sym)\n\n    # Basic ratio r_t = pi_theta(a|s) / pi_old(a|s) in log-domain\n    ratio = torch.exp(lp - olp)  # shape: (B, T)\n\n    # Unclipped policy gradient component\n    pg_unclipped = ratio * A\n\n    # Clipped policy gradient component with decoupled clipping:\n    # - For positive advantages, clip ratio to [1, 1+clip_high] (upper bound)\n    # - For negative advantages, clip ratio to [1-clip_low, 1] (lower bound)\n    pos_mask = A > 0\n    if clip_high is not None or clip_low is not None:\n        # Determine per-element clipped ratio depending on sign of advantage\n        if clip_high is not None:\n            r_clip_pos = torch.clamp(ratio, min=1.0, max=1.0 + float(clip_high))\n        else:\n            # If clip_high not provided, fall back to symmetric upper bound if available\n            r_clip_pos = ratio  # no extra clipping on positive side\n\n        if clip_low is not None:\n            r_clip_neg = torch.clamp(ratio, min=1.0 - float(clip_low), max=1.0)\n        else:\n            r_clip_neg = ratio  # no extra clipping on negative side\n    else:\n        # No decoupled clipping provided; use symmetric clipping with lb/ub\n        r_clip_pos = torch.clamp(ratio, min=1.0, max=ub)\n        r_clip_neg = torch.clamp(ratio, min=lb, max=1.0)\n\n    ratio_clipped = torch.where(pos_mask, r_clip_pos, r_clip_neg)\n    pg_clipped = ratio_clipped * A\n\n    # Per-token clipped objective (minimization form)\n    per_token_loss = torch.minimum(pg_unclipped, pg_clipped)\n\n    # Compute loss aggregation\n    weights = mask.float()\n    if loss_agg_mode == \"token-mean\":\n        pg_loss = - (per_token_loss * weights).sum() / (weights.sum() + 1e-6)\n    elif loss_agg_mode == \"token-sum\":\n        pg_loss = - (per_token_loss * weights).sum()\n    else:\n        # Default to token-mean if an unknown mode is provided\n        pg_loss = - (per_token_loss * weights).sum() / (weights.sum() + 1e-6)\n\n    # Clipping statistics\n    is_clipped = (ratio < lb) | (ratio > ub)\n    pg_clipfrac = (is_clipped & mask).float().sum() / (mask.sum() + 1e-6)\n\n    # Dual clipping lower-bound effectiveness (on negative advantages)\n    lower_bound_neg = 1.0 - (float(clip_low) if clip_low is not None else (getattr(config, \"clip_ratio\", 0.2)))\n    dual_active = (A < 0) & (ratio < lower_bound_neg) & mask\n    pg_clipfrac_lower = dual_active.float().sum() / (mask.sum() + 1e-6)\n\n    # Approximate KL divergence between old and current policies\n    ppo_kl = ((olp - lp) * mask).sum() / (mask.sum() + 1e-6)\n\n    # Optional off-policy truncation via rollout_log_probs\n    if rollout_log_probs is not None:\n        # Compute importance ratio old / rollout\n        r_off = torch.exp(olp - rollout_log_probs)\n        # Clip to upper bound (as described)\n        r_off_ub = ub  # same ub as policy clipping upper bound\n        r_off_clipped = torch.clamp(r_off, max=float(r_off_ub))\n        # Apply truncated importance sampling by scaling the per-token loss\n        per_token_loss = per_token_loss * r_off_clipped\n        # Recompute the final loss with the (potentially) reweighted per-token loss\n        if loss_agg_mode == \"token-mean\":\n            pg_loss = - (per_token_loss * weights).sum() / (weights.sum() + 1e-6)\n        elif loss_agg_mode == \"token-sum\":\n            pg_loss = - (per_token_loss * weights).sum()\n        else:\n            pg_loss = - (per_token_loss * weights).sum() / (weights.sum() + 1e-6)\n\n    return pg_loss, pg_clipfrac, ppo_kl, pg_clipfrac_lower"], "usage": {"prompt_tokens": 753, "completion_tokens": 8242, "total_tokens": 8995, "per_completion": [{"prompt_tokens": 753, "completion_tokens": 8242, "total_tokens": 8995}]}, "results": [false], "pass_ratios": [0.0]}
{"function_name": "DAPORewardManager.__call__", "overview": "DAPO's reward manager, responsible for computing task rewards and applying overlong penalty. This is one of the key components distinguishing DAPO from other methods, preventing the model from generating meaningless lengthy outputs through a linear penalty mechanism.", "function_signature": "def __call__(self, data: DataProto, return_dict: bool = False):", "input_parameters": "- `data`: DataProto object containing:\n- `batch[\"prompts\"]`: Prompt token IDs with shape `(bs, prompt_len)`\n- `batch[\"responses\"]`: Response token IDs with shape `(bs, response_len)`\n- `batch[\"attention_mask\"]`: Attention mask with shape `(bs, total_len)`\n- `non_tensor_batch[\"reward_model\"][\"ground_truth\"]`: Ground truth answers\n- `non_tensor_batch[\"data_source\"]`: Data source (e.g., \"gsm8k\", \"math\")\n- `return_dict`: Boolean, whether to return dictionary format (including additional information like accuracy, overlong penalty, etc.)", "detailed_description": "The function is responsible for computing reward scores for generated responses. First checks if there are pre-computed rewards; if so, returns directly; otherwise processes each response individually, decoding token sequences into text and computing task rewards based on ground truth answers (needs to call related functional functions). Computed scores are placed at the last valid token position in the response sequence. If length control is enabled, the function applies additional negative reward penalty for overlong responses (linear penalty mechanism). Throughout the process, detailed information for some samples is output for debugging verification, ultimately returning complete reward tensor or dictionary containing additional evaluation metrics.", "output": "- If `return_dict=False`: Returns sparse reward tensor with shape `(batch_size, response_length)` (only last valid token position is non-zero)\n- If `return_dict=True`: Returns dictionary containing:\n- `\"reward_tensor\"`: Reward tensor as above\n- `\"reward_extra_info\"`: Dictionary containing `\"acc\"` (accuracy list), `\"overlong_reward\"` (overlong penalty list), `\"overlong\"` (boolean list indicating if overlong), etc.", "func_implementation": "", "test_code_path": "code/test_code/test_dapo_reward_manager.py", "implementation_location": "code/verl/workers/reward_manager/dapo.py:line 53-149", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `DAPORewardManager.__call__`.\n\n**Function Overview:**\nDAPO's reward manager, responsible for computing task rewards and applying overlong penalty. This is one of the key components distinguishing DAPO from other methods, preventing the model from generating meaningless lengthy outputs through a linear penalty mechanism.\n\n**Function Signature:**\n```python\ndef __call__(self, data: DataProto, return_dict: bool = False):\n```\n\n**Input Parameters:**\n- `data`: DataProto object containing:\n- `batch[\"prompts\"]`: Prompt token IDs with shape `(bs, prompt_len)`\n- `batch[\"responses\"]`: Response token IDs with shape `(bs, response_len)`\n- `batch[\"attention_mask\"]`: Attention mask with shape `(bs, total_len)`\n- `non_tensor_batch[\"reward_model\"][\"ground_truth\"]`: Ground truth answers\n- `non_tensor_batch[\"data_source\"]`: Data source (e.g., \"gsm8k\", \"math\")\n- `return_dict`: Boolean, whether to return dictionary format (including additional information like accuracy, overlong penalty, etc.)\n\n**Detailed Description:**\nThe function is responsible for computing reward scores for generated responses. First checks if there are pre-computed rewards; if so, returns directly; otherwise processes each response individually, decoding token sequences into text and computing task rewards based on ground truth answers (needs to call related functional functions). Computed scores are placed at the last valid token position in the response sequence. If length control is enabled, the function applies additional negative reward penalty for overlong responses (linear penalty mechanism). Throughout the process, detailed information for some samples is output for debugging verification, ultimately returning complete reward tensor or dictionary containing additional evaluation metrics.\n\n**Expected Output:**\n- If `return_dict=False`: Returns sparse reward tensor with shape `(batch_size, response_length)` (only last valid token position is non-zero)\n- If `return_dict=True`: Returns dictionary containing:\n- `\"reward_tensor\"`: Reward tensor as above\n- `\"reward_extra_info\"`: Dictionary containing `\"acc\"` (accuracy list), `\"overlong_reward\"` (overlong penalty list), `\"overlong\"` (boolean list indicating if overlong), etc.\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def __call__(self, data: DataProto, return_dict: bool = False):\n    # Local imports to avoid global dependency issues\n    import torch\n\n    # Access batch data\n    batch = data.batch\n    non_tensor_batch = getattr(data, \"non_tensor_batch\", None)\n\n    # 1) Check for pre-computed rewards; return early if present\n    reward_tensor = None\n    if isinstance(batch, dict):\n        reward_tensor = batch.get(\"reward_tensor\", None)\n    else:\n        reward_tensor = getattr(batch, \"reward_tensor\", None)\n\n    if reward_tensor is not None:\n        if return_dict:\n            return {\"reward_tensor\": reward_tensor, \"reward_extra_info\": {\"acc\": [], \"overlong_reward\": [], \"overlong\": []}}\n        else:\n            return reward_tensor\n\n    # 2) Gather required inputs\n    if isinstance(batch, dict):\n        prompts = batch.get(\"prompts\", None)\n        responses = batch.get(\"responses\", None)\n        attention_mask = batch.get(\"attention_mask\", None)\n    else:\n        prompts = getattr(batch, \"prompts\", None)\n        responses = getattr(batch, \"responses\", None)\n        attention_mask = getattr(batch, \"attention_mask\", None)\n\n    if prompts is None or responses is None:\n        raise ValueError(\"DAPORewardManager: batch is missing prompts or responses.\")\n\n    bs = prompts.shape[0] if hasattr(prompts, \"shape\") else len(prompts)\n    resp_len = responses.shape[1] if hasattr(responses, \"shape\") else len(responses[0])\n\n    # Ground truth\n    ground_truth_list = []\n    if non_tensor_batch is not None:\n        if isinstance(non_tensor_batch, dict):\n            reward_model = non_tensor_batch.get(\"reward_model\", {})\n            gt = reward_model.get(\"ground_truth\", None)\n        else:\n            reward_model = getattr(non_tensor_batch, \"reward_model\", {})\n            gt = getattr(reward_model, \"ground_truth\", None)\n    else:\n        gt = None\n\n    if isinstance(gt, (list, tuple)):\n        ground_truth_list = list(gt)\n    else:\n        ground_truth_list = [gt] * bs if gt is not None else [None] * bs\n\n    data_source = None\n    if non_tensor_batch is not None:\n        if isinstance(non_tensor_batch, dict):\n            data_source = non_tensor_batch.get(\"data_source\", None)\n        else:\n            data_source = getattr(non_tensor_batch, \"data_source\", None)\n\n    # 3) Decoding helper (best-effort)\n    def _decode_ids(ids):\n        try:\n            if isinstance(ids, torch.Tensor):\n                ids = ids.tolist()\n            if isinstance(ids, (list, tuple)):\n                if hasattr(self, \"tokenizer\") and hasattr(self.tokenizer, \"decode\"):\n                    return self.tokenizer.decode(ids, skip_special_tokens=True)\n                elif hasattr(self, \"decode_fn\"):\n                    return self.decode_fn(ids)\n        except Exception:\n            pass\n        return \"\"\n\n    # 4) Length control and penalties\n    max_eval_len = getattr(self, \"max_eval_len\", None)\n    overlong_penalty = getattr(self, \"overlong_penalty\", 0.0)\n\n    device = None\n    if isinstance(prompts, torch.Tensor):\n        device = prompts.device\n    reward_tensor = torch.zeros((bs, resp_len), dtype=torch.float32, device=device) if device is not None else torch.zeros((bs, resp_len), dtype=torch.float32)\n\n    acc_list = []\n    overlong_list = []\n    overlong_penalty_list = []\n    decoded_responses = []\n\n    for i in range(bs):\n        resp_ids_i = responses[i]\n        text_i = _decode_ids(resp_ids_i)\n        decoded_responses.append(text_i)\n\n        gt_i = ground_truth_list[i] if i < len(ground_truth_list) else None\n        acc_i = 0.0\n        if gt_i is not None and isinstance(text_i, str) and len(text_i.strip()) > 0:\n            if text_i.strip().lower() == str(gt_i).strip().lower():\n                acc_i = 1.0\n        acc_list.append(acc_i)\n\n        over_tokens = 0\n        if max_eval_len is not None:\n            over_tokens = max(0, resp_len - max_eval_len)\n        overlong = over_tokens > 0\n        overlong_list.append(overlong)\n\n        penalty = -over_tokens * overlong_penalty if overlong else 0.0\n        overlong_penalty_list.append(penalty)\n\n        total = acc_i + penalty\n        reward_tensor[i, resp_len - 1] = total  # place reward at last valid token position\n\n    reward_extra_info = {\n        \"acc\": acc_list,\n        \"overlong_reward\": overlong_penalty_list,\n        \"overlong\": overlong_list,\n        \"ground_truth\": ground_truth_list,\n        \"decoded_responses\": decoded_responses,\n        \"data_source\": data_source,\n    }\n    # total per-sample rewards for debugging\n    reward_extra_info[\"total_reward\"] = [float(reward_tensor[i, resp_len - 1].item()) for i in range(bs)]\n\n    if return_dict:\n        return {\"reward_tensor\": reward_tensor, \"reward_extra_info\": reward_extra_info}\n    else:\n        return reward_tensor"], "usage": {"prompt_tokens": 638, "completion_tokens": 6690, "total_tokens": 7328, "per_completion": [{"prompt_tokens": 638, "completion_tokens": 6690, "total_tokens": 7328}]}, "results": [false], "pass_ratios": [0.0]}
