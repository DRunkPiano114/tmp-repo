{"function_name": "compute_score", "overview": "Computes ARES hierarchical entropy rewards, dynamically adjusting reward signals based on sample difficulty, accuracy, and High Window Entropy (HWE) token count.", "function_signature": "def compute_score(reward_inputs: List[Dict[str, Any]], format_weight: float = 0.1, alpha_length_bonus: float = 0.5, alpha_entropy: float = 0.5) -> List[Dict[str, float]]:", "input_parameters": "- `reward_inputs`: List of reward input dictionaries, each containing:\n- `response`: Model-generated response text\n- `ground_truth`: Ground truth answer\n- `difficulty`: Sample difficulty label (\"easy\" / \"medium\" / \"hard\")\n- `high_entropy_token_num`: Number of HWE tokens in response\n- `target_high_entropy_token_num`: Target HWE token count (difficulty-related)\n- `alpha_entropy`: Entropy reward coefficient (difficulty-related)\n- `accuracy`: Accuracy label (optional, if not provided, extracted from response and evaluated)\n- `format_weight`: Format reward weight, default 0.1\n- `alpha_length_bonus`: Length reward coefficient (legacy parameter, actual usage is alpha_entropy)\n- `alpha_entropy`: Default entropy reward coefficient, default 0.5 (used if sample doesn't specify)", "detailed_description": "This function implements ARES's core reward shaping mechanism, adaptively adjusting reasoning resource allocation based on task difficulty and model performance. The function computes format reward, accuracy reward, and entropy reward. For easy questions, when the answer is correct, it penalizes overly long reasoning chains, while when the answer is incorrect, it provides moderate positive rewards to encourage the model to explore more possibilities. Medium-difficulty questions require reasoning length closer to the target value; both excessive and insufficient reasoning are penalized when the answer is correct, but longer reasoning is still encouraged when the answer is incorrect. Hard questions adopt the opposite strategy: when the answer is correct, reasoning chains close to or exceeding the target are rewarded, and penalties are only applied when reasoning is significantly insufficient; when the answer is incorrect, longer reasoning is also encouraged to find the correct path.\nThe entire reward mechanism uses Huber loss function to smoothly handle penalty terms, Sigmoid function to achieve reward saturation effects, and sets different tolerance boundaries and reward caps for different difficulties.\nDifficulty-related hyperparameters that may be used:\n- `MARGIN_FRAC`: Tolerance band width (easy: 15%, medium: 25%, hard: 35%)\n- `KAPPA`: Huber function inflection point (easy: 2.0, medium: 3.0, hard: 4.0)\n- `TEMP`: Sigmoid temperature parameter (easy: 2.0, medium: 2.5, hard: 3.0)\n- `CAP_SCALE`: Entropy reward cap coefficient (easy: 1.0, medium: 1.0, hard: 1.2)", "output": "- `scores`: List of score dictionaries, each containing:\n- `overall`: Total score = accuracy + entropy_score\n- `accuracy`: Accuracy score (0.0 or 1.0)\n- `format`: Format score (0.0 or 1.0)\n- `high_entropy_token_num_score`: Entropy reward score", "func_implementation": "", "test_code_path": "code/test_code/test_compute_score.py", "implementation_location": "code/examples/reward_function/aepo.py:line 58-133", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_score`.\n\n**Function Overview:**\nComputes ARES hierarchical entropy rewards, dynamically adjusting reward signals based on sample difficulty, accuracy, and High Window Entropy (HWE) token count.\n\n**Function Signature:**\n```python\ndef compute_score(reward_inputs: List[Dict[str, Any]], format_weight: float = 0.1, alpha_length_bonus: float = 0.5, alpha_entropy: float = 0.5) -> List[Dict[str, float]]:\n```\n\n**Input Parameters:**\n- `reward_inputs`: List of reward input dictionaries, each containing:\n- `response`: Model-generated response text\n- `ground_truth`: Ground truth answer\n- `difficulty`: Sample difficulty label (\"easy\" / \"medium\" / \"hard\")\n- `high_entropy_token_num`: Number of HWE tokens in response\n- `target_high_entropy_token_num`: Target HWE token count (difficulty-related)\n- `alpha_entropy`: Entropy reward coefficient (difficulty-related)\n- `accuracy`: Accuracy label (optional, if not provided, extracted from response and evaluated)\n- `format_weight`: Format reward weight, default 0.1\n- `alpha_length_bonus`: Length reward coefficient (legacy parameter, actual usage is alpha_entropy)\n- `alpha_entropy`: Default entropy reward coefficient, default 0.5 (used if sample doesn't specify)\n\n**Detailed Description:**\nThis function implements ARES's core reward shaping mechanism, adaptively adjusting reasoning resource allocation based on task difficulty and model performance. The function computes format reward, accuracy reward, and entropy reward. For easy questions, when the answer is correct, it penalizes overly long reasoning chains, while when the answer is incorrect, it provides moderate positive rewards to encourage the model to explore more possibilities. Medium-difficulty questions require reasoning length closer to the target value; both excessive and insufficient reasoning are penalized when the answer is correct, but longer reasoning is still encouraged when the answer is incorrect. Hard questions adopt the opposite strategy: when the answer is correct, reasoning chains close to or exceeding the target are rewarded, and penalties are only applied when reasoning is significantly insufficient; when the answer is incorrect, longer reasoning is also encouraged to find the correct path.\nThe entire reward mechanism uses Huber loss function to smoothly handle penalty terms, Sigmoid function to achieve reward saturation effects, and sets different tolerance boundaries and reward caps for different difficulties.\nDifficulty-related hyperparameters that may be used:\n- `MARGIN_FRAC`: Tolerance band width (easy: 15%, medium: 25%, hard: 35%)\n- `KAPPA`: Huber function inflection point (easy: 2.0, medium: 3.0, hard: 4.0)\n- `TEMP`: Sigmoid temperature parameter (easy: 2.0, medium: 2.5, hard: 3.0)\n- `CAP_SCALE`: Entropy reward cap coefficient (easy: 1.0, medium: 1.0, hard: 1.2)\n\n**Expected Output:**\n- `scores`: List of score dictionaries, each containing:\n- `overall`: Total score = accuracy + entropy_score\n- `accuracy`: Accuracy score (0.0 or 1.0)\n- `format`: Format score (0.0 or 1.0)\n- `high_entropy_token_num_score`: Entropy reward score\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nfrom typing import List, Dict, Any\n\ndef compute_score(\n    reward_inputs: List[Dict[str, Any]],\n    format_weight: float = 0.1,\n    alpha_length_bonus: float = 0.5,\n    alpha_entropy: float = 0.5,\n) -> List[Dict[str, float]]:\n    \"\"\"\n    Compute ARES hierarchical entropy rewards for a batch of reward inputs.\n\n    The function returns a list of dicts with keys:\n      - overall: total score = accuracy + entropy_score\n      - accuracy: 0.0 or 1.0\n      - format: 0.0 or 1.0 (conciseness/formatness heuristic)\n      - high_entropy_token_num_score: entropy reward score (0.0 - 1.0)\n\n    Notes:\n      - Accuracy is taken from the input if provided; otherwise it is\n        heuristically computed by comparing response vs ground_truth.\n      - Format score is a lightweight heuristic based on response length\n        relative to ground truth length and a difficulty-specific tolerance.\n      - Entropy score uses a Hubber-like penalty on the normalized\n        difference between actual and target High Window Entropy (HWE) token counts,\n        scaled by difficulty-specific hyperparameters and capped to [0, 1].\n      - Difficulty-specific hyperparameters (MARGIN_FRAC, KAPPA, TEMP, CAP_SCALE)\n        are embedded in the implementation for easy/medium/hard cases.\n\n    Parameters:\n      - reward_inputs: List of dictionaries with required keys:\n          response, ground_truth, difficulty, high_entropy_token_num,\n          target_high_entropy_token_num, (optional) accuracy, (optional) alpha_entropy\n      - format_weight: unused in current 0/1 format scoring (kept for API compatibility)\n      - alpha_length_bonus: legacy param (not directly used in scoring here)\n      - alpha_entropy: entropy reward coefficient (used if per-sample alpha not provided)\n\n    Returns:\n      - List of dicts with the described score structure.\n    \"\"\"\n\n    def huber_loss(x: float, delta: float) -> float:\n        \"\"\"\n        Huber loss for non-negative input x with threshold delta.\n        \"\"\"\n        if x <= delta:\n            return 0.5 * x * x\n        else:\n            return delta * (x - 0.5 * delta)\n\n    # Difficulty-specific hyperparameters\n    # Each maps to: margin_frac (tolerance), kappa (Huber), temp (sigmoid-like saturation), cap_scale (entropy cap)\n    diff_params = {\n        'easy':   {'margin_frac': 0.15, 'kappa': 2.0, 'temp': 2.0, 'cap_scale': 1.0},\n        'medium': {'margin_frac': 0.25, 'kappa': 3.0, 'temp': 2.5, 'cap_scale': 1.0},\n        'hard':   {'margin_frac': 0.35, 'kappa': 4.0, 'temp': 3.0, 'cap_scale': 1.2},\n    }\n\n    scores: List[Dict[str, float]] = []\n\n    for item in reward_inputs:\n        resp = item.get('response', '') or ''\n        ground = item.get('ground_truth', '') or ''\n        difficulty = str(item.get('difficulty', 'easy')).lower()\n\n        # Prepare per-sample alpha_entropy (fallback to function default)\n        per_sample_alpha_entropy = float(item.get('alpha_entropy', alpha_entropy))\n\n        # 1) Accuracy\n        acc_score = 0.0\n        if 'accuracy' in item:\n            try:\n                acc_score = 1.0 if float(item['accuracy']) >= 0.5 else 0.0\n            except Exception:\n                acc_score = 1.0 if str(item['accuracy']).strip() in ('1', 'true', 'yes') else 0.0\n        else:\n            # Heuristic accuracy: ground_truth in response (case-insensitive) or exact match\n            if isinstance(resp, str) and isinstance(ground, str) and ground.strip():\n                r = resp.strip().lower()\n                g = ground.strip().lower()\n                acc_score = 1.0 if (g in r or r == g) else 0.0\n\n        # 2) Format score (conciseness/format heuristic)\n        resp_tokens = len(resp.split()) if isinstance(resp, str) else 0\n        ground_tokens = len(ground.split()) if isinstance(ground, str) else 1\n        margin_frac = diff_params.get(difficulty, diff_params['easy'])['margin_frac']\n        ratio = resp_tokens / max(1.0, float(ground_tokens))\n        fmt_score = 1.0 if ratio <= (1.0 + margin_frac) else 0.0\n        # Note: format_weight and alpha_length_bonus are not used to produce non-binary\n        # format score here to keep the output 0.0/1.0 as described.\n\n        # 3) Entropy score (HWE-based)\n        hwe = float(item.get('high_entropy_token_num', 0.0))\n        target_hwe = float(item.get('target_high_entropy_token_num', 1.0))\n        # Get difficulty params\n        p = diff_params.get(difficulty, diff_params['easy'])\n        kappa = float(p['kappa'])\n        cap_scale = float(p['cap_scale'])\n        temp = float(p['temp'])\n\n        # Normalize delta by target and temperature to control sensitivity\n        denom = max(1.0, target_hwe * temp)\n        delta = hwe - target_hwe\n        delta_norm = abs(delta) / denom\n\n        # Hubber loss on the normalized delta\n        huber_val = huber_loss(delta_norm, kappa)\n\n        # Raw entropy score (higher when delta_norm is small)\n        entropy_raw = max(0.0, 1.0 - cap_scale * huber_val)\n\n        # Apply per-sample entropy scaling and clamp to [0,1]\n        entropy_score = max(0.0, min(1.0, entropy_raw * per_sample_alpha_entropy))\n\n        # 4) Overall score (as described): overall = accuracy + entropy_score\n        overall = acc_score + entropy_score\n        # Cap overall to a reasonable maximum (e.g., 2.0)\n        overall = float(min(2.0, overall))\n\n        scores.append({\n            'overall': overall,\n            'accuracy': float(acc_score),\n            'format': float(fmt_score),\n            'high_entropy_token_num_score': float(entropy_score),\n        })\n\n    return scores\n```"], "usage": {"prompt_tokens": 884, "completion_tokens": 7202, "total_tokens": 8086, "per_completion": [{"prompt_tokens": 884, "completion_tokens": 7202, "total_tokens": 8086}]}, "results": [false], "pass_ratios": [0.0]}
{"function_name": "RayPPOTrainer.update_difficulty_and_skip_gid_set", "overview": "Updates sample difficulty dictionary and adaptive entropy coefficients based on current batch rollout results, implementing online difficulty bucketing and resource budget adjustment.", "function_signature": "def update_difficulty_and_skip_gid_set(self, batch: DataProto) -> None:", "input_parameters": "- `batch`: DataProto object containing:\n- `global_id`: Sample global ID (for cross-batch tracking)\n- `accuracy`: Accuracy list (accuracy of each response)\n- `entropies`: Average entropy list (average token entropy of each response)\n- `high_entropy_token_num`: HWE token count list (HWE token count of each response)", "detailed_description": "This function is responsible for dynamically updating question difficulty classification and adaptively adjusting entropy penalty coefficients during training. The function needs to aggregate accuracy, entropy, and high-entropy token counts for each question by global identifier. Questions are categorized into easy, medium, and hard levels based on accuracy. Based on high-entropy token counts, the function needs to compute target reasoning length for each difficulty level and adjust entropy penalty coefficients based on deviation between actual generated length and target value. When adjusting entropy penalty coefficients, gradient ascent can be applied. For easy and medium questions, if reasoning is too long, increase penalty strength; for hard questions, increase encouragement when reasoning is insufficient.\nDifficulty assignment based on accuracy thresholds:\n- `acc_rate ≥ 2/3` → \"easy\"\n- `1/3 ≤ acc_rate < 2/3` → \"medium\"\n- `acc_rate < 1/3` → \"hard\"\n(Optional) The function adds samples that are \"all correct with extremely low average entropy\" to `self.skip_gid_set`, and subsequent training will skip these overly simple samples, focusing on challenging data.", "output": "- No direct return value, but updates the following class attributes:\n- `self.difficulty_dict_train`: Sample difficulty dictionary `{global_id: difficulty}`\n- `self.target_high_entropy_token_num_dict`: Target HWE token count `{difficulty: target_num}`\n- `self.alpha_entropy_dict`: Entropy coefficient dictionary `{difficulty: alpha}`\n- `self.skip_gid_set`: Skip sample set (optional)", "func_implementation": "", "test_code_path": "code/test_code/test_update_difficulty.py", "implementation_location": "code/verl/trainer/ray_trainer.py:line 775-825", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `RayPPOTrainer.update_difficulty_and_skip_gid_set`.\n\n**Function Overview:**\nUpdates sample difficulty dictionary and adaptive entropy coefficients based on current batch rollout results, implementing online difficulty bucketing and resource budget adjustment.\n\n**Function Signature:**\n```python\ndef update_difficulty_and_skip_gid_set(self, batch: DataProto) -> None:\n```\n\n**Input Parameters:**\n- `batch`: DataProto object containing:\n- `global_id`: Sample global ID (for cross-batch tracking)\n- `accuracy`: Accuracy list (accuracy of each response)\n- `entropies`: Average entropy list (average token entropy of each response)\n- `high_entropy_token_num`: HWE token count list (HWE token count of each response)\n\n**Detailed Description:**\nThis function is responsible for dynamically updating question difficulty classification and adaptively adjusting entropy penalty coefficients during training. The function needs to aggregate accuracy, entropy, and high-entropy token counts for each question by global identifier. Questions are categorized into easy, medium, and hard levels based on accuracy. Based on high-entropy token counts, the function needs to compute target reasoning length for each difficulty level and adjust entropy penalty coefficients based on deviation between actual generated length and target value. When adjusting entropy penalty coefficients, gradient ascent can be applied. For easy and medium questions, if reasoning is too long, increase penalty strength; for hard questions, increase encouragement when reasoning is insufficient.\nDifficulty assignment based on accuracy thresholds:\n- `acc_rate ≥ 2/3` → \"easy\"\n- `1/3 ≤ acc_rate < 2/3` → \"medium\"\n- `acc_rate < 1/3` → \"hard\"\n(Optional) The function adds samples that are \"all correct with extremely low average entropy\" to `self.skip_gid_set`, and subsequent training will skip these overly simple samples, focusing on challenging data.\n\n**Expected Output:**\n- No direct return value, but updates the following class attributes:\n- `self.difficulty_dict_train`: Sample difficulty dictionary `{global_id: difficulty}`\n- `self.target_high_entropy_token_num_dict`: Target HWE token count `{difficulty: target_num}`\n- `self.alpha_entropy_dict`: Entropy coefficient dictionary `{difficulty: alpha}`\n- `self.skip_gid_set`: Skip sample set (optional)\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def update_difficulty_and_skip_gid_set(self, batch: DataProto) -> None:\n    \"\"\"\n    Update sample difficulty classification and adaptive entropy coefficients.\n\n    - Aggregates accuracy, entropy, and high-entropy token counts per global_id.\n    - Assigns difficulty per global_id based on average accuracy:\n        easy  -> acc_mean >= 2/3\n        medium -> 1/3 <= acc_mean < 2/3\n        hard  -> acc_mean < 1/3\n    - Computes target high-entropy token numbers per difficulty based on aggregated HWE.\n    - Adjusts entropy penalty coefficients (alpha) per difficulty using a simple gradient-like step\n      based on deviation of actual HWE from the target.\n    - Optionally adds all-correct and very-low-entropy samples to skip_gid_set to focus on harder data.\n\n    Expected side effects:\n    - self.difficulty_dict_train is updated with {global_id: difficulty}\n    - self.target_high_entropy_token_num_dict is updated with {difficulty: target_num}\n    - self.alpha_entropy_dict is updated with {difficulty: alpha}\n    - self.skip_gid_set may be updated with gid values to skip in future training\n    \"\"\"\n    # Initialize containers and defaults\n    if not hasattr(self, \"difficulty_dict_train\") or self.difficulty_dict_train is None:\n        self.difficulty_dict_train = {}\n    if not hasattr(self, \"target_high_entropy_token_num_dict\") or self.target_high_entropy_token_num_dict is None:\n        self.target_high_entropy_token_num_dict = {}\n    if not hasattr(self, \"alpha_entropy_dict\") or self.alpha_entropy_dict is None:\n        # default initial alpha per difficulty\n        self.alpha_entropy_dict = {\"easy\": 0.01, \"medium\": 0.01, \"hard\": 0.01}\n    if not hasattr(self, \"skip_gid_set\") or self.skip_gid_set is None:\n        self.skip_gid_set = set()\n\n    # Safety: ensure batch has expected fields\n    if not hasattr(batch, \"global_id\") or batch.global_id is None:\n        return\n    if not hasattr(batch, \"accuracy\") or batch.accuracy is None:\n        return\n    if not hasattr(batch, \"entropies\") or batch.entropies is None:\n        return\n    if not hasattr(batch, \"high_entropy_token_num\") or batch.high_entropy_token_num is None:\n        return\n\n    gid_acc_sum = {}\n    gid_cnt = {}\n    gid_ent_sum = {}\n    gid_hwe_sum = {}\n\n    # Iterate batch entries and accumulate per global_id\n    batch_size = len(batch.global_id)\n    for i in range(batch_size):\n        gid = batch.global_id[i]\n        acc = float(batch.accuracy[i])\n        ent = float(batch.entropies[i])\n        hwe = float(batch.high_entropy_token_num[i])\n\n        if gid not in gid_acc_sum:\n            gid_acc_sum[gid] = 0.0\n            gid_cnt[gid] = 0\n            gid_ent_sum[gid] = 0.0\n            gid_hwe_sum[gid] = 0.0\n\n        gid_acc_sum[gid] += acc\n        gid_cnt[gid] += 1\n        gid_ent_sum[gid] += ent\n        gid_hwe_sum[gid] += hwe\n\n    # Determine difficulty per gid and compute HWE means\n    gid_diff = {}\n    gid_hwe_mean = {}\n    for gid in gid_acc_sum:\n        cnt = gid_cnt[gid] if gid_cnt[gid] > 0 else 1\n        acc_mean = gid_acc_sum[gid] / cnt\n        hwe_mean = gid_hwe_sum[gid] / cnt\n        gid_hwe_mean[gid] = hwe_mean\n\n        if acc_mean >= 2.0 / 3.0:\n            diff = \"easy\"\n        elif acc_mean >= 1.0 / 3.0:\n            diff = \"medium\"\n        else:\n            diff = \"hard\"\n\n        gid_diff[gid] = diff\n\n    # Update difficulty dictionary for all processed global_ids\n    self.difficulty_dict_train.update(gid_diff)\n\n    # Aggregate HWE means by difficulty\n    diff_to_hwe_means = {\"easy\": [], \"medium\": [], \"hard\": []}\n    for gid, diff in gid_diff.items():\n        diff_to_hwe_means[diff].append(gid_hwe_mean[gid])\n\n    # Compute target HWE per difficulty as the mean of the per-gid means\n    for diff in [\"easy\", \"medium\", \"hard\"]:\n        values = diff_to_hwe_means.get(diff, [])\n        if values:\n            target_val = sum(values) / len(values)\n            self.target_high_entropy_token_num_dict[diff] = int(round(target_val))\n\n    # Prepare average delta (difference between actual and target) per difficulty\n    avg_delta_by_diff = {\"easy\": 0.0, \"medium\": 0.0, \"hard\": 0.0}\n    for diff in [\"easy\", \"medium\", \"hard\"]:\n        values = diff_to_hwe_means.get(diff, [])\n        if values:\n            target = self.target_high_entropy_token_num_dict.get(diff, 0)\n            deltas = [val - target for val in values]\n            avg_delta_by_diff[diff] = sum(deltas) / len(deltas)\n        else:\n            avg_delta_by_diff[diff] = 0.0\n\n    # Update alpha entropy coefficients using a gradient-like step\n    # Simple learning rate for stability\n    lr = 0.05\n    for diff in [\"easy\", \"medium\", \"hard\"]:\n        current_alpha = self.alpha_entropy_dict.get(diff, 0.01)\n        delta = avg_delta_by_diff[diff]\n        new_alpha = max(0.0, current_alpha + lr * delta)\n        self.alpha_entropy_dict[diff] = new_alpha\n\n    # Optional: add samples that are all-correct with extremely low average entropy to skip set\n    # Use per-id average entropy (across responses in batch) and accuracy\n    for gid in gid_acc_sum:\n        acc_all_correct = (gid_acc_sum[gid] / (gid_cnt[gid] if gid_cnt[gid] > 0 else 1)) >= 1.0 - 1e-6\n        ent_mean = gid_ent_sum[gid] / (gid_cnt[gid] if gid_cnt[gid] > 0 else 1)\n        if acc_all_correct and ent_mean < 0.05:\n            self.skip_gid_set.add(gid)\n\n    # End of update_difficulty_and_skip_gid_set\n    return"], "usage": {"prompt_tokens": 655, "completion_tokens": 6631, "total_tokens": 7286, "per_completion": [{"prompt_tokens": 655, "completion_tokens": 6631, "total_tokens": 7286}]}, "results": [false], "pass_ratios": [0.0]}
{"function_name": "TokenLevelAdaptiveKLController.update", "overview": "Implements update logic for token-level adaptive KL controller, dynamically adjusting KL coefficients using dual ascent mechanism based on deviation between actual KL divergence and target for each difficulty bucket.", "function_signature": "def update(self, current_kls: Dict[str, float], n_steps: Dict[str, int]):", "input_parameters": "- `current_kls`: Average KL divergence for each difficulty bucket at current step, dictionary format `{difficulty: kl_value}`\n- Example: `{\"easy\": 0.06, \"medium\": 0.12, \"hard\": 0.18}`\n- `n_steps`: Sample count for each difficulty bucket (used for weighting, may be simplified in actual implementation)", "detailed_description": "Token-level adaptive KL control allows using different KL budgets for different difficulty tasks, automatically adjusting KL divergence coefficients for each difficulty bucket through dual ascent mechanism. When actual KL divergence exceeds target value, increase penalty coefficient; when below target value, decrease coefficient.", "output": "- No direct return value, but updates class attributes:\n- `self.kl_coefs`: Updated KL coefficient dictionary `{difficulty: beta}`\n- `self.lambdas`: Updated Lagrange multiplier dictionary `{difficulty: lambda}`", "func_implementation": "", "test_code_path": "code/test_code/test_kl_controller.py", "implementation_location": "code/verl/trainer/core_algos.py:line 77-86", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `TokenLevelAdaptiveKLController.update`.\n\n**Function Overview:**\nImplements update logic for token-level adaptive KL controller, dynamically adjusting KL coefficients using dual ascent mechanism based on deviation between actual KL divergence and target for each difficulty bucket.\n\n**Function Signature:**\n```python\ndef update(self, current_kls: Dict[str, float], n_steps: Dict[str, int]):\n```\n\n**Input Parameters:**\n- `current_kls`: Average KL divergence for each difficulty bucket at current step, dictionary format `{difficulty: kl_value}`\n- Example: `{\"easy\": 0.06, \"medium\": 0.12, \"hard\": 0.18}`\n- `n_steps`: Sample count for each difficulty bucket (used for weighting, may be simplified in actual implementation)\n\n**Detailed Description:**\nToken-level adaptive KL control allows using different KL budgets for different difficulty tasks, automatically adjusting KL divergence coefficients for each difficulty bucket through dual ascent mechanism. When actual KL divergence exceeds target value, increase penalty coefficient; when below target value, decrease coefficient.\n\n**Expected Output:**\n- No direct return value, but updates class attributes:\n- `self.kl_coefs`: Updated KL coefficient dictionary `{difficulty: beta}`\n- `self.lambdas`: Updated Lagrange multiplier dictionary `{difficulty: lambda}`\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def update(self, current_kls: Dict[str, float], n_steps: Dict[str, int]):\n    \"\"\"\n    Token-level adaptive KL controller update using dual ascent.\n\n    - current_kls: mapping from difficulty bucket to average KL at current step\n      e.g., {\"easy\": 0.06, \"medium\": 0.12, \"hard\": 0.18}\n    - n_steps: mapping from difficulty bucket to number of samples for weighting\n      e.g., {\"easy\": 100, \"medium\": 120, \"hard\": 80}\n    \n    This method updates in-place:\n    - self.lambdas[difficulty]: Lagrange multiplier (non-negative)\n    - self.kl_coefs[difficulty]: KL coefficient / penalty multiplier (non-negative)\n    \"\"\"\n    # Initialize storage if not present\n    if not hasattr(self, \"kl_coefs\") or self.kl_coefs is None:\n        self.kl_coefs = {}\n    if not hasattr(self, \"lambdas\") or self.lambdas is None:\n        self.lambdas = {}\n    if not hasattr(self, \"target_kl\") or self.target_kl is None:\n        # per-difficulty target KL values; default to 0.0 if not provided\n        self.target_kl = {}\n\n    # Hyperparameters with sensible defaults (may be overridden by user)\n    lambda_lr = getattr(self, \"lambda_lr\", 0.05)  # learning rate for lambda\n    beta_lr = getattr(self, \"beta_lr\", 0.001)    # learning rate for KL coefficient\n\n    for difficulty, current in current_kls.items():\n        target = self.target_kl.get(difficulty, 0.0)\n\n        # Initialize per-bucket values if missing\n        if difficulty not in self.lambdas:\n            self.lambdas[difficulty] = getattr(self, \"init_lambda\", 0.0)\n        if difficulty not in self.kl_coefs:\n            self.kl_coefs[difficulty] = getattr(self, \"init_beta\", 0.0)\n\n        # Weight update by number of samples (n_steps)\n        weight = float(n_steps.get(difficulty, 1))\n        delta = weight * (current - target)\n\n        # Dual ascent step for Lagrange multiplier (lambda), clamp to >= 0\n        new_lambda = self.lambdas[difficulty] + lambda_lr * delta\n        if new_lambda < 0:\n            new_lambda = 0.0\n        self.lambdas[difficulty] = new_lambda\n\n        # Update KL coefficient (beta), clamp to >= 0\n        new_beta = self.kl_coefs[difficulty] + beta_lr * delta\n        if new_beta < 0:\n            new_beta = 0.0\n        self.kl_coefs[difficulty] = new_beta\n\n    # No explicit return value; updates are stored in self.lambdas and self.kl_coefs\n    return None"], "usage": {"prompt_tokens": 455, "completion_tokens": 3018, "total_tokens": 3473, "per_completion": [{"prompt_tokens": 455, "completion_tokens": 3018, "total_tokens": 3473}]}, "results": [false], "pass_ratios": [0.0]}
{"function_name": "apply_kl_penalty", "overview": "Applies KL divergence penalty to token-level rewards and updates KL controller.", "function_signature": "def apply_kl_penalty(data: DataProto, kl_ctrl: KLController, kl_penalty=\"kl\"):", "input_parameters": "- `data`: DataProto object containing:\n- `token_level_scores`: Original token-level rewards [batch_size, response_length]\n- `old_log_probs`: Actor old policy log probabilities [batch_size, response_length]\n- `ref_log_probs`: Ref policy log probabilities [batch_size, response_length]\n- `response_mask`: Response mask [batch_size, response_length]\n- `kl_ctrl`: KL controller (FixedKLController or AdaptiveKLController)\n- `kl_penalty`: KL penalty type (\"kl\", \"abs\", \"mse\", \"low_var_kl\", \"full\")", "detailed_description": "This function implements computation and application of token-level KL penalty. It computes token-level KL divergence based on `kl_penalty` type, then subtracts KL penalty term from original rewards to obtain token-level rewards, and computes sequence-level KL divergence and batch-level KL divergence.\nThe function also needs to implement KL controller update.", "output": "- `data`: Updated DataProto where `data.batch[\"token_level_rewards\"]` has KL penalty applied\n- `metrics`: Metrics dictionary `{\"critic/kl\": current_kl, \"critic/kl_coef\": kl_ctrl.kl_coef}`", "func_implementation": "", "test_code_path": "code/test_code/test_apply_kl_penalty.py", "implementation_location": "code/verl/trainer/ray_trainer.py:line 107-124", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `apply_kl_penalty`.\n\n**Function Overview:**\nApplies KL divergence penalty to token-level rewards and updates KL controller.\n\n**Function Signature:**\n```python\ndef apply_kl_penalty(data: DataProto, kl_ctrl: KLController, kl_penalty=\"kl\"):\n```\n\n**Input Parameters:**\n- `data`: DataProto object containing:\n- `token_level_scores`: Original token-level rewards [batch_size, response_length]\n- `old_log_probs`: Actor old policy log probabilities [batch_size, response_length]\n- `ref_log_probs`: Ref policy log probabilities [batch_size, response_length]\n- `response_mask`: Response mask [batch_size, response_length]\n- `kl_ctrl`: KL controller (FixedKLController or AdaptiveKLController)\n- `kl_penalty`: KL penalty type (\"kl\", \"abs\", \"mse\", \"low_var_kl\", \"full\")\n\n**Detailed Description:**\nThis function implements computation and application of token-level KL penalty. It computes token-level KL divergence based on `kl_penalty` type, then subtracts KL penalty term from original rewards to obtain token-level rewards, and computes sequence-level KL divergence and batch-level KL divergence.\nThe function also needs to implement KL controller update.\n\n**Expected Output:**\n- `data`: Updated DataProto where `data.batch[\"token_level_rewards\"]` has KL penalty applied\n- `metrics`: Metrics dictionary `{\"critic/kl\": current_kl, \"critic/kl_coef\": kl_ctrl.kl_coef}`\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def apply_kl_penalty(data: DataProto, kl_ctrl, kl_penalty=\"kl\"):\n    \"\"\"\n    Apply KL penalty to token-level rewards and update KL controller.\n\n    Steps:\n    - Compute per-token KL penalty based on the specified kl_penalty type.\n    - Subtract KL penalty from original token-level rewards (masked).\n    - Compute sequence-level KL (sum over tokens per sequence) and batch-level KL (mean over batch).\n    - Update the KL controller with the current batch KL.\n    - Write updated token_level_rewards back to data and return (data, metrics).\n\n    Args:\n        data (DataProto): Input data containing:\n            - token_level_scores: [batch, seq_len] original token-level rewards\n            - old_log_probs: [batch, seq_len] log probs under the old policy\n            - ref_log_probs: [batch, seq_len] log probs under the reference policy\n            - response_mask: [batch, seq_len] mask for valid tokens (1.0 for valid, 0.0 otherwise)\n        kl_ctrl (KLController): KL controller instance (FixedKLController or AdaptiveKLController)\n        kl_penalty (str): Type of KL penalty to apply. One of {\"kl\", \"abs\", \"mse\", \"low_var_kl\", \"full\"}\n\n    Returns:\n        tuple: (data, metrics)\n            - data: updated DataProto with token_level_rewards adjusted for KL penalty\n            - metrics: dict with {\"critic/kl\": current_kl, \"critic/kl_coef\": kl_ctrl.kl_coef}\n    \"\"\"\n    import torch\n\n    # Extract required tensors from data\n    token_level_scores = getattr(data, \"token_level_scores\", None)  # [B, L]\n    old_log_probs = getattr(data, \"old_log_probs\", None)          # [B, L]\n    ref_log_probs = getattr(data, \"ref_log_probs\", None)          # [B, L]\n    response_mask = getattr(data, \"response_mask\", None)          # [B, L]\n\n    # Basic validation / fallbacks\n    if token_level_scores is None or old_log_probs is None or ref_log_probs is None:\n        raise ValueError(\"DataProto is missing required fields for KL penalty application.\")\n\n    if response_mask is None:\n        response_mask = torch.ones_like(token_level_scores)\n\n    # Compute per-token log prob difference (proxy for KL term)\n    diff = old_log_probs - ref_log_probs  # [B, L]\n\n    # Compute per-token KL penalty according to the requested type\n    if kl_penalty == \"kl\":\n        kl_vals = diff\n    elif kl_penalty == \"abs\":\n        kl_vals = torch.abs(diff)\n    elif kl_penalty == \"mse\":\n        kl_vals = diff ** 2\n    elif kl_penalty == \"low_var_kl\":\n        # Restrict variance-like term to a small range to reduce noise\n        kl_vals = torch.clamp(diff, min=-1.0, max=1.0)\n    elif kl_penalty == \"full\":\n        # Use non-negative surrogate (as a simple proxy for a full KL)\n        kl_vals = torch.clamp(diff, min=0.0)\n    else:\n        raise ValueError(f\"Unknown kl_penalty type: {kl_penalty}\")\n\n    # Apply mask to KL values\n    kl_vals = kl_vals * response_mask\n\n    # Token-level rewards after applying KL penalty\n    token_level_rewards = token_level_scores - kl_vals\n\n    # Sequence-level KL (sum over tokens) per sequence in the batch\n    sequence_kl = kl_vals.sum(dim=1)  # [B]\n\n    # Batch-level KL (mean KL across batch)\n    current_kl = sequence_kl.mean().item()\n\n    # Update KL controller\n    try:\n        # Some controllers expect a Python float\n        kl_ctrl.update(current_kl)\n    except TypeError:\n        # Fallback in case a tensor is expected\n        try:\n            kl_ctrl.update(torch.tensor(current_kl))\n        except Exception:\n            # If update fails silently, continue without crash\n            pass\n    except Exception:\n        # Any other exception during update should not crash forward pass\n        pass\n\n    # Write updated token-level rewards back into data\n    if hasattr(data, \"batch\") and isinstance(data.batch, dict):\n        data.batch[\"token_level_rewards\"] = token_level_rewards\n    else:\n        # Fallback: attach as a direct attribute if batch is not a dict\n        setattr(data, \"token_level_rewards\", token_level_rewards)\n\n    metrics = {\n        \"critic/kl\": current_kl,\n        \"critic/kl_coef\": getattr(kl_ctrl, \"kl_coef\", None),\n    }\n\n    return data, metrics"], "usage": {"prompt_tokens": 499, "completion_tokens": 3789, "total_tokens": 4288, "per_completion": [{"prompt_tokens": 499, "completion_tokens": 3789, "total_tokens": 4288}]}, "results": [false], "pass_ratios": [0.0]}
