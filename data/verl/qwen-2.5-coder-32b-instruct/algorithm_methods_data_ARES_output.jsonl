{"function_name": "compute_score", "overview": "Computes ARES hierarchical entropy rewards, dynamically adjusting reward signals based on sample difficulty, accuracy, and High Window Entropy (HWE) token count.", "function_signature": "def compute_score(reward_inputs: List[Dict[str, Any]], format_weight: float = 0.1, alpha_length_bonus: float = 0.5, alpha_entropy: float = 0.5) -> List[Dict[str, float]]:", "input_parameters": "- `reward_inputs`: List of reward input dictionaries, each containing:\n- `response`: Model-generated response text\n- `ground_truth`: Ground truth answer\n- `difficulty`: Sample difficulty label (\"easy\" / \"medium\" / \"hard\")\n- `high_entropy_token_num`: Number of HWE tokens in response\n- `target_high_entropy_token_num`: Target HWE token count (difficulty-related)\n- `alpha_entropy`: Entropy reward coefficient (difficulty-related)\n- `accuracy`: Accuracy label (optional, if not provided, extracted from response and evaluated)\n- `format_weight`: Format reward weight, default 0.1\n- `alpha_length_bonus`: Length reward coefficient (legacy parameter, actual usage is alpha_entropy)\n- `alpha_entropy`: Default entropy reward coefficient, default 0.5 (used if sample doesn't specify)", "detailed_description": "This function implements ARES's core reward shaping mechanism, adaptively adjusting reasoning resource allocation based on task difficulty and model performance. The function computes format reward, accuracy reward, and entropy reward. For easy questions, when the answer is correct, it penalizes overly long reasoning chains, while when the answer is incorrect, it provides moderate positive rewards to encourage the model to explore more possibilities. Medium-difficulty questions require reasoning length closer to the target value; both excessive and insufficient reasoning are penalized when the answer is correct, but longer reasoning is still encouraged when the answer is incorrect. Hard questions adopt the opposite strategy: when the answer is correct, reasoning chains close to or exceeding the target are rewarded, and penalties are only applied when reasoning is significantly insufficient; when the answer is incorrect, longer reasoning is also encouraged to find the correct path.\nThe entire reward mechanism uses Huber loss function to smoothly handle penalty terms, Sigmoid function to achieve reward saturation effects, and sets different tolerance boundaries and reward caps for different difficulties.\nDifficulty-related hyperparameters that may be used:\n- `MARGIN_FRAC`: Tolerance band width (easy: 15%, medium: 25%, hard: 35%)\n- `KAPPA`: Huber function inflection point (easy: 2.0, medium: 3.0, hard: 4.0)\n- `TEMP`: Sigmoid temperature parameter (easy: 2.0, medium: 2.5, hard: 3.0)\n- `CAP_SCALE`: Entropy reward cap coefficient (easy: 1.0, medium: 1.0, hard: 1.2)", "output": "- `scores`: List of score dictionaries, each containing:\n- `overall`: Total score = accuracy + entropy_score\n- `accuracy`: Accuracy score (0.0 or 1.0)\n- `format`: Format score (0.0 or 1.0)\n- `high_entropy_token_num_score`: Entropy reward score", "func_implementation": "", "test_code_path": "code/test_code/test_compute_score.py", "implementation_location": "code/examples/reward_function/aepo.py:line 58-133", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_score`.\n\n**Function Overview:**\nComputes ARES hierarchical entropy rewards, dynamically adjusting reward signals based on sample difficulty, accuracy, and High Window Entropy (HWE) token count.\n\n**Function Signature:**\n```python\ndef compute_score(reward_inputs: List[Dict[str, Any]], format_weight: float = 0.1, alpha_length_bonus: float = 0.5, alpha_entropy: float = 0.5) -> List[Dict[str, float]]:\n```\n\n**Input Parameters:**\n- `reward_inputs`: List of reward input dictionaries, each containing:\n- `response`: Model-generated response text\n- `ground_truth`: Ground truth answer\n- `difficulty`: Sample difficulty label (\"easy\" / \"medium\" / \"hard\")\n- `high_entropy_token_num`: Number of HWE tokens in response\n- `target_high_entropy_token_num`: Target HWE token count (difficulty-related)\n- `alpha_entropy`: Entropy reward coefficient (difficulty-related)\n- `accuracy`: Accuracy label (optional, if not provided, extracted from response and evaluated)\n- `format_weight`: Format reward weight, default 0.1\n- `alpha_length_bonus`: Length reward coefficient (legacy parameter, actual usage is alpha_entropy)\n- `alpha_entropy`: Default entropy reward coefficient, default 0.5 (used if sample doesn't specify)\n\n**Detailed Description:**\nThis function implements ARES's core reward shaping mechanism, adaptively adjusting reasoning resource allocation based on task difficulty and model performance. The function computes format reward, accuracy reward, and entropy reward. For easy questions, when the answer is correct, it penalizes overly long reasoning chains, while when the answer is incorrect, it provides moderate positive rewards to encourage the model to explore more possibilities. Medium-difficulty questions require reasoning length closer to the target value; both excessive and insufficient reasoning are penalized when the answer is correct, but longer reasoning is still encouraged when the answer is incorrect. Hard questions adopt the opposite strategy: when the answer is correct, reasoning chains close to or exceeding the target are rewarded, and penalties are only applied when reasoning is significantly insufficient; when the answer is incorrect, longer reasoning is also encouraged to find the correct path.\nThe entire reward mechanism uses Huber loss function to smoothly handle penalty terms, Sigmoid function to achieve reward saturation effects, and sets different tolerance boundaries and reward caps for different difficulties.\nDifficulty-related hyperparameters that may be used:\n- `MARGIN_FRAC`: Tolerance band width (easy: 15%, medium: 25%, hard: 35%)\n- `KAPPA`: Huber function inflection point (easy: 2.0, medium: 3.0, hard: 4.0)\n- `TEMP`: Sigmoid temperature parameter (easy: 2.0, medium: 2.5, hard: 3.0)\n- `CAP_SCALE`: Entropy reward cap coefficient (easy: 1.0, medium: 1.0, hard: 1.2)\n\n**Expected Output:**\n- `scores`: List of score dictionaries, each containing:\n- `overall`: Total score = accuracy + entropy_score\n- `accuracy`: Accuracy score (0.0 or 1.0)\n- `format`: Format score (0.0 or 1.0)\n- `high_entropy_token_num_score`: Entropy reward score\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["Certainly! Below is the implementation of the `compute_score` function based on the provided specifications and detailed description. The function calculates the format reward, accuracy reward, and entropy reward for each input sample and returns the total score along with individual component scores.\n\n```python\nfrom typing import List, Dict, Any\nimport math\n\ndef sigmoid(x, temp):\n    \"\"\"Sigmoid function with temperature.\"\"\"\n    return  / ( + math.exp(-x / temp))\n\ndef huber_loss(x, kappa):\n    \"\"\"Huber loss function.\"\"\"\n    if abs(x) <= kappa:\n        return . * x * x\n    else:\n        return kappa * (abs(x) - . * kappa)\n\ndef compute_score(reward_inputs: List[Dict[str, Any]], format_weight: float = ., alpha_length_bonus: float = ., alpha_entropy: float = .) -> List[Dict[str, float]]:\n    scores =    \n    for input_data in reward_inputs:\n        response = input_data['response']\n        ground_truth = input_data['ground_truth']\n        difficulty = input_data['difficulty']\n        high_entropy_token_num = input_data['high_entropy_token_num']\n        target_high_entropy_token_num = input_data['target_high_entropy_token_num']\n        alpha_entropy = input_data.get('alpha_entropy', alpha_entropy)\n        accuracy = input_data.get('accuracy', None)\n        \n        # Determine difficulty-related parameters\n        if difficulty == 'easy':\n            MARGIN_FRAC = .\n            KAPPA = .\n            TEMP = .\n            CAP_SCALE = .\n        elif difficulty == 'medium':\n            MARGIN_FRAC = .\n            KAPPA = .\n            TEMP = .\n            CAP_SCALE = .\n        elif difficulty == 'hard':\n            MARGIN_FRAC = .\n            KAPPA = .\n            TEMP = .\n            CAP_SCALE = .\n        else:\n            raise ValueError(f\"Unknown difficulty level: {difficulty}\")\n        \n        # Calculate format reward (. or .)\n        format_reward = . if response.strip().startswith(ground_truth.strip()) else .\n        \n        # Calculate accuracy reward (. or .)\n        if accuracy is None:\n            accuracy = . if response.strip() == ground_truth.strip() else .\n        accuracy_reward = accuracy\n        \n        # Calculate entropy reward\n        margin = target_high_entropy_token_num * MARGIN_FRAC\n        diff = high_entropy_token_num - target_high_entropy_token_num\n        \n        if accuracy == .:\n            # Correct answer\n            if difficulty == 'easy':\n                entropy_reward = -huber_loss(diff, KAPPA) if abs(diff) > margin else .\n            elif difficulty == 'medium':\n                entropy_reward = -huber_loss(diff, KAPPA) if abs(diff) > margin else .\n            elif difficulty == 'hard':\n                entropy_reward = huber_loss(-diff, KAPPA) if diff < -margin else .\n        else:\n            # Incorrect answer\n            if difficulty == 'easy':\n                entropy_reward = huber_loss(diff, KAPPA) if diff > margin else .\n            elif difficulty == 'medium':\n                entropy_reward = huber_loss(diff, KAPPA) if diff > margin else .\n            elif difficulty == 'hard':\n                entropy_reward = huber_loss(diff, KAPPA) if diff > margin else .\n        \n        # Apply sigmoid for saturation and scale by alpha_entropy and CAP_SCALE\n        entropy_reward = sigmoid(entropy_reward, TEMP) * alpha_entropy * CAP_SCALE\n        \n        # Calculate overall score\n        overall_score = accuracy_reward + entropy_reward\n        \n        # Append the score dictionary to the results\n        scores.append({\n            'overall': overall_score,\n            'accuracy': accuracy_reward,\n            'format': format_reward,\n            'high_entropy_token_num_score': entropy_reward\n        })\n    \n    return scores\n```\n\n### Explanation:\n. **Sigmoid and Huber Loss Functions**: These helper functions are used to compute the entropy reward with smooth transitions and saturation effects.\n. **Parameter Handling**: The function handles different difficulty levels by setting appropriate parameters (`MARGIN_FRAC`, `KAPPA`, `TEMP`, `CAP_SCALE`).\n. **Format Reward**: This is a binary reward based on whether the response starts with the ground truth.\n. **Accuracy Reward**: This is a binary reward based on whether the response matches the ground truth.\n. **Entropy Reward**: This reward is calculated based on the difference between the actual and target high entropy token counts, with different strategies for correct and incorrect answers depending on the difficulty.\n. **Overall Score**: The total score is the sum of the accuracy reward and the entropy reward.\n\nThis implementation ensures that the function adheres to the specified logic and handles all input parameters correctly."], "usage": {"prompt_tokens": 896, "completion_tokens": 1079, "total_tokens": 1975, "per_completion": [{"prompt_tokens": 896, "completion_tokens": 1079, "total_tokens": 1975}]}}
{"function_name": "RayPPOTrainer.update_difficulty_and_skip_gid_set", "overview": "Updates sample difficulty dictionary and adaptive entropy coefficients based on current batch rollout results, implementing online difficulty bucketing and resource budget adjustment.", "function_signature": "def update_difficulty_and_skip_gid_set(self, batch: DataProto) -> None:", "input_parameters": "- `batch`: DataProto object containing:\n- `global_id`: Sample global ID (for cross-batch tracking)\n- `accuracy`: Accuracy list (accuracy of each response)\n- `entropies`: Average entropy list (average token entropy of each response)\n- `high_entropy_token_num`: HWE token count list (HWE token count of each response)", "detailed_description": "This function is responsible for dynamically updating question difficulty classification and adaptively adjusting entropy penalty coefficients during training. The function needs to aggregate accuracy, entropy, and high-entropy token counts for each question by global identifier. Questions are categorized into easy, medium, and hard levels based on accuracy. Based on high-entropy token counts, the function needs to compute target reasoning length for each difficulty level and adjust entropy penalty coefficients based on deviation between actual generated length and target value. When adjusting entropy penalty coefficients, gradient ascent can be applied. For easy and medium questions, if reasoning is too long, increase penalty strength; for hard questions, increase encouragement when reasoning is insufficient.\nDifficulty assignment based on accuracy thresholds:\n- `acc_rate ≥ 2/3` → \"easy\"\n- `1/3 ≤ acc_rate < 2/3` → \"medium\"\n- `acc_rate < 1/3` → \"hard\"\n(Optional) The function adds samples that are \"all correct with extremely low average entropy\" to `self.skip_gid_set`, and subsequent training will skip these overly simple samples, focusing on challenging data.", "output": "- No direct return value, but updates the following class attributes:\n- `self.difficulty_dict_train`: Sample difficulty dictionary `{global_id: difficulty}`\n- `self.target_high_entropy_token_num_dict`: Target HWE token count `{difficulty: target_num}`\n- `self.alpha_entropy_dict`: Entropy coefficient dictionary `{difficulty: alpha}`\n- `self.skip_gid_set`: Skip sample set (optional)", "func_implementation": "", "test_code_path": "code/test_code/test_update_difficulty.py", "implementation_location": "code/verl/trainer/ray_trainer.py:line 775-825", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `RayPPOTrainer.update_difficulty_and_skip_gid_set`.\n\n**Function Overview:**\nUpdates sample difficulty dictionary and adaptive entropy coefficients based on current batch rollout results, implementing online difficulty bucketing and resource budget adjustment.\n\n**Function Signature:**\n```python\ndef update_difficulty_and_skip_gid_set(self, batch: DataProto) -> None:\n```\n\n**Input Parameters:**\n- `batch`: DataProto object containing:\n- `global_id`: Sample global ID (for cross-batch tracking)\n- `accuracy`: Accuracy list (accuracy of each response)\n- `entropies`: Average entropy list (average token entropy of each response)\n- `high_entropy_token_num`: HWE token count list (HWE token count of each response)\n\n**Detailed Description:**\nThis function is responsible for dynamically updating question difficulty classification and adaptively adjusting entropy penalty coefficients during training. The function needs to aggregate accuracy, entropy, and high-entropy token counts for each question by global identifier. Questions are categorized into easy, medium, and hard levels based on accuracy. Based on high-entropy token counts, the function needs to compute target reasoning length for each difficulty level and adjust entropy penalty coefficients based on deviation between actual generated length and target value. When adjusting entropy penalty coefficients, gradient ascent can be applied. For easy and medium questions, if reasoning is too long, increase penalty strength; for hard questions, increase encouragement when reasoning is insufficient.\nDifficulty assignment based on accuracy thresholds:\n- `acc_rate ≥ 2/3` → \"easy\"\n- `1/3 ≤ acc_rate < 2/3` → \"medium\"\n- `acc_rate < 1/3` → \"hard\"\n(Optional) The function adds samples that are \"all correct with extremely low average entropy\" to `self.skip_gid_set`, and subsequent training will skip these overly simple samples, focusing on challenging data.\n\n**Expected Output:**\n- No direct return value, but updates the following class attributes:\n- `self.difficulty_dict_train`: Sample difficulty dictionary `{global_id: difficulty}`\n- `self.target_high_entropy_token_num_dict`: Target HWE token count `{difficulty: target_num}`\n- `self.alpha_entropy_dict`: Entropy coefficient dictionary `{difficulty: alpha}`\n- `self.skip_gid_set`: Skip sample set (optional)\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["Certainly! Below is the implementation of the `update_difficulty_and_skip_gid_set` function based on the provided specifications. This function processes the batch data to update the difficulty levels, target high-entropy token counts, entropy penalty coefficients, and optionally skips overly simple samples.\n\n```python\nfrom typing import Dict, Set, List\nfrom collections import defaultdict\n\nclass DataProto:\n    def __init__(self, global_id: List[int], accuracy: List[float], entropies: List[float], high_entropy_token_num: List[int]):\n        self.global_id = global_id\n        self.accuracy = accuracy\n        self.entropies = entropies\n        self.high_entropy_token_num = high_entropy_token_num\n\nclass RayPPOTrainer:\n    def __init__(self):\n        self.difficulty_dict_train: Dict[int, str] =        self.target_high_entropy_token_num_dict: Dict[str, float] = {'easy': ., 'medium': ., 'hard': .}\n        self.alpha_entropy_dict: Dict[str, float] = {'easy': ., 'medium': ., 'hard': .}\n        self.skip_gid_set: Set[int] = set()\n        self.difficulty_data: Dict[int, List[float]] = defaultdict(list)\n        self.entropy_data: Dict[int, List[float]] = defaultdict(list)\n        self.high_entropy_token_data: Dict[int, List[int]] = defaultdict(list)\n\n    def update_difficulty_and_skip_gid_set(self, batch: DataProto) -> None:\n        # Aggregate data by global_id\n        for gid, acc, ent, hetn in zip(batch.global_id, batch.accuracy, batch.entropies, batch.high_entropy_token_num):\n            self.difficulty_data[gid].append(acc)\n            self.entropy_data[gid].append(ent)\n            self.high_entropy_token_data[gid].append(hetn)\n\n        # Calculate average accuracy, entropy, and high-entropy token count for each global_id\n        for gid in self.difficulty_data:\n            avg_acc = sum(self.difficulty_data[gid]) / len(self.difficulty_data[gid])\n            avg_ent = sum(self.entropy_data[gid]) / len(self.entropy_data[gid])\n            avg_hetn = sum(self.high_entropy_token_data[gid]) / len(self.high_entropy_token_data[gid])\n\n            # Determine difficulty level based on accuracy\n            if avg_acc >= /:\n                difficulty = 'easy'\n            elif avg_acc >= /:\n                difficulty = 'medium'\n            else:\n                difficulty = 'hard'\n\n            # Update difficulty dictionary\n            self.difficulty_dict_train[gid] = difficulty\n\n            # Optionally skip samples that are all correct with extremely low average entropy\n            if avg_acc == . and avg_ent < .:  # Threshold for extremely low entropy can be adjusted\n                self.skip_gid_set.add(gid)\n\n            # Update target high-entropy token count and entropy penalty coefficient\n            if difficulty in self.target_high_entropy_token_num_dict:\n                # Example target reasoning length calculation (can be adjusted)\n                if difficulty == 'easy':\n                    target_hetn = .  # Example target for easy questions\n                elif difficulty == 'medium':\n                    target_hetn = .  # Example target for medium questions\n                else:\n                    target_hetn = .  # Example target for hard questions\n\n                self.target_high_entropy_token_num_dict[difficulty] = target_hetn\n\n                # Calculate deviation from target high-entropy token count\n                deviation = avg_hetn - target_hetn\n\n                # Adjust entropy penalty coefficient based on deviation\n                # Example adjustment logic (can be adjusted)\n                if difficulty in self.alpha_entropy_dict:\n                    alpha = self.alpha_entropy_dict[difficulty]\n                    if difficulty in ['easy', 'medium']:\n                        if deviation > :  # Reasoning is too long\n                            alpha += .  # Increase penalty strength\n                        else:  # Reasoning is too short\n                            alpha -= .  # Decrease penalty strength\n                    else:  # difficulty == 'hard'\n                        if deviation < :  # Reasoning is insufficient\n                            alpha += .  # Increase encouragement\n                        else:  # Reasoning is too long\n                            alpha -= .  # Decrease encouragement\n\n                    # Ensure alpha remains non-negative\n                    self.alpha_entropy_dict[difficulty] = max(., alpha)\n\n        # Clear aggregated data for the next batch\n        self.difficulty_data.clear()\n        self.entropy_data.clear()\n        self.high_entropy_token_data.clear()\n\n# Example usage\nif __name__ == \"__main__\":\n    batch = DataProto(\n        global_id=[, , , ],\n        accuracy=[., ., ., .],\n        entropies=[., ., ., .],\n        high_entropy_token_num=[, , , ]\n    )\n    trainer = RayPPOTrainer()\n    trainer.update_difficulty_and_skip_gid_set(batch)\n    print(trainer.difficulty_dict_train)\n    print(trainer.target_high_entropy_token_num_dict)\n    print(trainer.alpha_entropy_dict)\n    print(trainer.skip_gid_set)\n```\n\n### Explanation:\n. **Data Aggregation**: The function aggregates accuracy, entropy, and high-entropy token counts for each sample by its global ID.\n. **Difficulty Assignment**: Based on the average accuracy, each sample is classified into \"easy\", \"medium\", or \"hard\".\n. **Skip Condition**: Samples that are all correct with extremely low average entropy are added to the `skip_gid_set`.\n. **Target Reasoning Length**: Target high-entropy token counts are set for each difficulty level.\n. **Entropy Penalty Adjustment**: Entropy penalty coefficients are adjusted based on the deviation of the actual high-entropy token count from the target.\n. **Data Clearing**: Aggregated data is cleared for the next batch to avoid data leakage.\n\nThis implementation provides a basic framework and can be further refined based on specific requirements and experimental results."], "usage": {"prompt_tokens": 662, "completion_tokens": 1327, "total_tokens": 1989, "per_completion": [{"prompt_tokens": 662, "completion_tokens": 1327, "total_tokens": 1989}]}}
{"function_name": "TokenLevelAdaptiveKLController.update", "overview": "Implements update logic for token-level adaptive KL controller, dynamically adjusting KL coefficients using dual ascent mechanism based on deviation between actual KL divergence and target for each difficulty bucket.", "function_signature": "def update(self, current_kls: Dict[str, float], n_steps: Dict[str, int]):", "input_parameters": "- `current_kls`: Average KL divergence for each difficulty bucket at current step, dictionary format `{difficulty: kl_value}`\n- Example: `{\"easy\": 0.06, \"medium\": 0.12, \"hard\": 0.18}`\n- `n_steps`: Sample count for each difficulty bucket (used for weighting, may be simplified in actual implementation)", "detailed_description": "Token-level adaptive KL control allows using different KL budgets for different difficulty tasks, automatically adjusting KL divergence coefficients for each difficulty bucket through dual ascent mechanism. When actual KL divergence exceeds target value, increase penalty coefficient; when below target value, decrease coefficient.", "output": "- No direct return value, but updates class attributes:\n- `self.kl_coefs`: Updated KL coefficient dictionary `{difficulty: beta}`\n- `self.lambdas`: Updated Lagrange multiplier dictionary `{difficulty: lambda}`", "func_implementation": "", "test_code_path": "code/test_code/test_kl_controller.py", "implementation_location": "code/verl/trainer/core_algos.py:line 77-86", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `TokenLevelAdaptiveKLController.update`.\n\n**Function Overview:**\nImplements update logic for token-level adaptive KL controller, dynamically adjusting KL coefficients using dual ascent mechanism based on deviation between actual KL divergence and target for each difficulty bucket.\n\n**Function Signature:**\n```python\ndef update(self, current_kls: Dict[str, float], n_steps: Dict[str, int]):\n```\n\n**Input Parameters:**\n- `current_kls`: Average KL divergence for each difficulty bucket at current step, dictionary format `{difficulty: kl_value}`\n- Example: `{\"easy\": 0.06, \"medium\": 0.12, \"hard\": 0.18}`\n- `n_steps`: Sample count for each difficulty bucket (used for weighting, may be simplified in actual implementation)\n\n**Detailed Description:**\nToken-level adaptive KL control allows using different KL budgets for different difficulty tasks, automatically adjusting KL divergence coefficients for each difficulty bucket through dual ascent mechanism. When actual KL divergence exceeds target value, increase penalty coefficient; when below target value, decrease coefficient.\n\n**Expected Output:**\n- No direct return value, but updates class attributes:\n- `self.kl_coefs`: Updated KL coefficient dictionary `{difficulty: beta}`\n- `self.lambdas`: Updated Lagrange multiplier dictionary `{difficulty: lambda}`\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["Certainly! Below is the implementation of the `TokenLevelAdaptiveKLController.update` function based on the provided specifications. This function will update the KL coefficients and Lagrange multipliers for each difficulty bucket using a dual ascent mechanism.\n\n```python\nfrom typing import Dict\n\nclass TokenLevelAdaptiveKLController:\n    def __init__(self, target_kl: float, beta: float = ., lambda_init: float = .):\n        \"\"\"\n        Initialize the TokenLevelAdaptiveKLController.\n\n        :param target_kl: Target KL divergence for each difficulty bucket.\n        :param beta: Initial KL coefficient for each difficulty bucket.\n        :param lambda_init: Initial Lagrange multiplier for each difficulty bucket.\n        \"\"\"\n        self.target_kl = target_kl\n        self.kl_coefs =  # KL coefficients for each difficulty bucket\n        self.lambdas =    # Lagrange multipliers for each difficulty bucket\n        self.beta = beta\n        self.lambda_init = lambda_init\n\n    def update(self, current_kls: Dict[str, float], n_steps: Dict[str, int]):\n        \"\"\"\n        Update the KL coefficients and Lagrange multipliers for each difficulty bucket.\n\n        :param current_kls: Average KL divergence for each difficulty bucket at current step.\n        :param n_steps: Sample count for each difficulty bucket.\n        \"\"\"\n        for difficulty, kl_value in current_kls.items():\n            # Initialize beta and lambda if not already set\n            if difficulty not in self.kl_coefs:\n                self.kl_coefs[difficulty] = self.beta\n            if difficulty not in self.lambdas:\n                self.lambdas[difficulty] = self.lambda_init\n\n            # Calculate the difference between actual KL and target KL\n            kl_diff = kl_value - self.target_kl\n\n            # Update the Lagrange multiplier using dual ascent\n            self.lambdas[difficulty] += self.kl_coefs[difficulty] * kl_diff\n\n            # Update the KL coefficient\n            self.kl_coefs[difficulty] = max(e-, self.kl_coefs[difficulty] * ( ** (-self.lambdas[difficulty])))\n\n            # Optional: Print debug information\n            # print(f\"Difficulty: {difficulty}, Current KL: {kl_value}, Target KL: {self.target_kl}, \"\n            #       f\"KL Diff: {kl_diff}, Lambda: {self.lambdas[difficulty]}, Beta: {self.kl_coefs[difficulty]}\")\n\n# Example usage:\n# controller = TokenLevelAdaptiveKLController(target_kl=.)\n# controller.update({\"easy\": ., \"medium\": ., \"hard\": .}, {\"easy\": , \"medium\": , \"hard\": })\n# print(controller.kl_coefs)\n# print(controller.lambdas)\n```\n\n### Explanation:\n. **Initialization**: The `__init__` method initializes the target KL divergence, initial KL coefficients (`beta`), and initial Lagrange multipliers (`lambda_init`).\n\n. **Update Logic**:\n   - For each difficulty bucket, the function checks if the KL coefficient and Lagrange multiplier are initialized. If not, it initializes them.\n   - It calculates the difference between the current KL divergence and the target KL divergence.\n   - It updates the Lagrange multiplier using the dual ascent mechanism.\n   - It updates the KL coefficient based on the updated Lagrange multiplier. The KL coefficient is adjusted using an exponential decay mechanism to ensure it remains positive.\n\n. **Comments**: The code includes comments to explain each step of the process. Debug information can be printed if needed.\n\nThis implementation ensures that the KL coefficients and Lagrange multipliers are dynamically adjusted based on the deviation between the actual and target KL divergence for each difficulty bucket."], "usage": {"prompt_tokens": 464, "completion_tokens": 808, "total_tokens": 1272, "per_completion": [{"prompt_tokens": 464, "completion_tokens": 808, "total_tokens": 1272}]}}
{"function_name": "apply_kl_penalty", "overview": "Applies KL divergence penalty to token-level rewards and updates KL controller.", "function_signature": "def apply_kl_penalty(data: DataProto, kl_ctrl: KLController, kl_penalty=\"kl\"):", "input_parameters": "- `data`: DataProto object containing:\n- `token_level_scores`: Original token-level rewards [batch_size, response_length]\n- `old_log_probs`: Actor old policy log probabilities [batch_size, response_length]\n- `ref_log_probs`: Ref policy log probabilities [batch_size, response_length]\n- `response_mask`: Response mask [batch_size, response_length]\n- `kl_ctrl`: KL controller (FixedKLController or AdaptiveKLController)\n- `kl_penalty`: KL penalty type (\"kl\", \"abs\", \"mse\", \"low_var_kl\", \"full\")", "detailed_description": "This function implements computation and application of token-level KL penalty. It computes token-level KL divergence based on `kl_penalty` type, then subtracts KL penalty term from original rewards to obtain token-level rewards, and computes sequence-level KL divergence and batch-level KL divergence.\nThe function also needs to implement KL controller update.", "output": "- `data`: Updated DataProto where `data.batch[\"token_level_rewards\"]` has KL penalty applied\n- `metrics`: Metrics dictionary `{\"critic/kl\": current_kl, \"critic/kl_coef\": kl_ctrl.kl_coef}`", "func_implementation": "", "test_code_path": "code/test_code/test_apply_kl_penalty.py", "implementation_location": "code/verl/trainer/ray_trainer.py:line 107-124", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `apply_kl_penalty`.\n\n**Function Overview:**\nApplies KL divergence penalty to token-level rewards and updates KL controller.\n\n**Function Signature:**\n```python\ndef apply_kl_penalty(data: DataProto, kl_ctrl: KLController, kl_penalty=\"kl\"):\n```\n\n**Input Parameters:**\n- `data`: DataProto object containing:\n- `token_level_scores`: Original token-level rewards [batch_size, response_length]\n- `old_log_probs`: Actor old policy log probabilities [batch_size, response_length]\n- `ref_log_probs`: Ref policy log probabilities [batch_size, response_length]\n- `response_mask`: Response mask [batch_size, response_length]\n- `kl_ctrl`: KL controller (FixedKLController or AdaptiveKLController)\n- `kl_penalty`: KL penalty type (\"kl\", \"abs\", \"mse\", \"low_var_kl\", \"full\")\n\n**Detailed Description:**\nThis function implements computation and application of token-level KL penalty. It computes token-level KL divergence based on `kl_penalty` type, then subtracts KL penalty term from original rewards to obtain token-level rewards, and computes sequence-level KL divergence and batch-level KL divergence.\nThe function also needs to implement KL controller update.\n\n**Expected Output:**\n- `data`: Updated DataProto where `data.batch[\"token_level_rewards\"]` has KL penalty applied\n- `metrics`: Metrics dictionary `{\"critic/kl\": current_kl, \"critic/kl_coef\": kl_ctrl.kl_coef}`\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["Certainly! Below is the implementation of the `apply_kl_penalty` function based on the provided specifications. This function computes the KL divergence penalty, applies it to the token-level rewards, and updates the KL controller.\n\n```python\nimport torch\n\nclass DataProto:\n    def __init__(self, token_level_scores, old_log_probs, ref_log_probs, response_mask):\n        self.batch = {\n            \"token_level_scores\": token_level_scores,\n            \"old_log_probs\": old_log_probs,\n            \"ref_log_probs\": ref_log_probs,\n            \"response_mask\": response_mask\n        }\n\nclass KLController:\n    def __init__(self, kl_coef):\n        self.kl_coef = kl_coef\n\n    def update(self, kl):\n        raise NotImplementedError(\"This method should be overridden by subclasses\")\n\nclass FixedKLController(KLController):\n    def update(self, kl):\n        # In a fixed KL controller, the KL coefficient does not change\n        pass\n\nclass AdaptiveKLController(KLController):\n    def update(self, kl):\n        # Example adaptive update logic (this can be customized)\n        target_kl = .\n        if kl > target_kl * .:\n            self.kl_coef *= .\n        elif kl < target_kl * .:\n            self.kl_coef *= .\n\ndef compute_kl_divergence(old_log_probs, ref_log_probs, response_mask, kl_penalty=\"kl\"):\n    # Compute the KL divergence based on the penalty type\n    if kl_penalty == \"kl\":\n        kl_div = torch.exp(ref_log_probs) * (ref_log_probs - old_log_probs)\n    elif kl_penalty == \"abs\":\n        kl_div = torch.abs(ref_log_probs - old_log_probs)\n    elif kl_penalty == \"mse\":\n        kl_div = (ref_log_probs - old_log_probs) ** \n    elif kl_penalty == \"low_var_kl\":\n        kl_div = torch.exp(ref_log_probs) * (ref_log_probs - old_log_probs) - (ref_log_probs - old_log_probs)\n    elif kl_penalty == \"full\":\n        kl_div = torch.exp(ref_log_probs) * (ref_log_probs - old_log_probs) + (ref_log_probs - old_log_probs) ** \n    else:\n        raise ValueError(f\"Unknown KL penalty type: {kl_penalty}\")\n\n    # Apply the response mask\n    kl_div = kl_div * response_mask\n    return kl_div\n\ndef apply_kl_penalty(data: DataProto, kl_ctrl: KLController, kl_penalty=\"kl\"):\n    # Extract necessary data from DataProto\n    token_level_scores = data.batch[\"token_level_scores\"]\n    old_log_probs = data.batch[\"old_log_probs\"]\n    ref_log_probs = data.batch[\"ref_log_probs\"]\n    response_mask = data.batch[\"response_mask\"]\n\n    # Compute KL divergence\n    kl_div = compute_kl_divergence(old_log_probs, ref_log_probs, response_mask, kl_penalty)\n\n    # Compute sequence-level KL divergence\n    seq_kl = torch.sum(kl_div, dim=) / torch.sum(response_mask, dim=)\n\n    # Compute batch-level KL divergence\n    batch_kl = torch.mean(seq_kl)\n\n    # Apply KL penalty to token-level rewards\n    kl_penalty_term = kl_div * kl_ctrl.kl_coef\n    token_level_rewards = token_level_scores - kl_penalty_term\n\n    # Update the KL controller\n    kl_ctrl.update(batch_kl)\n\n    # Update the DataProto with the new token-level rewards\n    data.batch[\"token_level_rewards\"] = token_level_rewards\n\n    # Prepare the metrics dictionary\n    metrics = {\n        \"critic/kl\": batch_kl.item(),\n        \"critic/kl_coef\": kl_ctrl.kl_coef\n    }\n\n    return data, metrics\n\n# Example usage\nif __name__ == \"__main__\":\n    # Sample data\n    batch_size = \n    response_length = \n    token_level_scores = torch.randn(batch_size, response_length)\n    old_log_probs = torch.randn(batch_size, response_length)\n    ref_log_probs = torch.randn(batch_size, response_length)\n    response_mask = torch.ones(batch_size, response_length)\n\n    data = DataProto(token_level_scores, old_log_probs, ref_log_probs, response_mask)\n    kl_ctrl = AdaptiveKLController(kl_coef=.)\n\n    # Apply KL penalty\n    updated_data, metrics = apply_kl_penalty(data, kl_ctrl, kl_penalty=\"kl\")\n    print(\"Updated Token-Level Rewards:\", updated_data.batch[\"token_level_rewards\"])\n    print(\"Metrics:\", metrics)\n```\n\n### Explanation:\n. **DataProto and KLController Classes**: These are simplified versions of the actual classes you might use. `DataProto` holds the necessary data, and `KLController` manages the KL coefficient.\n. **compute_kl_divergence Function**: Computes the KL divergence based on the specified penalty type and applies the response mask.\n. **apply_kl_penalty Function**:\n   - Extracts necessary data from `DataProto`.\n   - Computes the KL divergence using `compute_kl_divergence`.\n   - Computes sequence-level and batch-level KL divergences.\n   - Applies the KL penalty to the token-level rewards.\n   - Updates the KL controller.\n   - Updates the `DataProto` with the new token-level rewards.\n   - Returns the updated `DataProto` and a metrics dictionary.\n\nThis implementation should meet the requirements specified in the detailed description."], "usage": {"prompt_tokens": 501, "completion_tokens": 1143, "total_tokens": 1644, "per_completion": [{"prompt_tokens": 501, "completion_tokens": 1143, "total_tokens": 1644}]}}
