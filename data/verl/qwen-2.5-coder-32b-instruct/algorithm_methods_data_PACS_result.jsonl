{"function_name": "compute_reward", "overview": "Computes \"reward\" for policy improvement, measuring log probability change of current policy relative to reference policy.", "function_signature": "def compute_reward(\nold_log_prob,\nlog_prob,\nresponse_mask,\nbeta=1.0,\nreward_method=\"1\",\n):", "input_parameters": "- `old_log_prob`: Reference policy token-level log probabilities, shape `[batch_size, response_length]`\n- `log_prob`: Current policy token-level log probabilities, shape `[batch_size, response_length]`\n- `response_mask`: Response portion mask, shape `[batch_size, response_length]`\n- `beta`: Reward scaling factor, controls aggressiveness of policy updates (default 1.0)\n- `reward_method`: Reward computation method, '1' or '2'", "detailed_description": "Function computes reward values in reinforcement learning based on specified method.\nMethod 1 computes sum of new-old log probability differences, measuring policy improvement magnitude.\nMethod 2 computes normalized sum of current log probabilities.\nIf an invalid method number is specified, function raises an exception.", "output": "- `reward`: Policy improvement reward, shape `[batch_size, 1]`", "func_implementation": "", "test_code_path": "code/test_code/test_compute_reward.py", "implementation_location": "code/src/pacs/pacs_core_algos.py:line 7-21", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_reward`.\n\n**Function Overview:**\nComputes \"reward\" for policy improvement, measuring log probability change of current policy relative to reference policy.\n\n**Function Signature:**\n```python\ndef compute_reward(\nold_log_prob,\nlog_prob,\nresponse_mask,\nbeta=1.0,\nreward_method=\"1\",\n):\n```\n\n**Input Parameters:**\n- `old_log_prob`: Reference policy token-level log probabilities, shape `[batch_size, response_length]`\n- `log_prob`: Current policy token-level log probabilities, shape `[batch_size, response_length]`\n- `response_mask`: Response portion mask, shape `[batch_size, response_length]`\n- `beta`: Reward scaling factor, controls aggressiveness of policy updates (default 1.0)\n- `reward_method`: Reward computation method, '1' or '2'\n\n**Detailed Description:**\nFunction computes reward values in reinforcement learning based on specified method.\nMethod 1 computes sum of new-old log probability differences, measuring policy improvement magnitude.\nMethod 2 computes normalized sum of current log probabilities.\nIf an invalid method number is specified, function raises an exception.\n\n**Expected Output:**\n- `reward`: Policy improvement reward, shape `[batch_size, 1]`\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["Certainly! Below is the implementation of the `compute_reward` function based on the provided specifications:\n\n```python\nimport torch\n\ndef compute_reward(\n    old_log_prob,\n    log_prob,\n    response_mask,\n    beta=.,\n    reward_method=\"\",\n):\n    \"\"\"\n    Computes the reward for policy improvement based on the specified method.\n    \n    Parameters:\n    - old_log_prob: Reference policy token-level log probabilities, shape [batch_size, response_length]\n    - log_prob: Current policy token-level log probabilities, shape [batch_size, response_length]\n    - response_mask: Response portion mask, shape [batch_size, response_length]\n    - beta: Reward scaling factor, controls aggressiveness of policy updates (default .)\n    - reward_method: Reward computation method, '' or ''\n    \n    Returns:\n    - reward: Policy improvement reward, shape [batch_size, ]\n    \"\"\"\n    \n    # Ensure the input tensors are of the same shape\n    assert old_log_prob.shape == log_prob.shape == response_mask.shape, \"Input tensors must have the same shape\"\n    \n    # Method : Compute sum of new-old log probability differences\n    if reward_method == \"\":\n        # Calculate the difference in log probabilities\n        log_prob_diff = log_prob - old_log_prob\n        # Apply the response mask to consider only the response portion\n        masked_log_prob_diff = log_prob_diff * response_mask\n        # Sum the differences for each batch and scale by beta\n        reward = beta * torch.sum(masked_log_prob_diff, dim=, keepdim=True)\n    \n    # Method : Compute normalized sum of current log probabilities\n    elif reward_method == \"\":\n        # Apply the response mask to consider only the response portion\n        masked_log_prob = log_prob * response_mask\n        # Sum the log probabilities for each batch\n        sum_log_prob = torch.sum(masked_log_prob, dim=, keepdim=True)\n        # Count the number of tokens in the response portion for each batch\n        count_tokens = torch.sum(response_mask, dim=, keepdim=True)\n        # Compute the average log probability\n        avg_log_prob = sum_log_prob / (count_tokens + e-)  # Add a small epsilon to avoid division by zero\n        # Scale by beta\n        reward = beta * avg_log_prob\n    \n    else:\n        raise ValueError(f\"Invalid reward method specified: {reward_method}. Please use '' or ''.\")\n    \n    return reward\n```\n\n### Explanation:\n. **Input Validation**: The function first checks if the input tensors `old_log_prob`, `log_prob`, and `response_mask` have the same shape.\n. **Reward Method **:\n   - Computes the difference between the current and reference log probabilities.\n   - Applies the `response_mask` to consider only the relevant tokens.\n   - Sums the differences for each batch and scales the result by `beta`.\n. **Reward Method **:\n   - Applies the `response_mask` to consider only the relevant tokens.\n   - Sums the log probabilities for each batch.\n   - Counts the number of tokens in the response portion for each batch.\n   - Computes the average log probability and scales the result by `beta`.\n. **Error Handling**: Raises a `ValueError` if an invalid `reward_method` is specified.\n\nThis implementation ensures that the function adheres to the specified requirements and handles different reward computation methods correctly."], "usage": {"prompt_tokens": 446, "completion_tokens": 729, "total_tokens": 1175, "per_completion": [{"prompt_tokens": 446, "completion_tokens": 729, "total_tokens": 1175}]}, "results": [false], "pass_ratios": [0.0]}
{"function_name": "compute_weight", "overview": "Computes sample weights to handle class imbalance problem (uneven positive/negative sample ratio).", "function_signature": "def compute_weight(score, rollout_n, mode=\"question\"):", "input_parameters": "- `score`: Verifiable rewards (labels), shape `[batch_size, 1]`\n- `rollout_n`: Number of responses generated per prompt\n- `mode`: Weight computation mode, 'question'/'only_positive'/'only_negative'", "detailed_description": "Function assigns training weights to samples based on different modes. In \"question\" mode, function groups samples by rollout_n size, using class-balanced weights for groups with label imbalance to make positive and negative samples contribute equally to training. In \"only_positive\" mode, only positive samples (label 1) are assigned weight 1.0, negative samples get weight 0. In \"only_negative\" mode, the opposite applies: only negative samples (label 0) are assigned weight 1.0. If an invalid mode is specified, function raises an exception.", "output": "- `weight`: Sample weights, shape `[batch_size, 1]`", "func_implementation": "", "test_code_path": "code/test_code/test_compute_weight.py", "implementation_location": "code/src/pacs/pacs_core_algos.py:line 59-86", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_weight`.\n\n**Function Overview:**\nComputes sample weights to handle class imbalance problem (uneven positive/negative sample ratio).\n\n**Function Signature:**\n```python\ndef compute_weight(score, rollout_n, mode=\"question\"):\n```\n\n**Input Parameters:**\n- `score`: Verifiable rewards (labels), shape `[batch_size, 1]`\n- `rollout_n`: Number of responses generated per prompt\n- `mode`: Weight computation mode, 'question'/'only_positive'/'only_negative'\n\n**Detailed Description:**\nFunction assigns training weights to samples based on different modes. In \"question\" mode, function groups samples by rollout_n size, using class-balanced weights for groups with label imbalance to make positive and negative samples contribute equally to training. In \"only_positive\" mode, only positive samples (label 1) are assigned weight 1.0, negative samples get weight 0. In \"only_negative\" mode, the opposite applies: only negative samples (label 0) are assigned weight 1.0. If an invalid mode is specified, function raises an exception.\n\n**Expected Output:**\n- `weight`: Sample weights, shape `[batch_size, 1]`\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["Certainly! Below is the implementation of the `compute_weight` function based on the provided specifications:\n\n```python\nimport numpy as np\n\ndef compute_weight(score, rollout_n, mode=\"question\"):\n    \"\"\"\n    Computes sample weights to handle class imbalance problem (uneven positive/negative sample ratio).\n\n    Parameters:\n    - score: Verifiable rewards (labels), shape [batch_size, ]\n    - rollout_n: Number of responses generated per prompt\n    - mode: Weight computation mode, 'question'/'only_positive'/'only_negative'\n\n    Returns:\n    - weight: Sample weights, shape [batch_size, ]\n    \"\"\"\n    # Ensure score is a numpy array for easier manipulation\n    score = np.array(score)\n    \n    # Check if the batch size is a multiple of rollout_n\n    if score.shape[] % rollout_n != :\n        raise ValueError(\"Batch size must be a multiple of rollout_n.\")\n    \n    # Initialize weights array with zeros\n    weight = np.zeros_like(score)\n    \n    if mode == \"question\":\n        # Group samples by rollout_n size\n        for i in range(, score.shape[], rollout_n):\n            group_scores = score[i:i + rollout_n]\n            pos_count = np.sum(group_scores == )\n            neg_count = np.sum(group_scores == )\n            \n            # Calculate class-balanced weights for the group\n            if pos_count >  and neg_count > :\n                pos_weight = neg_count / (pos_count + neg_count)\n                neg_weight = pos_count / (pos_count + neg_count)\n            else:\n                # If all samples in the group are of the same class, assign weight .\n                pos_weight = . if pos_count >  else .\n                neg_weight = . if neg_count >  else .\n            \n            # Assign weights to the group\n            weight[i:i + rollout_n][group_scores == ] = pos_weight\n            weight[i:i + rollout_n][group_scores == ] = neg_weight\n    \n    elif mode == \"only_positive\":\n        # Assign weight . to positive samples,  to negative samples\n        weight[score == ] = .\n    \n    elif mode == \"only_negative\":\n        # Assign weight . to negative samples,  to positive samples\n        weight[score == ] = .\n    \n    else:\n        raise ValueError(f\"Invalid mode specified: {mode}. Mode must be 'question', 'only_positive', or 'only_negative'.\")\n    \n    return weight\n\n# Example usage:\n# score = np.array([[], [], [], [], [], [], [], []])\n# rollout_n = \n# mode = \"question\"\n# print(compute_weight(score, rollout_n, mode))\n```\n\n### Explanation:\n. **Input Validation**: The function first checks if the batch size is a multiple of `rollout_n`. If not, it raises a `ValueError`.\n. **Weight Initialization**: Initializes a weight array of the same shape as `score` with zeros.\n. **Mode Handling**:\n   - **\"question\" Mode**: Groups samples by `rollout_n` and calculates class-balanced weights for each group. If a group has only one class, it assigns weight . to that class.\n   - **\"only_positive\" Mode**: Assigns weight . to positive samples and  to negative samples.\n   - **\"only_negative\" Mode**: Assigns weight . to negative samples and  to positive samples.\n. **Error Handling**: Raises a `ValueError` if an invalid mode is specified.\n\nThis implementation ensures that the function behaves as expected based on the provided detailed description."], "usage": {"prompt_tokens": 443, "completion_tokens": 813, "total_tokens": 1256, "per_completion": [{"prompt_tokens": 443, "completion_tokens": 813, "total_tokens": 1256}]}, "results": [false], "pass_ratios": [0.0]}
{"function_name": "compute_pacs_loss", "overview": "Computes PACS's core loss function, using advantage function as logit and verifiable rewards as labels with binary cross-entropy loss.", "function_signature": "def compute_pacs_loss(\nprompts,\nold_log_prob,\nlog_prob,\ntoken_level_scores,\nresponse_mask,\nrollout_n,\nalgorithm_config=None,\n):", "input_parameters": "- `prompts`: Input prompts, shape `[batch_size, prompt_length]`\n- `old_log_prob`: Reference policy log probabilities, shape `[batch_size, response_length]`\n- `log_prob`: Current policy log probabilities, shape `[batch_size, response_length]`\n- `token_level_scores`: Verifiable rewards (labels), shape `[batch_size, response_length]`\n- `response_mask`: Response mask, shape `[batch_size, response_length]`\n- `rollout_n`: Number of responses generated per prompt\n- `algorithm_config`: Algorithm configuration object containing `beta`, `reward_method`, `adv_estimator`, `use_weight`, `weight_mode`, etc.", "detailed_description": "Function needs to verify that every rollout_n samples correspond to the same prompt, then compute reward values and advantage values (based on specified advantage estimator). Finally, function sums token-level scores and uses binary cross-entropy loss with advantage values as supervision signal and scores as predicted values to compute loss. If sample weighting is enabled, balanced weights are computed based on score labels to handle sample imbalance.", "output": "- `pacs_loss`: PACS loss value, scalar", "func_implementation": "", "test_code_path": "code/test_code/test_compute_pacs_loss.py", "implementation_location": "code/src/pacs/pacs_core_algos.py:line 89-138", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_pacs_loss`.\n\n**Function Overview:**\nComputes PACS's core loss function, using advantage function as logit and verifiable rewards as labels with binary cross-entropy loss.\n\n**Function Signature:**\n```python\ndef compute_pacs_loss(\nprompts,\nold_log_prob,\nlog_prob,\ntoken_level_scores,\nresponse_mask,\nrollout_n,\nalgorithm_config=None,\n):\n```\n\n**Input Parameters:**\n- `prompts`: Input prompts, shape `[batch_size, prompt_length]`\n- `old_log_prob`: Reference policy log probabilities, shape `[batch_size, response_length]`\n- `log_prob`: Current policy log probabilities, shape `[batch_size, response_length]`\n- `token_level_scores`: Verifiable rewards (labels), shape `[batch_size, response_length]`\n- `response_mask`: Response mask, shape `[batch_size, response_length]`\n- `rollout_n`: Number of responses generated per prompt\n- `algorithm_config`: Algorithm configuration object containing `beta`, `reward_method`, `adv_estimator`, `use_weight`, `weight_mode`, etc.\n\n**Detailed Description:**\nFunction needs to verify that every rollout_n samples correspond to the same prompt, then compute reward values and advantage values (based on specified advantage estimator). Finally, function sums token-level scores and uses binary cross-entropy loss with advantage values as supervision signal and scores as predicted values to compute loss. If sample weighting is enabled, balanced weights are computed based on score labels to handle sample imbalance.\n\n**Expected Output:**\n- `pacs_loss`: PACS loss value, scalar\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["To implement the `compute_pacs_loss` function as described, we need to follow these steps:\n\n. **Verify Rollouts**: Ensure that every `rollout_n` samples correspond to the same prompt.\n. **Compute Rewards and Advantages**: Calculate the reward values and advantage values based on the specified advantage estimator.\n. **Compute Loss**: Use binary cross-entropy loss with advantage values as supervision signals and token-level scores as predicted values.\n. **Sample Weighting**: If sample weighting is enabled, compute balanced weights based on score labels to handle sample imbalance.\n\nBelow is the implementation of the `compute_pacs_loss` function:\n\n```python\nimport torch\nimport torch.nn.functional as F\n\ndef compute_pacs_loss(\n    prompts,\n    old_log_prob,\n    log_prob,\n    token_level_scores,\n    response_mask,\n    rollout_n,\n    algorithm_config=None,\n):\n    \"\"\"\n    Computes PACS's core loss function, using advantage function as logit and verifiable rewards as labels with binary cross-entropy loss.\n    \n    Parameters:\n    - prompts: Input prompts, shape [batch_size, prompt_length]\n    - old_log_prob: Reference policy log probabilities, shape [batch_size, response_length]\n    - log_prob: Current policy log probabilities, shape [batch_size, response_length]\n    - token_level_scores: Verifiable rewards (labels), shape [batch_size, response_length]\n    - response_mask: Response mask, shape [batch_size, response_length]\n    - rollout_n: Number of responses generated per prompt\n    - algorithm_config: Algorithm configuration object containing beta, reward_method, adv_estimator, use_weight, weight_mode, etc.\n    \n    Returns:\n    - pacs_loss: PACS loss value, scalar\n    \"\"\"\n    \n    # Ensure algorithm_config is not None\n    if algorithm_config is None:\n        raise ValueError(\"algorithm_config must be provided\")\n    \n    # Extract necessary parameters from algorithm_config\n    beta = algorithm_config.get('beta', .)\n    reward_method = algorithm_config.get('reward_method', 'direct')\n    adv_estimator = algorithm_config.get('adv_estimator', 'simple')\n    use_weight = algorithm_config.get('use_weight', False)\n    weight_mode = algorithm_config.get('weight_mode', 'balanced')\n    \n    # Verify that every rollout_n samples correspond to the same prompt\n    batch_size, response_length = token_level_scores.shape\n    if batch_size % rollout_n != :\n        raise ValueError(\"Batch size must be a multiple of rollout_n\")\n    \n    # Compute advantages\n    if adv_estimator == 'simple':\n        # Simple advantage estimator: advantage = log_prob - old_log_prob\n        advantages = log_prob - old_log_prob\n    else:\n        raise ValueError(f\"Unsupported advantage estimator: {adv_estimator}\")\n    \n    # Apply response mask to advantages and token_level_scores\n    masked_advantages = advantages * response_mask\n    masked_scores = token_level_scores * response_mask\n    \n    # Compute binary cross-entropy loss\n    bce_loss = F.binary_cross_entropy_with_logits(masked_advantages, masked_scores, reduction='none')\n    \n    # Apply sample weighting if enabled\n    if use_weight:\n        if weight_mode == 'balanced':\n            # Compute balanced weights based on score labels\n            pos_weight = torch.sum(masked_scores == ) / torch.sum(masked_scores == )\n            bce_loss = F.binary_cross_entropy_with_logits(masked_advantages, masked_scores, pos_weight=pos_weight, reduction='none')\n        else:\n            raise ValueError(f\"Unsupported weight mode: {weight_mode}\")\n    \n    # Sum the loss over the response length and average over the batch\n    pacs_loss = torch.sum(bce_loss) / torch.sum(response_mask)\n    \n    return pacs_loss\n```\n\n### Explanation:\n. **Verification of Rollouts**: We check if the batch size is a multiple of `rollout_n` to ensure that every `rollout_n` samples correspond to the same prompt.\n. **Advantage Calculation**: We use a simple advantage estimator by default, which computes the difference between the current and old log probabilities.\n. **Masking**: We apply the response mask to both the advantages and token-level scores to ensure that only valid tokens are considered in the loss computation.\n. **Binary Cross-Entropy Loss**: We compute the binary cross-entropy loss using the masked advantages as logits and the masked scores as labels.\n. **Sample Weighting**: If sample weighting is enabled, we compute balanced weights based on the score labels to handle class imbalance.\n. **Loss Aggregation**: We sum the loss over the response length and average over the batch to get the final PACS loss value.\n\nThis implementation should meet the requirements specified in the detailed description."], "usage": {"prompt_tokens": 520, "completion_tokens": 979, "total_tokens": 1499, "per_completion": [{"prompt_tokens": 520, "completion_tokens": 979, "total_tokens": 1499}]}, "results": [false], "pass_ratios": [0.0]}
