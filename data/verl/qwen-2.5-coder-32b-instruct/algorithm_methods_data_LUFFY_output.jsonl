{"function_name": "compute_grpo_outcome_advantage_split", "overview": "Computes group GRPO (Group Relative Policy Optimization) advantage estimation, with the key innovation of using only on-policy samples to compute within-group baseline.", "function_signature": "def compute_grpo_outcome_advantage_split(token_level_rewards: torch.Tensor,\neos_mask: torch.Tensor,\nindex: torch.Tensor,\non_policy_mask: torch.Tensor,\nepsilon: float = 1e-6,\nuse_std: bool = True):", "input_parameters": "- `token_level_rewards`: Token-level reward tensor with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `eos_mask`: Valid token mask with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `index`: Sample grouping indices, n responses from the same prompt share the same index. **Note: This parameter should be a Python list or indexable sequence (e.g., `[0, 0, 1, 1]`), not `torch.Tensor`, as index values will be used as dictionary keys**\n- `on_policy_mask`: Boolean mask with shape `[batch_size]`, type `torch.Tensor`, marking which samples are on-policy generated\n- `epsilon`: Numerical stability constant, default 1e-6\n- `use_std`: Whether to use standard deviation for normalization, default True", "detailed_description": "This function needs to sum token-level reward tensors along the time dimension for each response sequence to obtain sequence-level total scores.\nThe main implementation performs grouped normalization (including both on-policy and off-policy) for multiple responses from the same prompt, computing relative advantage values. During normalization, only samples marked as on-policy are selected to compute the group's mean and standard deviation. If a group has only one on-policy sample, set mean to 0 and standard deviation to 1; if there are multiple samples, compute statistics normally. For cases where standard deviation is 0, replace it with 1 to avoid division by zero errors.\nIf standard deviation normalization is enabled, compute (score - mean) / (std + epsilon); otherwise only subtract the mean. Finally, expand sequence-level advantage values to token-level advantage tensors.", "output": "- `advantages`: Token-level advantage tensor with shape `[batch_size, response_length]`\n- `returns`: Token-level return tensor with shape `[batch_size, response_length]`, same as advantages", "func_implementation": "", "test_code_path": "code/test_code/test_compute_grpo_outcome_advantage_split.py", "implementation_location": "code/luffy/verl/verl/mix_src/mix_core_alg.py:line 12-67", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_grpo_outcome_advantage_split`.\n\n**Function Overview:**\nComputes group GRPO (Group Relative Policy Optimization) advantage estimation, with the key innovation of using only on-policy samples to compute within-group baseline.\n\n**Function Signature:**\n```python\ndef compute_grpo_outcome_advantage_split(token_level_rewards: torch.Tensor,\neos_mask: torch.Tensor,\nindex: torch.Tensor,\non_policy_mask: torch.Tensor,\nepsilon: float = 1e-6,\nuse_std: bool = True):\n```\n\n**Input Parameters:**\n- `token_level_rewards`: Token-level reward tensor with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `eos_mask`: Valid token mask with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `index`: Sample grouping indices, n responses from the same prompt share the same index. **Note: This parameter should be a Python list or indexable sequence (e.g., `[0, 0, 1, 1]`), not `torch.Tensor`, as index values will be used as dictionary keys**\n- `on_policy_mask`: Boolean mask with shape `[batch_size]`, type `torch.Tensor`, marking which samples are on-policy generated\n- `epsilon`: Numerical stability constant, default 1e-6\n- `use_std`: Whether to use standard deviation for normalization, default True\n\n**Detailed Description:**\nThis function needs to sum token-level reward tensors along the time dimension for each response sequence to obtain sequence-level total scores.\nThe main implementation performs grouped normalization (including both on-policy and off-policy) for multiple responses from the same prompt, computing relative advantage values. During normalization, only samples marked as on-policy are selected to compute the group's mean and standard deviation. If a group has only one on-policy sample, set mean to 0 and standard deviation to 1; if there are multiple samples, compute statistics normally. For cases where standard deviation is 0, replace it with 1 to avoid division by zero errors.\nIf standard deviation normalization is enabled, compute (score - mean) / (std + epsilon); otherwise only subtract the mean. Finally, expand sequence-level advantage values to token-level advantage tensors.\n\n**Expected Output:**\n- `advantages`: Token-level advantage tensor with shape `[batch_size, response_length]`\n- `returns`: Token-level return tensor with shape `[batch_size, response_length]`, same as advantages\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["Certainly! Below is the implementation of the `compute_grpo_outcome_advantage_split` function based on the provided specifications:\n\n```python\nimport torch\n\ndef compute_grpo_outcome_advantage_split(token_level_rewards: torch.Tensor,\n                                         eos_mask: torch.Tensor,\n                                         index: list,\n                                         on_policy_mask: torch.Tensor,\n                                         epsilon: float = e-,\n                                         use_std: bool = True):\n    \"\"\"\n    Computes group GRPO (Group Relative Policy Optimization) advantage estimation,\n    using only on-policy samples to compute within-group baseline.\n\n    Parameters:\n    - token_level_rewards: Token-level reward tensor with shape [batch_size, response_length]\n    - eos_mask: Valid token mask with shape [batch_size, response_length]\n    - index: Sample grouping indices, n responses from the same prompt share the same index\n    - on_policy_mask: Boolean mask with shape [batch_size], marking which samples are on-policy generated\n    - epsilon: Numerical stability constant, default e-\n    - use_std: Whether to use standard deviation for normalization, default True\n\n    Returns:\n    - advantages: Token-level advantage tensor with shape [batch_size, response_length]\n    - returns: Token-level return tensor with shape [batch_size, response_length], same as advantages\n    \"\"\"\n    batch_size, response_length = token_level_rewards.shape\n\n    # Step : Sum token-level rewards to get sequence-level total scores\n    sequence_level_rewards = (token_level_rewards * eos_mask).sum(dim=)\n\n    # Step : Group normalization\n    group_means =    group_stds =    for group_idx in set(index):\n        group_indices = [i for i, idx in enumerate(index) if idx == group_idx]\n        on_policy_indices = [i for i in group_indices if on_policy_mask[i]]\n\n        if len(on_policy_indices) == :\n            # No on-policy samples in this group, set mean to  and std to \n            group_means[group_idx] = .\n            group_stds[group_idx] = .\n        else:\n            # Compute mean and std using only on-policy samples\n            on_policy_rewards = sequence_level_rewards[on_policy_indices]\n            group_means[group_idx] = on_policy_rewards.mean().item()\n            group_stds[group_idx] = on_policy_rewards.std().item()\n\n            # Avoid division by zero\n            if group_stds[group_idx] < epsilon:\n                group_stds[group_idx] = .\n\n    # Step : Compute advantages and returns\n    advantages = torch.zeros_like(token_level_rewards)\n    returns = torch.zeros_like(token_level_rewards)\n\n    for i in range(batch_size):\n        group_idx = index[i]\n        mean = group_means[group_idx]\n        std = group_stds[group_idx]\n\n        if use_std:\n            advantage = (sequence_level_rewards[i] - mean) / (std + epsilon)\n        else:\n            advantage = sequence_level_rewards[i] - mean\n\n        # Expand sequence-level advantage to token-level\n        advantages[i] = advantage * eos_mask[i]\n        returns[i] = advantage * eos_mask[i]\n\n    return advantages, returns\n```\n\n### Explanation:\n. **Summing Rewards**: We first sum the token-level rewards for each sequence to get the sequence-level total scores.\n. **Group Normalization**:\n   - We iterate over each unique group index.\n   - For each group, we identify the indices of on-policy samples.\n   - If there are no on-policy samples, we set the mean to  and the standard deviation to .\n   - Otherwise, we compute the mean and standard deviation of the on-policy samples.\n   - We ensure numerical stability by setting the standard deviation to  if it is too small.\n. **Advantage Calculation**:\n   - For each sample, we compute the advantage using the group's mean and standard deviation.\n   - We expand the sequence-level advantage to token-level by multiplying with the `eos_mask`.\n. **Return Values**: The function returns the token-level advantage and return tensors, which are identical in this implementation.\n\nThis implementation ensures that the function adheres to the provided specifications and handles all edge cases appropriately."], "usage": {"prompt_tokens": 697, "completion_tokens": 880, "total_tokens": 1577, "per_completion": [{"prompt_tokens": 697, "completion_tokens": 880, "total_tokens": 1577}]}}
{"function_name": "compute_token_on_off_policy_loss", "overview": "Computes mixed policy loss, combining on-policy PPO clip loss and off-policy importance sampling loss, supporting multiple policy shaping methods to emphasize low-probability but critical actions.", "function_signature": "def compute_token_on_off_policy_loss(\nold_log_prob,\nlog_prob,\nadvantages,\neos_mask,\ncliprange,\nclip_upper_bound,\nprefix_mask,\noff_cliprange,\noff_normalize=False,\noff_abs_cliprange=None,\noff_max_clip=None,\noff_min_clip=None,\nall_max_clip=None,\noff_policy_reshape=\"no_reshape\",\noff_policy_reshape_weight=1.0,\noff_policy_reshape_pow_exp=0.5,\non_policy_reshape=\"no_reshape\",\non_policy_reshape_weight=1.0,\non_policy_reshape_pow_exp=0.5,\ntarget_probs=None,\nloss_remove_token_mean=False,\nloss_remove_clip=False,\n):", "input_parameters": "- `old_log_prob`: Old policy log probabilities with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `log_prob`: Current policy log probabilities with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `advantages`: Advantage estimates with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `eos_mask`: Valid token mask with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `cliprange`: PPO clip range, default 0.2\n- `clip_upper_bound`: PPO clip upper bound, default 100.0\n- `prefix_mask`: Mask marking off-policy tokens with shape `[batch_size, response_length]`, **type must be `torch.Tensor` with dtype `torch.bool`, as the code uses bitwise NOT operator `~prefix_mask` to obtain on-policy token mask**\n- `off_cliprange`: off-policy clip range (not used in current implementation)\n- `off_normalize`: Whether to normalize off-policy ratio, default False\n- `off_abs_cliprange`: off-policy absolute value clip range (not used in current implementation)\n- `off_max_clip`: off-policy ratio upper limit, None means no limit\n- `off_min_clip`: off-policy ratio lower limit, None means no limit\n- `all_max_clip`: Global probability upper limit, None means no limit\n- `off_policy_reshape`: off-policy policy shaping method name, default 'no_reshape'\n- `off_policy_reshape_weight`: Policy shaping weight parameter, default 1.0\n- `off_policy_reshape_pow_exp`: Power transformation exponent, default 0.5\n- `on_policy_reshape`: on-policy policy shaping method name, default 'no_reshape'\n- `on_policy_reshape_weight`: on-policy policy shaping weight, default 1.0\n- `on_policy_reshape_pow_exp`: on-policy power transformation exponent, default 0.5\n- `target_probs`: Optional target probabilities (for explicit behavior policy), default None\n- `loss_remove_token_mean`: Whether to remove token-level averaging, default False\n- `loss_remove_clip`: Whether to remove clip operation, default False", "detailed_description": "This function computes mixed policy gradient loss, with the core idea of simultaneously handling both on-policy and off-policy data. For on-policy data, it uses standard PPO algorithm to compute importance sampling ratio and perform clipping; for off-policy data, it directly uses current policy probabilities (or ratio relative to target probabilities) to compute loss, without relying on old policy.\nThe function distinguishes which tokens are off-policy and which are on-policy through prefix_mask, then combines the two losses with weighting. Additionally, the function should support multiple probability reshaping strategies and clipping mechanisms.", "output": "Returns dictionary containing the following key-value pairs:\n- `pg_loss`: Total policy gradient loss (scalar)\n- `off_pg_loss`: off-policy loss component (scalar)\n- `on_pg_loss`: on-policy loss component (scalar)\n- `off_pg_clipfrac`: off-policy clip fraction (scalar, currently 0 in implementation)\n- `on_pg_clipfrac`: on-policy clip fraction (scalar)\n- `ppo_kl`: PPO KL divergence (scalar)\n- `off_policy_prob`: Average probability of off-policy samples (scalar)\n- `on_policy_prob`: Average probability of on-policy samples (scalar)\n- `off_ratio_mean`: off-policy average ratio (scalar)\n- `off_ratio_max_clip_frac`: Fraction reaching upper clip limit (scalar)\n- `off_ratio_min_clip_frac`: Fraction reaching lower clip limit (scalar)", "func_implementation": "", "test_code_path": "code/test_code/test_compute_token_on_off_policy_loss.py", "implementation_location": "code/luffy/verl/verl/mix_src/mix_core_alg.py:line 69-251", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_token_on_off_policy_loss`.\n\n**Function Overview:**\nComputes mixed policy loss, combining on-policy PPO clip loss and off-policy importance sampling loss, supporting multiple policy shaping methods to emphasize low-probability but critical actions.\n\n**Function Signature:**\n```python\ndef compute_token_on_off_policy_loss(\nold_log_prob,\nlog_prob,\nadvantages,\neos_mask,\ncliprange,\nclip_upper_bound,\nprefix_mask,\noff_cliprange,\noff_normalize=False,\noff_abs_cliprange=None,\noff_max_clip=None,\noff_min_clip=None,\nall_max_clip=None,\noff_policy_reshape=\"no_reshape\",\noff_policy_reshape_weight=1.0,\noff_policy_reshape_pow_exp=0.5,\non_policy_reshape=\"no_reshape\",\non_policy_reshape_weight=1.0,\non_policy_reshape_pow_exp=0.5,\ntarget_probs=None,\nloss_remove_token_mean=False,\nloss_remove_clip=False,\n):\n```\n\n**Input Parameters:**\n- `old_log_prob`: Old policy log probabilities with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `log_prob`: Current policy log probabilities with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `advantages`: Advantage estimates with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `eos_mask`: Valid token mask with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `cliprange`: PPO clip range, default 0.2\n- `clip_upper_bound`: PPO clip upper bound, default 100.0\n- `prefix_mask`: Mask marking off-policy tokens with shape `[batch_size, response_length]`, **type must be `torch.Tensor` with dtype `torch.bool`, as the code uses bitwise NOT operator `~prefix_mask` to obtain on-policy token mask**\n- `off_cliprange`: off-policy clip range (not used in current implementation)\n- `off_normalize`: Whether to normalize off-policy ratio, default False\n- `off_abs_cliprange`: off-policy absolute value clip range (not used in current implementation)\n- `off_max_clip`: off-policy ratio upper limit, None means no limit\n- `off_min_clip`: off-policy ratio lower limit, None means no limit\n- `all_max_clip`: Global probability upper limit, None means no limit\n- `off_policy_reshape`: off-policy policy shaping method name, default 'no_reshape'\n- `off_policy_reshape_weight`: Policy shaping weight parameter, default 1.0\n- `off_policy_reshape_pow_exp`: Power transformation exponent, default 0.5\n- `on_policy_reshape`: on-policy policy shaping method name, default 'no_reshape'\n- `on_policy_reshape_weight`: on-policy policy shaping weight, default 1.0\n- `on_policy_reshape_pow_exp`: on-policy power transformation exponent, default 0.5\n- `target_probs`: Optional target probabilities (for explicit behavior policy), default None\n- `loss_remove_token_mean`: Whether to remove token-level averaging, default False\n- `loss_remove_clip`: Whether to remove clip operation, default False\n\n**Detailed Description:**\nThis function computes mixed policy gradient loss, with the core idea of simultaneously handling both on-policy and off-policy data. For on-policy data, it uses standard PPO algorithm to compute importance sampling ratio and perform clipping; for off-policy data, it directly uses current policy probabilities (or ratio relative to target probabilities) to compute loss, without relying on old policy.\nThe function distinguishes which tokens are off-policy and which are on-policy through prefix_mask, then combines the two losses with weighting. Additionally, the function should support multiple probability reshaping strategies and clipping mechanisms.\n\n**Expected Output:**\nReturns dictionary containing the following key-value pairs:\n- `pg_loss`: Total policy gradient loss (scalar)\n- `off_pg_loss`: off-policy loss component (scalar)\n- `on_pg_loss`: on-policy loss component (scalar)\n- `off_pg_clipfrac`: off-policy clip fraction (scalar, currently 0 in implementation)\n- `on_pg_clipfrac`: on-policy clip fraction (scalar)\n- `ppo_kl`: PPO KL divergence (scalar)\n- `off_policy_prob`: Average probability of off-policy samples (scalar)\n- `on_policy_prob`: Average probability of on-policy samples (scalar)\n- `off_ratio_mean`: off-policy average ratio (scalar)\n- `off_ratio_max_clip_frac`: Fraction reaching upper clip limit (scalar)\n- `off_ratio_min_clip_frac`: Fraction reaching lower clip limit (scalar)\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["Certainly! Below is the implementation of the `compute_token_on_off_policy_loss` function based on the provided specifications and detailed description. The function handles both on-policy and off-policy losses, applies various reshaping methods, and computes the necessary statistics.\n\n```python\nimport torch\n\ndef compute_token_on_off_policy_loss(\n    old_log_prob,\n    log_prob,\n    advantages,\n    eos_mask,\n    cliprange=.,\n    clip_upper_bound=.,\n    prefix_mask=None,\n    off_cliprange=None,\n    off_normalize=False,\n    off_abs_cliprange=None,\n    off_max_clip=None,\n    off_min_clip=None,\n    all_max_clip=None,\n    off_policy_reshape=\"no_reshape\",\n    off_policy_reshape_weight=.,\n    off_policy_reshape_pow_exp=.,\n    on_policy_reshape=\"no_reshape\",\n    on_policy_reshape_weight=.,\n    on_policy_reshape_pow_exp=.,\n    target_probs=None,\n    loss_remove_token_mean=False,\n    loss_remove_clip=False,\n):\n    # Ensure prefix_mask is a boolean tensor\n    if prefix_mask is None:\n        prefix_mask = torch.zeros_like(old_log_prob, dtype=torch.bool)\n    else:\n        assert prefix_mask.dtype == torch.bool, \"prefix_mask must be a torch.Tensor with dtype torch.bool\"\n\n    # Compute probabilities\n    old_prob = torch.exp(old_log_prob)\n    prob = torch.exp(log_prob)\n\n    # Apply global probability upper limit\n    if all_max_clip is not None:\n        old_prob = torch.clamp(old_prob, max=all_max_clip)\n        prob = torch.clamp(prob, max=all_max_clip)\n\n    # Compute ratios\n    ratio = prob / old_prob\n\n    # Off-policy mask\n    off_policy_mask = prefix_mask\n    # On-policy mask\n    on_policy_mask = ~off_policy_mask\n\n    # Off-policy loss computation\n    if target_probs is not None:\n        off_ratio = prob / target_probs\n    else:\n        off_ratio = ratio\n\n    # Apply off-policy clipping\n    if off_max_clip is not None:\n        off_ratio = torch.clamp(off_ratio, max=off_max_clip)\n    if off_min_clip is not None:\n        off_ratio = torch.clamp(off_ratio, min=off_min_clip)\n\n    # Off-policy loss\n    off_policy_loss = off_ratio * advantages\n    if not loss_remove_clip:\n        off_policy_loss = torch.clamp(off_policy_loss, min=. - cliprange, max=. + cliprange)\n\n    # Apply off-policy reshaping\n    if off_policy_reshape == \"pow\":\n        off_policy_loss = torch.pow(off_policy_loss, off_policy_reshape_pow_exp)\n    off_policy_loss = off_policy_loss * off_policy_reshape_weight\n\n    # On-policy loss computation\n    on_policy_loss = ratio * advantages\n    if not loss_remove_clip:\n        on_policy_loss = torch.clamp(on_policy_loss, min=. - cliprange, max=. + cliprange)\n\n    # Apply on-policy reshaping\n    if on_policy_reshape == \"pow\":\n        on_policy_loss = torch.pow(on_policy_loss, on_policy_reshape_pow_exp)\n    on_policy_loss = on_policy_loss * on_policy_reshape_weight\n\n    # Mask the losses\n    off_policy_loss = off_policy_loss * off_policy_mask\n    on_policy_loss = on_policy_loss * on_policy_mask\n\n    # Compute mean loss\n    if not loss_remove_token_mean:\n        off_policy_loss = off_policy_loss.sum() / (eos_mask.sum() + e-)\n        on_policy_loss = on_policy_loss.sum() / (eos_mask.sum() + e-)\n    else:\n        off_policy_loss = off_policy_loss.sum()\n        on_policy_loss = on_policy_loss.sum()\n\n    # Total policy gradient loss\n    pg_loss = off_policy_loss + on_policy_loss\n\n    # Compute clip fractions\n    on_policy_clipfrac = ((ratio < . - cliprange) | (ratio > . + cliprange)).float().mean().item()\n    off_policy_clipfrac = .  # Currently not used in implementation\n\n    # Compute PPO KL divergence\n    ppo_kl = (old_prob * (old_log_prob - log_prob)).sum() / (eos_mask.sum() + e-)\n\n    # Compute average probabilities\n    off_policy_prob = (prob * off_policy_mask).sum() / (off_policy_mask.sum() + e-)\n    on_policy_prob = (prob * on_policy_mask).sum() / (on_policy_mask.sum() + e-)\n\n    # Compute off-policy ratio statistics\n    off_ratio_mean = (off_ratio * off_policy_mask).sum() / (off_policy_mask.sum() + e-)\n    off_ratio_max_clip_frac = (off_ratio == off_max_clip).float().mean().item() if off_max_clip is not None else .\n    off_ratio_min_clip_frac = (off_ratio == off_min_clip).float().mean().item() if off_min_clip is not None else .\n\n    # Return the results in a dictionary\n    return {\n        \"pg_loss\": pg_loss.item(),\n        \"off_pg_loss\": off_policy_loss.item(),\n        \"on_pg_loss\": on_policy_loss.item(),\n        \"off_pg_clipfrac\": off_policy_clipfrac,\n        \"on_pg_clipfrac\": on_policy_clipfrac,\n        \"ppo_kl\": ppo_kl.item(),\n        \"off_policy_prob\": off_policy_prob.item(),\n        \"on_policy_prob\": on_policy_prob.item(),\n        \"off_ratio_mean\": off_ratio_mean.item(),\n        \"off_ratio_max_clip_frac\": off_ratio_max_clip_frac,\n        \"off_ratio_min_clip_frac\": off_ratio_min_clip_frac,\n    }\n```\n\n### Explanation:\n. **Probability Calculation**: The probabilities are computed from the log probabilities.\n. **Global Probability Clipping**: If `all_max_clip` is provided, probabilities are clipped to this value.\n. **Ratio Calculation**: The importance sampling ratio is calculated.\n. **Off-Policy Loss Calculation**:\n   - If `target_probs` are provided, the off-policy ratio is computed relative to these target probabilities.\n   - Off-policy ratios are clipped if `off_max_clip` or `off_min_clip` are provided.\n   - Off-policy loss is computed and reshaped if specified.\n. **On-Policy Loss Calculation**:\n   - On-policy loss is computed and reshaped if specified.\n. **Masking**: Losses are masked according to whether they are on-policy or off-policy.\n. **Mean Loss Calculation**: If `loss_remove_token_mean` is `False`, the mean loss is computed over valid tokens.\n. **Clip Fractions**: The fraction of clipped on-policy ratios is computed.\n. **PPO KL Divergence**: The KL divergence between old and new policies is computed.\n. **Average Probabilities**: The average probabilities of on-policy and off-policy samples are computed.\n. **Off-Policy Ratio Statistics**: Mean and clipping fractions of off-policy ratios are computed.\n\nThis implementation ensures that all specified parameters and logic are correctly handled."], "usage": {"prompt_tokens": 1141, "completion_tokens": 1522, "total_tokens": 2663, "per_completion": [{"prompt_tokens": 1141, "completion_tokens": 1522, "total_tokens": 2663}]}}
{"function_name": "compute_sft_pure_loss", "overview": "Computes pure SFT (Supervised Fine-Tuning) loss, i.e., negative log likelihood loss, used for supervised learning on off-policy samples in multi-task learning.", "function_signature": "", "input_parameters": "- `log_prob`: Model-predicted log probabilities with shape `[batch_size, response_length]`\n- `eos_mask`: Valid token mask with shape `[batch_size, response_length]`", "detailed_description": "Computes negative log probability for each token as loss value. Uses eos_mask to mask the loss, computing loss only at valid token positions. Averages the masked loss to obtain batch-level SFT loss.", "output": "- `sft_loss`: Scalar representing average SFT loss", "func_implementation": "", "test_code_path": "code/test_code/test_compute_sft_pure_loss.py", "implementation_location": "code/luffy/verl/verl/mix_src/mix_core_alg.py:line 7-10", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_sft_pure_loss`.\n\n**Function Overview:**\nComputes pure SFT (Supervised Fine-Tuning) loss, i.e., negative log likelihood loss, used for supervised learning on off-policy samples in multi-task learning.\n\n**Function Signature:**\n```python\n\n```\n\n**Input Parameters:**\n- `log_prob`: Model-predicted log probabilities with shape `[batch_size, response_length]`\n- `eos_mask`: Valid token mask with shape `[batch_size, response_length]`\n\n**Detailed Description:**\nComputes negative log probability for each token as loss value. Uses eos_mask to mask the loss, computing loss only at valid token positions. Averages the masked loss to obtain batch-level SFT loss.\n\n**Expected Output:**\n- `sft_loss`: Scalar representing average SFT loss\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["Certainly! Below is the implementation of the `compute_sft_pure_loss` function based on the provided specifications:\n\n```python\nimport torch\n\ndef compute_sft_pure_loss(log_prob: torch.Tensor, eos_mask: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Computes the pure SFT (Supervised Fine-Tuning) loss, i.e., negative log likelihood loss,\n    used for supervised learning on off-policy samples in multi-task learning.\n\n    Parameters:\n    - log_prob: Model-predicted log probabilities with shape [batch_size, response_length]\n    - eos_mask: Valid token mask with shape [batch_size, response_length]\n\n    Returns:\n    - sft_loss: Scalar representing average SFT loss\n    \"\"\"\n    # Ensure the log_prob and eos_mask have the same shape\n    assert log_prob.shape == eos_mask.shape, \"log_prob and eos_mask must have the same shape\"\n\n    # Compute the negative log likelihood loss for each token\n    nll_loss = -log_prob\n\n    # Mask the loss using eos_mask\n    masked_loss = nll_loss * eos_mask\n\n    # Sum the masked loss over the response_length dimension\n    masked_loss_sum = masked_loss.sum(dim=)\n\n    # Count the number of valid tokens for each sample in the batch\n    valid_token_count = eos_mask.sum(dim=)\n\n    # Compute the average loss per sample\n    # Avoid division by zero by clamping the valid_token_count to be at least \n    average_loss_per_sample = masked_loss_sum / valid_token_count.clamp(min=)\n\n    # Compute the batch-level average SFT loss\n    sft_loss = average_loss_per_sample.mean()\n\n    return sft_loss\n```\n\n### Explanation:\n. **Input Validation**: The function first checks if `log_prob` and `eos_mask` have the same shape to ensure they can be element-wise multiplied.\n. **Negative Log Likelihood**: The negative log likelihood (NLL) is computed by negating the `log_prob`.\n. **Masking**: The NLL is then masked using `eos_mask` to consider only the valid tokens.\n. **Summation and Counting**: The masked loss is summed over the `response_length` dimension, and the number of valid tokens is counted for each sample.\n. **Average Loss Calculation**: The average loss per sample is computed by dividing the masked loss sum by the number of valid tokens. Clamping is used to avoid division by zero.\n. **Batch-Level Average Loss**: Finally, the batch-level average SFT loss is computed by taking the mean of the average losses per sample.\n\nThis function should work efficiently for the given task and handle the input parameters as specified."], "usage": {"prompt_tokens": 363, "completion_tokens": 561, "total_tokens": 924, "per_completion": [{"prompt_tokens": 363, "completion_tokens": 561, "total_tokens": 924}]}}
