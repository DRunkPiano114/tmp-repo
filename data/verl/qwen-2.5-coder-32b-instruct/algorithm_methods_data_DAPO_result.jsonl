{"function_name": "agg_loss", "overview": "Aggregates loss matrix to scalar, supporting multiple aggregation modes. DAPO recommends using `token-mean` mode, making each token's contribution to loss equal, avoiding long sequences dominating gradients.", "function_signature": "def agg_loss(loss_mat: torch.Tensor, loss_mask: torch.Tensor, loss_agg_mode: str):", "input_parameters": "- `loss_mat`: Loss matrix with shape `(batch_size, response_length)`\n- `loss_mask`: Mask tensor with shape `(batch_size, response_length)` (response part is 1, padding is 0)\n- `loss_agg_mode`: String specifying aggregation mode\n- `\"token-mean\"`: Average over all tokens (**DAPO recommended**)\n- `\"seq-mean-token-sum\"`: Sum tokens within sequences first, then average across sequences\n- `\"seq-mean-token-mean\"`: Average tokens within sequences first, then average across sequences\n- `\"seq-mean-token-sum-norm\"`: Sum sequences then normalize by sequence length", "detailed_description": "The core function is to aggregate a 2D loss matrix into a single scalar value, supporting the following four aggregation strategies: token-mean, seq-mean-token-sum, seq-mean-token-mean, seq-mean-token-sum-norm.", "output": "- Scalar loss value for backpropagation", "func_implementation": "", "test_code_path": "code/test_code/test_agg_loss.py", "implementation_location": "code/verl/trainer/ppo/core_algos.py:line 704-737", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `agg_loss`.\n\n**Function Overview:**\nAggregates loss matrix to scalar, supporting multiple aggregation modes. DAPO recommends using `token-mean` mode, making each token's contribution to loss equal, avoiding long sequences dominating gradients.\n\n**Function Signature:**\n```python\ndef agg_loss(loss_mat: torch.Tensor, loss_mask: torch.Tensor, loss_agg_mode: str):\n```\n\n**Input Parameters:**\n- `loss_mat`: Loss matrix with shape `(batch_size, response_length)`\n- `loss_mask`: Mask tensor with shape `(batch_size, response_length)` (response part is 1, padding is 0)\n- `loss_agg_mode`: String specifying aggregation mode\n- `\"token-mean\"`: Average over all tokens (**DAPO recommended**)\n- `\"seq-mean-token-sum\"`: Sum tokens within sequences first, then average across sequences\n- `\"seq-mean-token-mean\"`: Average tokens within sequences first, then average across sequences\n- `\"seq-mean-token-sum-norm\"`: Sum sequences then normalize by sequence length\n\n**Detailed Description:**\nThe core function is to aggregate a 2D loss matrix into a single scalar value, supporting the following four aggregation strategies: token-mean, seq-mean-token-sum, seq-mean-token-mean, seq-mean-token-sum-norm.\n\n**Expected Output:**\n- Scalar loss value for backpropagation\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["Certainly! Below is the implementation of the `agg_loss` function based on the provided specifications:\n\n```python\nimport torch\n\ndef agg_loss(loss_mat: torch.Tensor, loss_mask: torch.Tensor, loss_agg_mode: str) -> torch.Tensor:\n    \"\"\"\n    Aggregates loss matrix to scalar based on the specified aggregation mode.\n\n    Parameters:\n    - loss_mat: Loss matrix with shape (batch_size, response_length)\n    - loss_mask: Mask tensor with shape (batch_size, response_length) (response part is , padding is )\n    - loss_agg_mode: String specifying aggregation mode\n        - \"token-mean\": Average over all tokens (DAPO recommended)\n        - \"seq-mean-token-sum\": Sum tokens within sequences first, then average across sequences\n        - \"seq-mean-token-mean\": Average tokens within sequences first, then average across sequences\n        - \"seq-mean-token-sum-norm\": Sum sequences then normalize by sequence length\n\n    Returns:\n    - Scalar loss value for backpropagation\n    \"\"\"\n    # Ensure the loss matrix and mask have the same shape\n    assert loss_mat.shape == loss_mask.shape, \"loss_mat and loss_mask must have the same shape\"\n\n    if loss_agg_mode == \"token-mean\":\n        # Average over all tokens\n        # Mask the loss matrix to ignore padding tokens\n        masked_loss = loss_mat * loss_mask\n        # Sum all the masked losses\n        total_loss = masked_loss.sum()\n        # Count the number of non-padded tokens\n        num_tokens = loss_mask.sum()\n        # Compute the mean loss\n        loss = total_loss / num_tokens\n\n    elif loss_agg_mode == \"seq-mean-token-sum\":\n        # Sum tokens within sequences first, then average across sequences\n        # Mask the loss matrix to ignore padding tokens\n        masked_loss = loss_mat * loss_mask\n        # Sum tokens within each sequence\n        seq_loss_sum = masked_loss.sum(dim=)\n        # Compute the mean loss across sequences\n        loss = seq_loss_sum.mean()\n\n    elif loss_agg_mode == \"seq-mean-token-mean\":\n        # Average tokens within sequences first, then average across sequences\n        # Mask the loss matrix to ignore padding tokens\n        masked_loss = loss_mat * loss_mask\n        # Sum tokens within each sequence\n        seq_loss_sum = masked_loss.sum(dim=)\n        # Count the number of non-padded tokens in each sequence\n        seq_token_count = loss_mask.sum(dim=)\n        # Compute the mean loss within each sequence\n        seq_loss_mean = seq_loss_sum / seq_token_count\n        # Compute the mean loss across sequences\n        loss = seq_loss_mean.mean()\n\n    elif loss_agg_mode == \"seq-mean-token-sum-norm\":\n        # Sum sequences then normalize by sequence length\n        # Mask the loss matrix to ignore padding tokens\n        masked_loss = loss_mat * loss_mask\n        # Sum tokens within each sequence\n        seq_loss_sum = masked_loss.sum(dim=)\n        # Count the number of non-padded tokens in each sequence\n        seq_token_count = loss_mask.sum(dim=)\n        # Normalize the sum of each sequence by its length\n        seq_loss_norm = seq_loss_sum / seq_token_count\n        # Compute the mean loss across sequences\n        loss = seq_loss_norm.mean()\n\n    else:\n        raise ValueError(f\"Unsupported loss aggregation mode: {loss_agg_mode}\")\n\n    return loss\n```\n\n### Explanation:\n- **Input Validation**: The function first checks if `loss_mat` and `loss_mask` have the same shape.\n- **Aggregation Modes**:\n  - **\"token-mean\"**: Computes the mean of all non-padded tokens.\n  - **\"seq-mean-token-sum\"**: Sums the losses within each sequence and then computes the mean across sequences.\n  - **\"seq-mean-token-mean\"**: Computes the mean of losses within each sequence and then computes the mean across sequences.\n  - **\"seq-mean-token-sum-norm\"**: Sums the losses within each sequence, normalizes by the sequence length, and then computes the mean across sequences.\n- **Error Handling**: Raises a `ValueError` if an unsupported aggregation mode is provided.\n\nThis implementation ensures that the loss is aggregated correctly based on the specified mode, and it handles the input parameters and output as expected."], "usage": {"prompt_tokens": 471, "completion_tokens": 891, "total_tokens": 1362, "per_completion": [{"prompt_tokens": 471, "completion_tokens": 891, "total_tokens": 1362}]}, "results": [false], "pass_ratios": [0.0]}
{"function_name": "compute_policy_loss_vanilla", "overview": "Computes PPO policy gradient loss, supporting decoupled clipping (DAPO's core innovation). Through `clip_ratio_low` and `clip_ratio_high` parameters, allows asymmetric clipping for positive and negative advantages, implementing the \"Clip-Higher\" mechanism.", "function_signature": "def compute_policy_loss_vanilla(\nold_log_prob: torch.Tensor,\nlog_prob: torch.Tensor,\nadvantages: torch.Tensor,\nresponse_mask: torch.Tensor,\nloss_agg_mode: str = \"token-mean\",\nconfig: Optional[DictConfig | AlgoConfig] = None,\nrollout_log_probs: torch.Tensor | None = None,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:", "input_parameters": "- `old_log_prob`: Old policy log probabilities with shape `(batch_size, response_length)`\n- `log_prob`: Current policy log probabilities with shape `(batch_size, response_length)`\n- `advantages`: Advantage estimates with shape `(batch_size, response_length)`\n- `response_mask`: Response mask with shape `(batch_size, response_length)`\n- `loss_agg_mode`: Loss aggregation mode (default is token-mean)\n- `config`: Actor configuration object, containing:\n- `clip_ratio`: Default clipping parameter (backward compatibility)\n- `clip_ratio_low`: Lower bound clipping parameter (**DAPO new**)\n- `clip_ratio_high`: Upper bound clipping parameter (**DAPO new**)\n- `clip_ratio_c`: Dual clipping lower bound (handles extreme negative advantages, default 3.0)\n- `rollout_log_probs`: Optional, Rollout policy log probabilities (for off-policy training)", "detailed_description": "The function implements enhanced PPO policy loss computation, including standard PPO clipped loss calculation, dual-clip PPO, and truncated importance sampling (optional), ultimately calling related functions to implement loss aggregation.\nStandard PPO clipping mechanism supports asymmetric clipping, requiring computation of unclipped original policy gradient loss and asymmetrically clipped loss, taking the maximum of both as pessimistic update strategy. When advantage value is negative, apply dual clipping to more conservatively reduce the probability of that action. If truncated importance sampling is enabled and rollout policy log probabilities are provided, the function computes the importance ratio of old policy relative to rollout policy, clips it to the configured upper limit, then multiplies it into the policy gradient loss.", "output": "Returns four scalar tensors:\n- `pg_loss`: Policy gradient loss\n- `pg_clipfrac`: Clipping fraction (proportion of clipped tokens, for monitoring training dynamics)\n- `ppo_kl`: Approximate KL divergence\n- `pg_clipfrac_lower`: Lower bound clipping fraction (proportion of tokens where dual clipping takes effect)", "func_implementation": "", "test_code_path": "code/test_code/test_compute_policy_loss_vanilla.py", "implementation_location": "code/verl/trainer/ppo/core_algos.py:line 816-902", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_policy_loss_vanilla`.\n\n**Function Overview:**\nComputes PPO policy gradient loss, supporting decoupled clipping (DAPO's core innovation). Through `clip_ratio_low` and `clip_ratio_high` parameters, allows asymmetric clipping for positive and negative advantages, implementing the \"Clip-Higher\" mechanism.\n\n**Function Signature:**\n```python\ndef compute_policy_loss_vanilla(\nold_log_prob: torch.Tensor,\nlog_prob: torch.Tensor,\nadvantages: torch.Tensor,\nresponse_mask: torch.Tensor,\nloss_agg_mode: str = \"token-mean\",\nconfig: Optional[DictConfig | AlgoConfig] = None,\nrollout_log_probs: torch.Tensor | None = None,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n```\n\n**Input Parameters:**\n- `old_log_prob`: Old policy log probabilities with shape `(batch_size, response_length)`\n- `log_prob`: Current policy log probabilities with shape `(batch_size, response_length)`\n- `advantages`: Advantage estimates with shape `(batch_size, response_length)`\n- `response_mask`: Response mask with shape `(batch_size, response_length)`\n- `loss_agg_mode`: Loss aggregation mode (default is token-mean)\n- `config`: Actor configuration object, containing:\n- `clip_ratio`: Default clipping parameter (backward compatibility)\n- `clip_ratio_low`: Lower bound clipping parameter (**DAPO new**)\n- `clip_ratio_high`: Upper bound clipping parameter (**DAPO new**)\n- `clip_ratio_c`: Dual clipping lower bound (handles extreme negative advantages, default 3.0)\n- `rollout_log_probs`: Optional, Rollout policy log probabilities (for off-policy training)\n\n**Detailed Description:**\nThe function implements enhanced PPO policy loss computation, including standard PPO clipped loss calculation, dual-clip PPO, and truncated importance sampling (optional), ultimately calling related functions to implement loss aggregation.\nStandard PPO clipping mechanism supports asymmetric clipping, requiring computation of unclipped original policy gradient loss and asymmetrically clipped loss, taking the maximum of both as pessimistic update strategy. When advantage value is negative, apply dual clipping to more conservatively reduce the probability of that action. If truncated importance sampling is enabled and rollout policy log probabilities are provided, the function computes the importance ratio of old policy relative to rollout policy, clips it to the configured upper limit, then multiplies it into the policy gradient loss.\n\n**Expected Output:**\nReturns four scalar tensors:\n- `pg_loss`: Policy gradient loss\n- `pg_clipfrac`: Clipping fraction (proportion of clipped tokens, for monitoring training dynamics)\n- `ppo_kl`: Approximate KL divergence\n- `pg_clipfrac_lower`: Lower bound clipping fraction (proportion of tokens where dual clipping takes effect)\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["Certainly! Below is the implementation of the `compute_policy_loss_vanilla` function based on the provided specifications and detailed description:\n\n```python\nimport torch\nfrom typing import Optional, Dict\nfrom omegaconf import DictConfig\n\ndef compute_policy_loss_vanilla(\n    old_log_prob: torch.Tensor,\n    log_prob: torch.Tensor,\n    advantages: torch.Tensor,\n    response_mask: torch.Tensor,\n    loss_agg_mode: str = \"token-mean\",\n    config: Optional[DictConfig] = None,\n    rollout_log_probs: torch.Tensor = None,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Computes the PPO policy gradient loss with decoupled clipping (DAPO's core innovation).\n\n    Parameters:\n    - old_log_prob: Old policy log probabilities with shape (batch_size, response_length)\n    - log_prob: Current policy log probabilities with shape (batch_size, response_length)\n    - advantages: Advantage estimates with shape (batch_size, response_length)\n    - response_mask: Response mask with shape (batch_size, response_length)\n    - loss_agg_mode: Loss aggregation mode (default is token-mean)\n    - config: Actor configuration object, containing:\n        - clip_ratio: Default clipping parameter (backward compatibility)\n        - clip_ratio_low: Lower bound clipping parameter (DAPO new)\n        - clip_ratio_high: Upper bound clipping parameter (DAPO new)\n        - clip_ratio_c: Dual clipping lower bound (handles extreme negative advantages, default .)\n        - rollout_log_probs: Optional, Rollout policy log probabilities (for off-policy training)\n\n    Returns:\n    - pg_loss: Policy gradient loss\n    - pg_clipfrac: Clipping fraction (proportion of clipped tokens, for monitoring training dynamics)\n    - ppo_kl: Approximate KL divergence\n    - pg_clipfrac_lower: Lower bound clipping fraction (proportion of tokens where dual clipping takes effect)\n    \"\"\"\n    if config is None:\n        raise ValueError(\"Config must be provided\")\n\n    # Extract configuration parameters\n    clip_ratio = config.get(\"clip_ratio\", .)\n    clip_ratio_low = config.get(\"clip_ratio_low\", -clip_ratio)\n    clip_ratio_high = config.get(\"clip_ratio_high\", clip_ratio)\n    clip_ratio_c = config.get(\"clip_ratio_c\", .)\n\n    # Compute the log ratio and importance ratio\n    log_ratio = log_prob - old_log_prob\n    if rollout_log_probs is not None:\n        rollout_log_ratio = old_log_prob - rollout_log_probs\n        importance_ratio = torch.exp(rollout_log_ratio)\n        importance_ratio_clipped = torch.clamp(importance_ratio, max=clip_ratio_high)\n        log_ratio = log_ratio * importance_ratio_clipped\n\n    # Compute the unclipped policy gradient loss\n    unclipped_loss = log_ratio * advantages\n\n    # Compute the clipped policy gradient loss\n    clipped_loss = torch.clamp(log_ratio, clip_ratio_low, clip_ratio_high) * advantages\n\n    # Compute the final policy gradient loss\n    pg_loss = -torch.min(unclipped_loss, clipped_loss)\n\n    # Apply response mask\n    pg_loss = pg_loss * response_mask\n\n    # Compute the clipping fraction\n    clipped = (unclipped_loss != clipped_loss).float()\n    pg_clipfrac = clipped.mean()\n\n    # Compute the lower bound clipping fraction\n    clipped_lower = (log_ratio < clip_ratio_low).float()\n    pg_clipfrac_lower = clipped_lower.mean()\n\n    # Compute the approximate KL divergence\n    ppo_kl = (old_log_prob - log_prob).mean()\n\n    # Aggregate the loss based on the specified mode\n    if loss_agg_mode == \"token-mean\":\n        pg_loss = pg_loss.sum() / response_mask.sum()\n    elif loss_agg_mode == \"batch-mean\":\n        pg_loss = pg_loss.mean()\n    else:\n        raise ValueError(f\"Unsupported loss aggregation mode: {loss_agg_mode}\")\n\n    return pg_loss, pg_clipfrac, ppo_kl, pg_clipfrac_lower\n```\n\n### Explanation:\n. **Configuration Handling**: The function first checks if the `config` is provided and extracts necessary parameters such as `clip_ratio`, `clip_ratio_low`, `clip_ratio_high`, and `clip_ratio_c`.\n\n. **Log Ratio Calculation**: The log ratio is calculated as the difference between the current and old log probabilities. If `rollout_log_probs` are provided, the importance ratio is computed and clipped to `clip_ratio_high` before being multiplied into the log ratio.\n\n. **Loss Calculation**:\n   - **Unclipped Loss**: The unclipped policy gradient loss is computed as the product of the log ratio and advantages.\n   - **Clipped Loss**: The clipped policy gradient loss is computed using the `torch.clamp` function to apply the asymmetric clipping.\n   - **Final Loss**: The final policy gradient loss is the minimum of the unclipped and clipped losses, negated to form a loss function.\n\n. **Masking and Clipping Fractions**: The response mask is applied to the loss, and clipping fractions are computed to monitor training dynamics.\n\n. **KL Divergence**: The approximate KL divergence is computed as the mean difference between the old and current log probabilities.\n\n. **Loss Aggregation**: The loss is aggregated based on the specified mode (`token-mean` or `batch-mean`).\n\nThis implementation ensures that all specified features and configurations are handled correctly, and the function returns the expected outputs."], "usage": {"prompt_tokens": 763, "completion_tokens": 1136, "total_tokens": 1899, "per_completion": [{"prompt_tokens": 763, "completion_tokens": 1136, "total_tokens": 1899}]}, "results": [false], "pass_ratios": [0.0]}
{"function_name": "DAPORewardManager.__call__", "overview": "DAPO's reward manager, responsible for computing task rewards and applying overlong penalty. This is one of the key components distinguishing DAPO from other methods, preventing the model from generating meaningless lengthy outputs through a linear penalty mechanism.", "function_signature": "def __call__(self, data: DataProto, return_dict: bool = False):", "input_parameters": "- `data`: DataProto object containing:\n- `batch[\"prompts\"]`: Prompt token IDs with shape `(bs, prompt_len)`\n- `batch[\"responses\"]`: Response token IDs with shape `(bs, response_len)`\n- `batch[\"attention_mask\"]`: Attention mask with shape `(bs, total_len)`\n- `non_tensor_batch[\"reward_model\"][\"ground_truth\"]`: Ground truth answers\n- `non_tensor_batch[\"data_source\"]`: Data source (e.g., \"gsm8k\", \"math\")\n- `return_dict`: Boolean, whether to return dictionary format (including additional information like accuracy, overlong penalty, etc.)", "detailed_description": "The function is responsible for computing reward scores for generated responses. First checks if there are pre-computed rewards; if so, returns directly; otherwise processes each response individually, decoding token sequences into text and computing task rewards based on ground truth answers (needs to call related functional functions). Computed scores are placed at the last valid token position in the response sequence. If length control is enabled, the function applies additional negative reward penalty for overlong responses (linear penalty mechanism). Throughout the process, detailed information for some samples is output for debugging verification, ultimately returning complete reward tensor or dictionary containing additional evaluation metrics.", "output": "- If `return_dict=False`: Returns sparse reward tensor with shape `(batch_size, response_length)` (only last valid token position is non-zero)\n- If `return_dict=True`: Returns dictionary containing:\n- `\"reward_tensor\"`: Reward tensor as above\n- `\"reward_extra_info\"`: Dictionary containing `\"acc\"` (accuracy list), `\"overlong_reward\"` (overlong penalty list), `\"overlong\"` (boolean list indicating if overlong), etc.", "func_implementation": "", "test_code_path": "code/test_code/test_dapo_reward_manager.py", "implementation_location": "code/verl/workers/reward_manager/dapo.py:line 53-149", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `DAPORewardManager.__call__`.\n\n**Function Overview:**\nDAPO's reward manager, responsible for computing task rewards and applying overlong penalty. This is one of the key components distinguishing DAPO from other methods, preventing the model from generating meaningless lengthy outputs through a linear penalty mechanism.\n\n**Function Signature:**\n```python\ndef __call__(self, data: DataProto, return_dict: bool = False):\n```\n\n**Input Parameters:**\n- `data`: DataProto object containing:\n- `batch[\"prompts\"]`: Prompt token IDs with shape `(bs, prompt_len)`\n- `batch[\"responses\"]`: Response token IDs with shape `(bs, response_len)`\n- `batch[\"attention_mask\"]`: Attention mask with shape `(bs, total_len)`\n- `non_tensor_batch[\"reward_model\"][\"ground_truth\"]`: Ground truth answers\n- `non_tensor_batch[\"data_source\"]`: Data source (e.g., \"gsm8k\", \"math\")\n- `return_dict`: Boolean, whether to return dictionary format (including additional information like accuracy, overlong penalty, etc.)\n\n**Detailed Description:**\nThe function is responsible for computing reward scores for generated responses. First checks if there are pre-computed rewards; if so, returns directly; otherwise processes each response individually, decoding token sequences into text and computing task rewards based on ground truth answers (needs to call related functional functions). Computed scores are placed at the last valid token position in the response sequence. If length control is enabled, the function applies additional negative reward penalty for overlong responses (linear penalty mechanism). Throughout the process, detailed information for some samples is output for debugging verification, ultimately returning complete reward tensor or dictionary containing additional evaluation metrics.\n\n**Expected Output:**\n- If `return_dict=False`: Returns sparse reward tensor with shape `(batch_size, response_length)` (only last valid token position is non-zero)\n- If `return_dict=True`: Returns dictionary containing:\n- `\"reward_tensor\"`: Reward tensor as above\n- `\"reward_extra_info\"`: Dictionary containing `\"acc\"` (accuracy list), `\"overlong_reward\"` (overlong penalty list), `\"overlong\"` (boolean list indicating if overlong), etc.\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["Certainly! Below is the implementation of the `DAPORewardManager.__call__` function based on the provided specifications. This function processes the input data to compute rewards for generated responses, applies overlong penalties if necessary, and returns the results in the specified format.\n\n```python\nimport torch\nfrom typing import Dict, List, Union\n\nclass DataProto:\n    def __init__(self, batch: Dict[str, torch.Tensor], non_tensor_batch: Dict[str, Union[Dict, str]]):\n        self.batch = batch\n        self.non_tensor_batch = non_tensor_batch\n\nclass DAPORewardManager:\n    def __init__(self, tokenizer, reward_model, length_control: bool = False, max_length: int = , penalty_factor: float = .):\n        \"\"\"\n        Initialize the DAPORewardManager.\n\n        :param tokenizer: Tokenizer for decoding token IDs to text.\n        :param reward_model: Model for computing task rewards.\n        :param length_control: Whether to apply overlong penalty.\n        :param max_length: Maximum allowed response length.\n        :param penalty_factor: Factor for the overlong penalty.\n        \"\"\"\n        self.tokenizer = tokenizer\n        self.reward_model = reward_model\n        self.length_control = length_control\n        self.max_length = max_length\n        self.penalty_factor = penalty_factor\n\n    def __call__(self, data: DataProto, return_dict: bool = False):\n        \"\"\"\n        Compute task rewards and apply overlong penalty.\n\n        :param data: DataProto object containing prompts, responses, attention masks, ground truth, and data source.\n        :param return_dict: Whether to return dictionary format with additional information.\n        :return: Sparse reward tensor or dictionary with reward tensor and additional info.\n        \"\"\"\n        batch = data.batch\n        non_tensor_batch = data.non_tensor_batch\n\n        prompts = batch[\"prompts\"]\n        responses = batch[\"responses\"]\n        attention_mask = batch[\"attention_mask\"]\n        ground_truth = non_tensor_batch[\"reward_model][ground_truth\"]\n        data_source = non_tensor_batch[\"data_source\"]\n\n        batch_size, response_len = responses.shape\n\n        # Initialize reward tensor with zeros\n        reward_tensor = torch.zeros((batch_size, response_len), dtype=torch.float)\n\n        # Lists to store additional information\n        acc_list =        overlong_reward_list =        overlong_list =        # Check if pre-computed rewards exist\n        if \"precomputed_rewards\" in non_tensor_batch:\n            return non_tensor_batch[\"precomputed_rewards\"]\n\n        # Process each response\n        for i in range(batch_size):\n            # Find the last valid token position in the response\n            last_valid_pos = torch.where(attention_mask[i] == )[]\n            if last_valid_pos.numel() == :\n                last_valid_pos = response_len - \n            else:\n                last_valid_pos = last_valid_pos[] - \n\n            # Decode the response\n            response_text = self.tokenizer.decode(responses[i][:last_valid_pos + ], skip_special_tokens=True)\n\n            # Compute task reward\n            task_reward = self.compute_task_reward(response_text, ground_truth[i], data_source)\n            reward_tensor[i, last_valid_pos] = task_reward\n\n            # Check for overlong penalty\n            overlong = last_valid_pos >= self.max_length\n            overlong_penalty = .\n            if self.length_control and overlong:\n                overlong_penalty = -self.penalty_factor * (last_valid_pos - self.max_length + )\n                reward_tensor[i, last_valid_pos] += overlong_penalty\n\n            # Store additional information\n            acc_list.append(task_reward > )  # Assuming task_reward >  indicates correct answer\n            overlong_reward_list.append(overlong_penalty)\n            overlong_list.append(overlong)\n\n            # Debugging output for some samples\n            if i < :  # Print details for the first  samples\n                print(f\"Sample {i}:\")\n                print(f\"  Prompt: {self.tokenizer.decode(prompts[i], skip_special_tokens=True)}\")\n                print(f\"  Response: {response_text}\")\n                print(f\"  Ground Truth: {ground_truth[i]}\")\n                print(f\"  Task Reward: {task_reward}\")\n                print(f\"  Overlong Penalty: {overlong_penalty}\")\n                print(f\"  Final Reward: {reward_tensor[i, last_valid_pos]}\")\n                print(f\"  Overlong: {overlong}\")\n\n        if return_dict:\n            return {\n                \"reward_tensor\": reward_tensor,\n                \"reward_extra_info\": {\n                    \"acc\": acc_list,\n                    \"overlong_reward\": overlong_reward_list,\n                    \"overlong\": overlong_list\n                }\n            }\n        else:\n            return reward_tensor\n\n    def compute_task_reward(self, response_text: str, ground_truth: str, data_source: str) -> float:\n        \"\"\"\n        Compute the task reward based on the response and ground truth.\n\n        :param response_text: Decoded response text.\n        :param ground_truth: Ground truth answer.\n        :param data_source: Data source (e.g., \"gsmk\", \"math\").\n        :return: Task reward score.\n        \"\"\"\n        # Placeholder for actual reward computation logic\n        # This should be replaced with the actual logic based on the data source and task requirements\n        if response_text.strip().lower() == ground_truth.strip().lower():\n            return .\n        else:\n            return .\n\n# Example usage\n# tokenizer = ...  # Initialize your tokenizer\n# reward_model = ...  # Initialize your reward model\n# data = DataProto(batch={\"prompts\": ..., \"responses\": ..., \"attention_mask\": ...},\n#                  non_tensor_batch={\"reward_model\": {\"ground_truth\": ...}, \"data_source\": ...})\n# reward_manager = DAPORewardManager(tokenizer, reward_model, length_control=True, max_length=, penalty_factor=.)\n# rewards = reward_manager(data, return_dict=True)\n```\n\n### Explanation:\n. **Initialization**: The `DAPORewardManager` class is initialized with a tokenizer, reward model, and parameters for length control and penalty.\n. **Pre-computed Rewards Check**: If pre-computed rewards are available in `non_tensor_batch`, they are returned directly.\n. **Response Processing**: For each response, the last valid token position is identified, and the response is decoded.\n. **Task Reward Calculation**: The task reward is computed using the `compute_task_reward` method, which compares the decoded response with the ground truth.\n. **Overlong Penalty**: If length control is enabled and the response is overlong, a penalty is applied.\n. **Return Format**: Depending on the `return_dict` parameter, the function returns either a sparse reward tensor or a dictionary containing the reward tensor and additional information.\n\nThis implementation provides a basic framework and can be extended with more sophisticated reward computation logic as needed."], "usage": {"prompt_tokens": 645, "completion_tokens": 1464, "total_tokens": 2109, "per_completion": [{"prompt_tokens": 645, "completion_tokens": 1464, "total_tokens": 2109}]}, "results": [false], "pass_ratios": [0.0]}
