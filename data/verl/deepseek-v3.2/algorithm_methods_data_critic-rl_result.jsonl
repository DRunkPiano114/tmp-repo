{"function_name": "RewardModelForCritic.build_revision_messages", "overview": "Builds prompt messages for generating revised code. Combines original problem, initial solution, and critique into chat template format.", "function_signature": "def build_revision_messages(self, init_messages, critique):", "input_parameters": "- `init_messages`: Initial dialogue history list containing problem (user role) and initial solution (assistant role)\n- `critique`: Model-generated critique text string", "detailed_description": "Function receives initial dialogue history and critique text, validates that initial dialogue history is list type and last message role is assistant, calls related functions for content cleaning (keeping only code portion) and updating, then converts processed plain text critique with no leading/trailing whitespace and revision instruction \"Please finalize your answer accordingly using the same format.\" into new message with user role appended to end of dialogue.", "output": "- Complete dialogue message list (List[Dict]) containing original problem, cleaned initial code, and revision instruction", "func_implementation": "", "test_code_path": "code/test_code/test_build_revision_messages.py", "implementation_location": "code/ctrl/rl/critic_rm.py:line 136-150", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `RewardModelForCritic.build_revision_messages`.\n\n**Function Overview:**\nBuilds prompt messages for generating revised code. Combines original problem, initial solution, and critique into chat template format.\n\n**Function Signature:**\n```python\ndef build_revision_messages(self, init_messages, critique):\n```\n\n**Input Parameters:**\n- `init_messages`: Initial dialogue history list containing problem (user role) and initial solution (assistant role)\n- `critique`: Model-generated critique text string\n\n**Detailed Description:**\nFunction receives initial dialogue history and critique text, validates that initial dialogue history is list type and last message role is assistant, calls related functions for content cleaning (keeping only code portion) and updating, then converts processed plain text critique with no leading/trailing whitespace and revision instruction \"Please finalize your answer accordingly using the same format.\" into new message with user role appended to end of dialogue.\n\n**Expected Output:**\n- Complete dialogue message list (List[Dict]) containing original problem, cleaned initial code, and revision instruction\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\ndef build_revision_messages(self, init_messages, critique):\n    \"\"\"\n    Builds prompt messages for generating revised code by combining original problem,\n    initial solution, and critique into chat template format.\n    \n    Args:\n        init_messages: Initial dialogue history list containing problem (user role) \n                      and initial solution (assistant role)\n        critique: Model-generated critique text string\n    \n    Returns:\n        Complete dialogue message list (List[Dict]) containing original problem,\n        cleaned initial code, and revision instruction\n    \n    Raises:\n        ValueError: If init_messages is not a list or last message is not from assistant\n    \"\"\"\n    # Validate input types and structure\n    if not isinstance(init_messages, list):\n        raise ValueError(\"init_messages must be a list\")\n    \n    if not init_messages:\n        raise ValueError(\"init_messages cannot be empty\")\n    \n    # Check that last message is from assistant\n    last_message = init_messages[-1]\n    if last_message.get(\"role\") != \"assistant\":\n        raise ValueError(\"Last message in init_messages must have role 'assistant'\")\n    \n    # Clean the initial solution to keep only code portion\n    # Assuming there's a method to extract code from message content\n    cleaned_code = self._extract_code_from_message(last_message)\n    \n    # Create cleaned version of the last message (initial solution)\n    cleaned_last_message = {\n        \"role\": \"assistant\",\n        \"content\": cleaned_code\n    }\n    \n    # Prepare critique with revision instruction\n    # Remove leading/trailing whitespace from critique\n    processed_critique = critique.strip()\n    revision_instruction = \"Please finalize your answer accordingly using the same format.\"\n    critique_content = f\"{processed_critique}\\n\\n{revision_instruction}\"\n    \n    # Build the complete message list\n    # Keep all messages except the last one (original problem and any context)\n    # Replace the last message with cleaned version, then add critique\n    revision_messages = init_messages[:-1] + [cleaned_last_message]\n    \n    # Add the critique as a new user message\n    revision_messages.append({\n        \"role\": \"user\",\n        \"content\": critique_content\n    })\n    \n    return revision_messages\n\ndef _extract_code_from_message(self, message):\n    \"\"\"\n    Helper method to extract code portion from a message.\n    This is a placeholder implementation - actual implementation may vary\n    based on how code is formatted in the messages.\n    \n    Args:\n        message: Dictionary containing message with 'content' field\n    \n    Returns:\n        String containing only the code portion of the message\n    \"\"\"\n    content = message.get(\"content\", \"\")\n    \n    # Simple implementation: if content contains code blocks, extract them\n    # This can be customized based on actual formatting used\n    if \"```\" in content:\n        # Extract content between first and last code blocks\n        parts = content.split(\"```\")\n        if len(parts) >= 3:\n            # Return the code part (typically at index 1 for first code block)\n            return parts[1].strip()\n    \n    # If no code blocks found, return the entire content\n    return content.strip()\n```"], "usage": {"prompt_tokens": 403, "completion_tokens": 2667, "total_tokens": 3070, "per_completion": [{"prompt_tokens": 403, "completion_tokens": 2667, "total_tokens": 3070}]}, "results": [false], "pass_ratios": [0.0]}
{"function_name": "RewardModelForCritic.get_reward_all", "overview": "Concurrently executes sandbox tests for multiple critique-revision pairs, computing verifiable rewards. Improves execution efficiency through asynchronous I/O mechanism, supporting large-scale parallel evaluation.", "function_signature": "async def get_reward_all(\nself, critiques: List[str], revisions: List[str], samples: List[Any]\n) -> List[float]:", "input_parameters": "- `critiques`: Critique text list, each element corresponding to a generated critique\n- `revisions`: Revised code list, one-to-one correspondence with critiques\n- `samples`: Test sample list containing test cases and problem metadata", "detailed_description": "Function calls underlying reward computation method for each triplet (sample, critique, revision), evaluating improvement quality of revised version relative to original sample, returning numerical reward scores. All computation tasks execute asynchronously and concurrently, maximizing processing throughput while ensuring resource control, with default maximum concurrency set to MAX_REQUESTS (256), requiring progress bar (\"Generating rewards\") for real-time completion feedback.", "output": "- Float list, each element representing reward value for corresponding critique-revision pair, range [0.0, 1.0]", "func_implementation": "", "test_code_path": "code/test_code/test_get_reward_all.py", "implementation_location": "code/ctrl/rl/critic_rm.py:line 198-209", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `RewardModelForCritic.get_reward_all`.\n\n**Function Overview:**\nConcurrently executes sandbox tests for multiple critique-revision pairs, computing verifiable rewards. Improves execution efficiency through asynchronous I/O mechanism, supporting large-scale parallel evaluation.\n\n**Function Signature:**\n```python\nasync def get_reward_all(\nself, critiques: List[str], revisions: List[str], samples: List[Any]\n) -> List[float]:\n```\n\n**Input Parameters:**\n- `critiques`: Critique text list, each element corresponding to a generated critique\n- `revisions`: Revised code list, one-to-one correspondence with critiques\n- `samples`: Test sample list containing test cases and problem metadata\n\n**Detailed Description:**\nFunction calls underlying reward computation method for each triplet (sample, critique, revision), evaluating improvement quality of revised version relative to original sample, returning numerical reward scores. All computation tasks execute asynchronously and concurrently, maximizing processing throughput while ensuring resource control, with default maximum concurrency set to MAX_REQUESTS (256), requiring progress bar (\"Generating rewards\") for real-time completion feedback.\n\n**Expected Output:**\n- Float list, each element representing reward value for corresponding critique-revision pair, range [0.0, 1.0]\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nasync def get_reward_all(\n    self, critiques: List[str], revisions: List[str], samples: List[Any]\n) -> List[float]:\n    \"\"\"\n    Asynchronously computes rewards for multiple critique-revision pairs.\n    \n    Args:\n        critiques: List of critique texts, each corresponding to a generated critique\n        revisions: List of revised code texts, one-to-one correspondence with critiques\n        samples: List of test samples containing test cases and problem metadata\n        \n    Returns:\n        List[float]: Reward scores for each critique-revision pair, range [0.0, 1.0]\n    \"\"\"\n    # Validate input lengths match\n    if not (len(critiques) == len(revisions) == len(samples)):\n        raise ValueError(\n            f\"Input lists must have the same length. \"\n            f\"Got: critiques={len(critiques)}, revisions={len(revisions)}, samples={len(samples)}\"\n        )\n    \n    # Create list of tasks for concurrent execution\n    tasks = []\n    for critique, revision, sample in zip(critiques, revisions, samples):\n        task = self._get_single_reward(critique, revision, sample)\n        tasks.append(task)\n    \n    # Execute tasks with controlled concurrency\n    from asyncio import Semaphore, create_task, gather\n    from tqdm.asyncio import tqdm\n    \n    # Create semaphore to limit concurrent requests\n    semaphore = Semaphore(self.MAX_REQUESTS)  # Default MAX_REQUESTS = 256\n    \n    async def bounded_task(task):\n        async with semaphore:\n            return await task\n    \n    # Create bounded tasks\n    bounded_tasks = [bounded_task(task) for task in tasks]\n    \n    # Execute all tasks with progress bar\n    rewards = []\n    try:\n        # Use tqdm for progress tracking\n        with tqdm(\n            total=len(bounded_tasks),\n            desc=\"Generating rewards\",\n            unit=\"pair\",\n            bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]\"\n        ) as pbar:\n            for coro in asyncio.as_completed(bounded_tasks):\n                result = await coro\n                rewards.append(result)\n                pbar.update(1)\n    except Exception as e:\n        # Handle potential errors gracefully\n        self.logger.error(f\"Error during reward computation: {e}\")\n        raise\n    \n    return rewards\n\n# Helper method for single reward computation (assumed to exist)\nasync def _get_single_reward(self, critique: str, revision: str, sample: Any) -> float:\n    \"\"\"\n    Compute reward for a single critique-revision pair.\n    This would be implemented based on specific reward model requirements.\n    \"\"\"\n    # Implement the actual reward computation logic here\n    # This typically involves:\n    # 1. Running test cases on the revised code\n    # 2. Comparing with original sample performance\n    # 3. Calculating improvement score\n    \n    # Placeholder implementation - returns random score for demonstration\n    import random\n    \n    # Simulate async processing\n    import asyncio\n    await asyncio.sleep(0.01)  # Simulate I/O delay\n    \n    # Placeholder: Generate a reward score between 0.0 and 1.0\n    reward_score = random.uniform(0.0, 1.0)\n    return reward_score\n```\n\n**Implementation Notes:**\n\n1. **Input Validation**: Ensures all input lists have the same length to maintain one-to-one correspondence\n2. **Concurrency Control**: Uses `Semaphore` to limit concurrent operations to `MAX_REQUESTS` (default 256) for resource management\n3. **Progress Tracking**: Implements `tqdm` progress bar with detailed format for real-time feedback\n4. **Error Handling**: Includes try-except block to handle potential exceptions during async execution\n5. **Helper Method**: Assumes `_get_single_reward` method exists for individual reward computation\n6. **Performance**: Uses `asyncio.as_completed()` to process tasks as they complete, improving throughput\n7. **Type Hints**: Maintains proper type annotations as per the function signature\n\n**Key Features:**\n- Efficient asynchronous execution for large-scale parallel evaluation\n- Resource-aware concurrency limiting\n- Real-time progress feedback\n- Graceful error handling\n- Maintains one-to-one correspondence between inputs and outputs\n- Returns reward scores in the specified [0.0, 1.0] range"], "usage": {"prompt_tokens": 446, "completion_tokens": 961, "total_tokens": 1407, "per_completion": [{"prompt_tokens": 446, "completion_tokens": 961, "total_tokens": 1407}]}, "results": [false], "pass_ratios": [0.0]}
{"function_name": "sanitize", "overview": "Extracts pure code blocks from mixed text containing code and explanations, removes explanatory text, preserves markdown format code block structure.", "function_signature": "def sanitize(text):", "input_parameters": "- `text`: Mixed text string containing code blocks", "detailed_description": "Function uses regular expressions to identify and extract markdown format code blocks, matching code blocks wrapped in triple backticks through regex, recognizing cases with or without python identifier. Matching process is case-insensitive. After successful match, extracts content of first code block and reformats it to standard markdown format ````python\\n{code}\\n``` for return. If no code block found in text or code block is empty, directly returns original text.", "output": "- Formatted code block string (````python\\n{code}\\n```) or original text (when no code block)\nIn `build_revision_messages` function, used to clean model-generated initial solution, remove explanatory text, keep only code portion, ensuring conciseness of revision prompt.", "func_implementation": "", "test_code_path": "code/test_code/test_sanitize_desanitize.py", "implementation_location": "code/ctrl/rl/critic_rm.py:line 76-84", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `sanitize`.\n\n**Function Overview:**\nExtracts pure code blocks from mixed text containing code and explanations, removes explanatory text, preserves markdown format code block structure.\n\n**Function Signature:**\n```python\ndef sanitize(text):\n```\n\n**Input Parameters:**\n- `text`: Mixed text string containing code blocks\n\n**Detailed Description:**\nFunction uses regular expressions to identify and extract markdown format code blocks, matching code blocks wrapped in triple backticks through regex, recognizing cases with or without python identifier. Matching process is case-insensitive. After successful match, extracts content of first code block and reformats it to standard markdown format ````python\\n{code}\\n``` for return. If no code block found in text or code block is empty, directly returns original text.\n\n**Expected Output:**\n- Formatted code block string (````python\\n{code}\\n```) or original text (when no code block)\nIn `build_revision_messages` function, used to clean model-generated initial solution, remove explanatory text, keep only code portion, ensuring conciseness of revision prompt.\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport re\n\ndef sanitize(text):\n    \"\"\"\n    Extracts pure code blocks from mixed text containing code and explanations.\n    \n    This function identifies markdown code blocks wrapped in triple backticks,\n    extracts the code content, and reformats it into a standard markdown format.\n    If no code block is found or the code block is empty, the original text is returned.\n    \n    Args:\n        text (str): Mixed text string containing code blocks and explanatory text.\n        \n    Returns:\n        str: Formatted code block string (```python\\n{code}\\n```) or original text.\n    \"\"\"\n    # Regular expression pattern to match markdown code blocks with triple backticks\n    # Pattern explanation:\n    # - `(?i)` makes the pattern case-insensitive\n    # - ````python` matches the opening backticks with optional language identifier\n    # - `(.*?)` captures the code content (non-greedy)\n    # - ```` matches the closing backticks\n    # - `re.DOTALL` flag makes `.` match newlines as well\n    pattern = r'(?i)```python\\s*(.*?)\\s*```'\n    \n    # Search for code blocks in the text\n    match = re.search(pattern, text, re.DOTALL)\n    \n    # If a code block is found and it contains non-whitespace content\n    if match and match.group(1).strip():\n        # Extract the code content and remove surrounding whitespace\n        code_content = match.group(1).strip()\n        # Return the code in standard markdown format\n        return f\"```python\\n{code_content}\\n```\"\n    \n    # If no valid code block is found, return the original text\n    return text\n```"], "usage": {"prompt_tokens": 412, "completion_tokens": 375, "total_tokens": 787, "per_completion": [{"prompt_tokens": 412, "completion_tokens": 375, "total_tokens": 787}]}, "results": [false], "pass_ratios": [0.0]}
{"function_name": "desanitize", "overview": "Removes markdown code block markers (triple backticks) from code text, extracts pure code string.", "function_signature": "def desanitize(text):", "input_parameters": "- `text`: Text with or without markdown code block markers", "detailed_description": "Function uses regular expression to match code blocks wrapped in triple backticks, recognizing cases with or without python identifier. After successful match, extracts content inside code block, returns after removing leading/trailing whitespace. If no code block format found in text, directly returns original text.", "output": "- Pure code string with markdown markers removed\nIn `get_sandbox_response` function, used to extract pure code from revision text, remove markdown format markers, prepare code for sandbox execution.", "func_implementation": "", "test_code_path": "code/test_code/test_sanitize_desanitize.py", "implementation_location": "code/ctrl/rl/critic_rm.py:line 66-73", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `desanitize`.\n\n**Function Overview:**\nRemoves markdown code block markers (triple backticks) from code text, extracts pure code string.\n\n**Function Signature:**\n```python\ndef desanitize(text):\n```\n\n**Input Parameters:**\n- `text`: Text with or without markdown code block markers\n\n**Detailed Description:**\nFunction uses regular expression to match code blocks wrapped in triple backticks, recognizing cases with or without python identifier. After successful match, extracts content inside code block, returns after removing leading/trailing whitespace. If no code block format found in text, directly returns original text.\n\n**Expected Output:**\n- Pure code string with markdown markers removed\nIn `get_sandbox_response` function, used to extract pure code from revision text, remove markdown format markers, prepare code for sandbox execution.\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport re\n\ndef desanitize(text):\n    \"\"\"\n    Removes markdown code block markers (triple backticks) from code text,\n    extracting the pure code string.\n    \n    Args:\n        text: Text with or without markdown code block markers\n        \n    Returns:\n        Pure code string with markdown markers removed, or original text\n        if no code block format is found.\n    \"\"\"\n    # Regular expression pattern to match markdown code blocks\n    # Pattern matches:\n    # 1. ```python\\n[content]\\n``` (with language identifier)\n    # 2. ```\\n[content]\\n``` (without language identifier)\n    # The pattern uses non-greedy matching to handle multiple code blocks\n    pattern = r'```(?:\\w+)?\\n?(.*?)```'\n    \n    # Search for code blocks in the text\n    match = re.search(pattern, text, re.DOTALL)\n    \n    if match:\n        # Extract the content inside the code block\n        code_content = match.group(1)\n        # Remove leading and trailing whitespace\n        return code_content.strip()\n    else:\n        # If no code block format found, return original text\n        return text.strip()\n```"], "usage": {"prompt_tokens": 364, "completion_tokens": 260, "total_tokens": 624, "per_completion": [{"prompt_tokens": 364, "completion_tokens": 260, "total_tokens": 624}]}, "results": [false], "pass_ratios": [0.0]}
{"function_name": "normalize_code", "overview": "Normalizes Python code through Abstract Syntax Tree (AST) level transformation, unifies variable naming, improves cache hit rate.", "function_signature": "def normalize_code(code_str):", "input_parameters": "- `code_str`: Python code string to be normalized", "detailed_description": "Function uniformly renames variables in Python code to standard form (v_0, v_1, ...) through AST parsing, enabling functionally identical but differently named code to be recognized as equivalent. If code has syntax errors, function returns original code without transformation.", "output": "- Normalized code string, or original code (when parsing fails)\nDuring training, model may generate multiple semantically identical but differently named code versions. For example, `x = 1; return x` and `y = 1; return y` are functionally completely equivalent. Through normalization, both are converted to `v_0 = 1; return v_0`, using same cache key, avoiding repeated sandbox execution, significantly improving training efficiency.", "func_implementation": "", "test_code_path": "code/test_code/test_normalize_code.py", "implementation_location": "code/ctrl/rl/critic_rm.py:line 35-63", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `normalize_code`.\n\n**Function Overview:**\nNormalizes Python code through Abstract Syntax Tree (AST) level transformation, unifies variable naming, improves cache hit rate.\n\n**Function Signature:**\n```python\ndef normalize_code(code_str):\n```\n\n**Input Parameters:**\n- `code_str`: Python code string to be normalized\n\n**Detailed Description:**\nFunction uniformly renames variables in Python code to standard form (v_0, v_1, ...) through AST parsing, enabling functionally identical but differently named code to be recognized as equivalent. If code has syntax errors, function returns original code without transformation.\n\n**Expected Output:**\n- Normalized code string, or original code (when parsing fails)\nDuring training, model may generate multiple semantically identical but differently named code versions. For example, `x = 1; return x` and `y = 1; return y` are functionally completely equivalent. Through normalization, both are converted to `v_0 = 1; return v_0`, using same cache key, avoiding repeated sandbox execution, significantly improving training efficiency.\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport ast\nimport re\nfrom collections import defaultdict\n\ndef normalize_code(code_str):\n    \"\"\"\n    Normalizes Python code by uniformly renaming variables to standard form (v_0, v_1, ...)\n    through AST parsing. If the code has syntax errors, returns the original code unchanged.\n    \n    Args:\n        code_str: Python code string to be normalized\n        \n    Returns:\n        Normalized code string, or original code if parsing fails\n    \"\"\"\n    try:\n        # Parse the code into AST\n        tree = ast.parse(code_str)\n        \n        # Track variable names and their mappings\n        var_counter = 0\n        var_mapping = {}\n        \n        # Track function arguments separately to avoid conflicts\n        func_args = set()\n        \n        # First pass: collect all variable names and function arguments\n        class VariableCollector(ast.NodeVisitor):\n            def __init__(self):\n                self.variables = set()\n                self.function_args = set()\n                \n            def visit_Name(self, node):\n                if isinstance(node.ctx, (ast.Store, ast.Load)):\n                    self.variables.add(node.id)\n                    \n            def visit_FunctionDef(self, node):\n                # Collect function arguments\n                for arg in node.args.args:\n                    self.function_args.add(arg.arg)\n                for arg in node.args.kwonlyargs:\n                    self.function_args.add(arg.arg)\n                if node.args.vararg:\n                    self.function_args.add(node.args.vararg.arg)\n                if node.args.kwarg:\n                    self.function_args.add(node.args.kwarg.arg)\n                self.generic_visit(node)\n                \n            def visit_AsyncFunctionDef(self, node):\n                # Same as FunctionDef for async functions\n                for arg in node.args.args:\n                    self.function_args.add(arg.arg)\n                for arg in node.args.kwonlyargs:\n                    self.function_args.add(arg.arg)\n                if node.args.vararg:\n                    self.function_args.add(node.args.vararg.arg)\n                if node.args.kwarg:\n                    self.function_args.add(node.args.kwarg.arg)\n                self.generic_visit(node)\n                \n            def visit_Lambda(self, node):\n                # Collect lambda arguments\n                for arg in node.args.args:\n                    self.function_args.add(arg.arg)\n                for arg in node.args.kwonlyargs:\n                    self.function_args.add(arg.arg)\n                if node.args.vararg:\n                    self.function_args.add(node.args.vararg.arg)\n                if node.args.kwarg:\n                    self.function_args.add(node.args.kwarg.arg)\n                self.generic_visit(node)\n        \n        collector = VariableCollector()\n        collector.visit(tree)\n        \n        # Create mapping for non-function-argument variables\n        for var_name in sorted(collector.variables):\n            if var_name not in collector.function_args:\n                # Skip Python builtins and dunder methods\n                if not (var_name.startswith('__') and var_name.endswith('__')):\n                    if var_name not in var_mapping:\n                        var_mapping[var_name] = f\"v_{var_counter}\"\n                        var_counter += 1\n        \n        # Second pass: rename variables using the mapping\n        class VariableRenamer(ast.NodeTransformer):\n            def __init__(self, mapping, func_args):\n                self.mapping = mapping\n                self.func_args = func_args\n                \n            def visit_Name(self, node):\n                # Only rename if it's in our mapping and not a function argument\n                if node.id in self.mapping and node.id not in self.func_args:\n                    new_node = ast.Name(id=self.mapping[node.id], ctx=node.ctx)\n                    return ast.copy_location(new_node, node)\n                return node\n                \n            def visit_FunctionDef(self, node):\n                # Don't rename function names themselves\n                return self.generic_visit(node)\n                \n            def visit_AsyncFunctionDef(self, node):\n                # Don't rename async function names themselves\n                return self.generic_visit(node)\n                \n            def visit_ClassDef(self, node):\n                # Don't rename class names themselves\n                return self.generic_visit(node)\n        \n        renamer = VariableRenamer(var_mapping, collector.function_args)\n        normalized_tree = renamer.visit(tree)\n        \n        # Convert AST back to code string\n        normalized_code = ast.unparse(normalized_tree)\n        \n        return normalized_code\n        \n    except (SyntaxError, ValueError):\n        # If parsing fails, return original code\n        return code_str\n```"], "usage": {"prompt_tokens": 406, "completion_tokens": 979, "total_tokens": 1385, "per_completion": [{"prompt_tokens": 406, "completion_tokens": 979, "total_tokens": 1385}]}, "results": [true], "pass_ratios": [1.0]}
