{"function_name": "agg_loss", "overview": "Aggregates loss matrix to scalar, supporting multiple aggregation modes. DAPO recommends using `token-mean` mode, making each token's contribution to loss equal, avoiding long sequences dominating gradients.", "function_signature": "def agg_loss(loss_mat: torch.Tensor, loss_mask: torch.Tensor, loss_agg_mode: str):", "input_parameters": "- `loss_mat`: Loss matrix with shape `(batch_size, response_length)`\n- `loss_mask`: Mask tensor with shape `(batch_size, response_length)` (response part is 1, padding is 0)\n- `loss_agg_mode`: String specifying aggregation mode\n- `\"token-mean\"`: Average over all tokens (**DAPO recommended**)\n- `\"seq-mean-token-sum\"`: Sum tokens within sequences first, then average across sequences\n- `\"seq-mean-token-mean\"`: Average tokens within sequences first, then average across sequences\n- `\"seq-mean-token-sum-norm\"`: Sum sequences then normalize by sequence length", "detailed_description": "The core function is to aggregate a 2D loss matrix into a single scalar value, supporting the following four aggregation strategies: token-mean, seq-mean-token-sum, seq-mean-token-mean, seq-mean-token-sum-norm.", "output": "- Scalar loss value for backpropagation", "func_implementation": "", "test_code_path": "code/test_code/test_agg_loss.py", "implementation_location": "code/verl/trainer/ppo/core_algos.py:line 704-737", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `agg_loss`.\n\n**Function Overview:**\nAggregates loss matrix to scalar, supporting multiple aggregation modes. DAPO recommends using `token-mean` mode, making each token's contribution to loss equal, avoiding long sequences dominating gradients.\n\n**Function Signature:**\n```python\ndef agg_loss(loss_mat: torch.Tensor, loss_mask: torch.Tensor, loss_agg_mode: str):\n```\n\n**Input Parameters:**\n- `loss_mat`: Loss matrix with shape `(batch_size, response_length)`\n- `loss_mask`: Mask tensor with shape `(batch_size, response_length)` (response part is 1, padding is 0)\n- `loss_agg_mode`: String specifying aggregation mode\n- `\"token-mean\"`: Average over all tokens (**DAPO recommended**)\n- `\"seq-mean-token-sum\"`: Sum tokens within sequences first, then average across sequences\n- `\"seq-mean-token-mean\"`: Average tokens within sequences first, then average across sequences\n- `\"seq-mean-token-sum-norm\"`: Sum sequences then normalize by sequence length\n\n**Detailed Description:**\nThe core function is to aggregate a 2D loss matrix into a single scalar value, supporting the following four aggregation strategies: token-mean, seq-mean-token-sum, seq-mean-token-mean, seq-mean-token-sum-norm.\n\n**Expected Output:**\n- Scalar loss value for backpropagation\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport torch\n\ndef agg_loss(loss_mat: torch.Tensor, loss_mask: torch.Tensor, loss_agg_mode: str):\n    \"\"\"\n    Aggregates loss matrix to scalar, supporting multiple aggregation modes.\n    \n    Args:\n        loss_mat: Loss matrix with shape (batch_size, response_length)\n        loss_mask: Mask tensor with shape (batch_size, response_length) \n                   (response part is 1, padding is 0)\n        loss_agg_mode: String specifying aggregation mode:\n            - \"token-mean\": Average over all tokens (DAPO recommended)\n            - \"seq-mean-token-sum\": Sum tokens within sequences first, then average across sequences\n            - \"seq-mean-token-mean\": Average tokens within sequences first, then average across sequences\n            - \"seq-mean-token-sum-norm\": Sum sequences then normalize by sequence length\n    \n    Returns:\n        Scalar loss value for backpropagation\n    \"\"\"\n    # Validate input shapes\n    assert loss_mat.shape == loss_mask.shape, \\\n        f\"loss_mat shape {loss_mat.shape} must match loss_mask shape {loss_mask.shape}\"\n    assert len(loss_mat.shape) == 2, \\\n        f\"loss_mat must be 2D tensor, got shape {loss_mat.shape}\"\n    \n    # Apply mask to loss matrix (zero out padding positions)\n    masked_loss = loss_mat * loss_mask\n    \n    if loss_agg_mode == \"token-mean\":\n        # Average over all tokens (DAPO recommended)\n        # Sum of all valid tokens divided by total number of valid tokens\n        total_loss = masked_loss.sum()\n        total_valid_tokens = loss_mask.sum()\n        loss = total_loss / (total_valid_tokens + 1e-8)\n        \n    elif loss_agg_mode == \"seq-mean-token-sum\":\n        # Sum tokens within sequences first, then average across sequences\n        # Sum along sequence dimension (dim=1)\n        seq_losses = masked_loss.sum(dim=1)\n        # Average across batch dimension\n        loss = seq_losses.mean()\n        \n    elif loss_agg_mode == \"seq-mean-token-mean\":\n        # Average tokens within sequences first, then average across sequences\n        # Average along sequence dimension (dim=1), ignoring padding\n        seq_losses = (masked_loss.sum(dim=1) / (loss_mask.sum(dim=1) + 1e-8))\n        # Average across batch dimension\n        loss = seq_losses.mean()\n        \n    elif loss_agg_mode == \"seq-mean-token-sum-norm\":\n        # Sum sequences then normalize by sequence length\n        # Sum along sequence dimension (dim=1)\n        seq_losses = masked_loss.sum(dim=1)\n        # Normalize by sequence length (number of valid tokens in each sequence)\n        seq_losses_norm = seq_losses / (loss_mask.sum(dim=1) + 1e-8)\n        # Average across batch dimension\n        loss = seq_losses_norm.mean()\n        \n    else:\n        raise ValueError(f\"Unknown loss_agg_mode: {loss_agg_mode}. \"\n                         f\"Supported modes: 'token-mean', 'seq-mean-token-sum', \"\n                         f\"'seq-mean-token-mean', 'seq-mean-token-sum-norm'\")\n    \n    return loss\n```"], "usage": {"prompt_tokens": 491, "completion_tokens": 712, "total_tokens": 1203, "per_completion": [{"prompt_tokens": 491, "completion_tokens": 712, "total_tokens": 1203}]}, "results": [false], "pass_ratios": [0.9090909090909091]}
{"function_name": "compute_policy_loss_vanilla", "overview": "Computes PPO policy gradient loss, supporting decoupled clipping (DAPO's core innovation). Through `clip_ratio_low` and `clip_ratio_high` parameters, allows asymmetric clipping for positive and negative advantages, implementing the \"Clip-Higher\" mechanism.", "function_signature": "def compute_policy_loss_vanilla(\nold_log_prob: torch.Tensor,\nlog_prob: torch.Tensor,\nadvantages: torch.Tensor,\nresponse_mask: torch.Tensor,\nloss_agg_mode: str = \"token-mean\",\nconfig: Optional[DictConfig | AlgoConfig] = None,\nrollout_log_probs: torch.Tensor | None = None,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:", "input_parameters": "- `old_log_prob`: Old policy log probabilities with shape `(batch_size, response_length)`\n- `log_prob`: Current policy log probabilities with shape `(batch_size, response_length)`\n- `advantages`: Advantage estimates with shape `(batch_size, response_length)`\n- `response_mask`: Response mask with shape `(batch_size, response_length)`\n- `loss_agg_mode`: Loss aggregation mode (default is token-mean)\n- `config`: Actor configuration object, containing:\n- `clip_ratio`: Default clipping parameter (backward compatibility)\n- `clip_ratio_low`: Lower bound clipping parameter (**DAPO new**)\n- `clip_ratio_high`: Upper bound clipping parameter (**DAPO new**)\n- `clip_ratio_c`: Dual clipping lower bound (handles extreme negative advantages, default 3.0)\n- `rollout_log_probs`: Optional, Rollout policy log probabilities (for off-policy training)", "detailed_description": "The function implements enhanced PPO policy loss computation, including standard PPO clipped loss calculation, dual-clip PPO, and truncated importance sampling (optional), ultimately calling related functions to implement loss aggregation.\nStandard PPO clipping mechanism supports asymmetric clipping, requiring computation of unclipped original policy gradient loss and asymmetrically clipped loss, taking the maximum of both as pessimistic update strategy. When advantage value is negative, apply dual clipping to more conservatively reduce the probability of that action. If truncated importance sampling is enabled and rollout policy log probabilities are provided, the function computes the importance ratio of old policy relative to rollout policy, clips it to the configured upper limit, then multiplies it into the policy gradient loss.", "output": "Returns four scalar tensors:\n- `pg_loss`: Policy gradient loss\n- `pg_clipfrac`: Clipping fraction (proportion of clipped tokens, for monitoring training dynamics)\n- `ppo_kl`: Approximate KL divergence\n- `pg_clipfrac_lower`: Lower bound clipping fraction (proportion of tokens where dual clipping takes effect)", "func_implementation": "", "test_code_path": "code/test_code/test_compute_policy_loss_vanilla.py", "implementation_location": "code/verl/trainer/ppo/core_algos.py:line 816-902", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_policy_loss_vanilla`.\n\n**Function Overview:**\nComputes PPO policy gradient loss, supporting decoupled clipping (DAPO's core innovation). Through `clip_ratio_low` and `clip_ratio_high` parameters, allows asymmetric clipping for positive and negative advantages, implementing the \"Clip-Higher\" mechanism.\n\n**Function Signature:**\n```python\ndef compute_policy_loss_vanilla(\nold_log_prob: torch.Tensor,\nlog_prob: torch.Tensor,\nadvantages: torch.Tensor,\nresponse_mask: torch.Tensor,\nloss_agg_mode: str = \"token-mean\",\nconfig: Optional[DictConfig | AlgoConfig] = None,\nrollout_log_probs: torch.Tensor | None = None,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n```\n\n**Input Parameters:**\n- `old_log_prob`: Old policy log probabilities with shape `(batch_size, response_length)`\n- `log_prob`: Current policy log probabilities with shape `(batch_size, response_length)`\n- `advantages`: Advantage estimates with shape `(batch_size, response_length)`\n- `response_mask`: Response mask with shape `(batch_size, response_length)`\n- `loss_agg_mode`: Loss aggregation mode (default is token-mean)\n- `config`: Actor configuration object, containing:\n- `clip_ratio`: Default clipping parameter (backward compatibility)\n- `clip_ratio_low`: Lower bound clipping parameter (**DAPO new**)\n- `clip_ratio_high`: Upper bound clipping parameter (**DAPO new**)\n- `clip_ratio_c`: Dual clipping lower bound (handles extreme negative advantages, default 3.0)\n- `rollout_log_probs`: Optional, Rollout policy log probabilities (for off-policy training)\n\n**Detailed Description:**\nThe function implements enhanced PPO policy loss computation, including standard PPO clipped loss calculation, dual-clip PPO, and truncated importance sampling (optional), ultimately calling related functions to implement loss aggregation.\nStandard PPO clipping mechanism supports asymmetric clipping, requiring computation of unclipped original policy gradient loss and asymmetrically clipped loss, taking the maximum of both as pessimistic update strategy. When advantage value is negative, apply dual clipping to more conservatively reduce the probability of that action. If truncated importance sampling is enabled and rollout policy log probabilities are provided, the function computes the importance ratio of old policy relative to rollout policy, clips it to the configured upper limit, then multiplies it into the policy gradient loss.\n\n**Expected Output:**\nReturns four scalar tensors:\n- `pg_loss`: Policy gradient loss\n- `pg_clipfrac`: Clipping fraction (proportion of clipped tokens, for monitoring training dynamics)\n- `ppo_kl`: Approximate KL divergence\n- `pg_clipfrac_lower`: Lower bound clipping fraction (proportion of tokens where dual clipping takes effect)\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["Here's the implementation of `compute_policy_loss_vanilla`:\n\n```python\nimport torch\nfrom typing import Optional, Tuple\nfrom omegaconf import DictConfig\n\n\ndef compute_policy_loss_vanilla(\n    old_log_prob: torch.Tensor,\n    log_prob: torch.Tensor,\n    advantages: torch.Tensor,\n    response_mask: torch.Tensor,\n    loss_agg_mode: str = \"token-mean\",\n    config: Optional[DictConfig] = None,\n    rollout_log_probs: torch.Tensor | None = None,\n) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Computes PPO policy gradient loss with support for decoupled clipping (DAPO).\n    \n    Args:\n        old_log_prob: Old policy log probabilities with shape (batch_size, response_length)\n        log_prob: Current policy log probabilities with shape (batch_size, response_length)\n        advantages: Advantage estimates with shape (batch_size, response_length)\n        response_mask: Response mask with shape (batch_size, response_length)\n        loss_agg_mode: Loss aggregation mode (default: \"token-mean\")\n        config: Actor configuration object containing clipping parameters\n        rollout_log_probs: Optional rollout policy log probabilities for off-policy training\n        \n    Returns:\n        pg_loss: Policy gradient loss\n        pg_clipfrac: Clipping fraction (proportion of clipped tokens)\n        ppo_kl: Approximate KL divergence\n        pg_clipfrac_lower: Lower bound clipping fraction\n    \"\"\"\n    \n    # Ensure config is provided\n    if config is None:\n        raise ValueError(\"config parameter must be provided\")\n    \n    # Extract clipping parameters with backward compatibility\n    clip_ratio = getattr(config, 'clip_ratio', 0.2)\n    clip_ratio_low = getattr(config, 'clip_ratio_low', clip_ratio)\n    clip_ratio_high = getattr(config, 'clip_ratio_high', clip_ratio)\n    clip_ratio_c = getattr(config, 'clip_ratio_c', 3.0)\n    \n    # Compute importance ratio\n    ratio = torch.exp(log_prob - old_log_prob)\n    \n    # Compute unclipped policy gradient loss\n    pg_loss_unclipped = -advantages * ratio\n    \n    # Apply asymmetric clipping based on advantage sign\n    # For positive advantages: clip_ratio_high, for negative advantages: clip_ratio_low\n    clip_low = 1.0 - clip_ratio_low\n    clip_high = 1.0 + clip_ratio_high\n    \n    # Compute clipped policy gradient loss\n    clipped_ratio = torch.where(\n        advantages >= 0,\n        torch.clamp(ratio, max=clip_high),\n        torch.clamp(ratio, min=clip_low)\n    )\n    pg_loss_clipped = -advantages * clipped_ratio\n    \n    # Take the maximum of unclipped and clipped loss (pessimistic update)\n    pg_loss = torch.max(pg_loss_unclipped, pg_loss_clipped)\n    \n    # Apply dual clipping for extreme negative advantages\n    if clip_ratio_c is not None:\n        # Compute dual clipped loss\n        dual_clipped_ratio = torch.clamp(ratio, max=clip_ratio_c)\n        pg_loss_dual_clipped = -advantages * dual_clipped_ratio\n        \n        # Take the maximum of all three losses\n        pg_loss = torch.maximum(pg_loss, pg_loss_dual_clipped)\n    \n    # Apply truncated importance sampling if enabled\n    if rollout_log_probs is not None:\n        # Compute importance ratio of old policy relative to rollout policy\n        importance_ratio = torch.exp(old_log_prob - rollout_log_probs)\n        \n        # Clip importance ratio to configured upper limit\n        importance_clip_ratio = getattr(config, 'importance_clip_ratio', 1.0)\n        clipped_importance_ratio = torch.clamp(importance_ratio, max=importance_clip_ratio)\n        \n        # Multiply importance ratio into policy gradient loss\n        pg_loss = pg_loss * clipped_importance_ratio\n    \n    # Compute clipping fractions for monitoring\n    with torch.no_grad():\n        # Standard clipping fraction\n        clipped_mask = (ratio > clip_high) | (ratio < clip_low)\n        pg_clipfrac = (clipped_mask * response_mask).sum() / response_mask.sum()\n        \n        # Lower bound clipping fraction (dual clipping)\n        if clip_ratio_c is not None:\n            dual_clipped_mask = (ratio > clip_ratio_c)\n            pg_clipfrac_lower = (dual_clipped_mask * response_mask).sum() / response_mask.sum()\n        else:\n            pg_clipfrac_lower = torch.tensor(0.0, device=pg_loss.device)\n        \n        # Compute approximate KL divergence\n        ppo_kl = (old_log_prob - log_prob) * response_mask\n        if loss_agg_mode == \"token-mean\":\n            ppo_kl = ppo_kl.sum() / response_mask.sum()\n        elif loss_agg_mode == \"sum\":\n            ppo_kl = ppo_kl.sum()\n        else:\n            raise ValueError(f\"Unsupported loss_agg_mode: {loss_agg_mode}\")\n    \n    # Apply response mask and aggregate loss\n    pg_loss = pg_loss * response_mask\n    \n    if loss_agg_mode == \"token-mean\":\n        pg_loss = pg_loss.sum() / response_mask.sum()\n    elif loss_agg_mode == \"sum\":\n        pg_loss = pg_loss.sum()\n    else:\n        raise ValueError(f\"Unsupported loss_agg_mode: {loss_agg_mode}\")\n    \n    return pg_loss, pg_clipfrac, ppo_kl, pg_clipfrac_lower\n```\n\nThis implementation:\n\n1. **Handles asymmetric clipping** (`clip_ratio_low` and `clip_ratio_high`) based on the sign of advantages\n2. **Implements dual clipping** (`clip_ratio_c`) for extreme negative advantages\n3. **Supports truncated importance sampling** when `rollout_log_probs` is provided\n4. **Computes monitoring metrics** including clipping fractions and KL divergence\n5. **Applies response mask** properly and supports different aggregation modes\n6. **Maintains backward compatibility** with standard PPO clipping parameters\n\nThe function follows the DAPO \"Clip-Higher\" mechanism by applying different clipping bounds for positive and negative advantages, providing more flexible control over policy updates."], "usage": {"prompt_tokens": 793, "completion_tokens": 1374, "total_tokens": 2167, "per_completion": [{"prompt_tokens": 793, "completion_tokens": 1374, "total_tokens": 2167}]}, "results": [false], "pass_ratios": [0.0]}
{"function_name": "DAPORewardManager.__call__", "overview": "DAPO's reward manager, responsible for computing task rewards and applying overlong penalty. This is one of the key components distinguishing DAPO from other methods, preventing the model from generating meaningless lengthy outputs through a linear penalty mechanism.", "function_signature": "def __call__(self, data: DataProto, return_dict: bool = False):", "input_parameters": "- `data`: DataProto object containing:\n- `batch[\"prompts\"]`: Prompt token IDs with shape `(bs, prompt_len)`\n- `batch[\"responses\"]`: Response token IDs with shape `(bs, response_len)`\n- `batch[\"attention_mask\"]`: Attention mask with shape `(bs, total_len)`\n- `non_tensor_batch[\"reward_model\"][\"ground_truth\"]`: Ground truth answers\n- `non_tensor_batch[\"data_source\"]`: Data source (e.g., \"gsm8k\", \"math\")\n- `return_dict`: Boolean, whether to return dictionary format (including additional information like accuracy, overlong penalty, etc.)", "detailed_description": "The function is responsible for computing reward scores for generated responses. First checks if there are pre-computed rewards; if so, returns directly; otherwise processes each response individually, decoding token sequences into text and computing task rewards based on ground truth answers (needs to call related functional functions). Computed scores are placed at the last valid token position in the response sequence. If length control is enabled, the function applies additional negative reward penalty for overlong responses (linear penalty mechanism). Throughout the process, detailed information for some samples is output for debugging verification, ultimately returning complete reward tensor or dictionary containing additional evaluation metrics.", "output": "- If `return_dict=False`: Returns sparse reward tensor with shape `(batch_size, response_length)` (only last valid token position is non-zero)\n- If `return_dict=True`: Returns dictionary containing:\n- `\"reward_tensor\"`: Reward tensor as above\n- `\"reward_extra_info\"`: Dictionary containing `\"acc\"` (accuracy list), `\"overlong_reward\"` (overlong penalty list), `\"overlong\"` (boolean list indicating if overlong), etc.", "func_implementation": "", "test_code_path": "code/test_code/test_dapo_reward_manager.py", "implementation_location": "code/verl/workers/reward_manager/dapo.py:line 53-149", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `DAPORewardManager.__call__`.\n\n**Function Overview:**\nDAPO's reward manager, responsible for computing task rewards and applying overlong penalty. This is one of the key components distinguishing DAPO from other methods, preventing the model from generating meaningless lengthy outputs through a linear penalty mechanism.\n\n**Function Signature:**\n```python\ndef __call__(self, data: DataProto, return_dict: bool = False):\n```\n\n**Input Parameters:**\n- `data`: DataProto object containing:\n- `batch[\"prompts\"]`: Prompt token IDs with shape `(bs, prompt_len)`\n- `batch[\"responses\"]`: Response token IDs with shape `(bs, response_len)`\n- `batch[\"attention_mask\"]`: Attention mask with shape `(bs, total_len)`\n- `non_tensor_batch[\"reward_model\"][\"ground_truth\"]`: Ground truth answers\n- `non_tensor_batch[\"data_source\"]`: Data source (e.g., \"gsm8k\", \"math\")\n- `return_dict`: Boolean, whether to return dictionary format (including additional information like accuracy, overlong penalty, etc.)\n\n**Detailed Description:**\nThe function is responsible for computing reward scores for generated responses. First checks if there are pre-computed rewards; if so, returns directly; otherwise processes each response individually, decoding token sequences into text and computing task rewards based on ground truth answers (needs to call related functional functions). Computed scores are placed at the last valid token position in the response sequence. If length control is enabled, the function applies additional negative reward penalty for overlong responses (linear penalty mechanism). Throughout the process, detailed information for some samples is output for debugging verification, ultimately returning complete reward tensor or dictionary containing additional evaluation metrics.\n\n**Expected Output:**\n- If `return_dict=False`: Returns sparse reward tensor with shape `(batch_size, response_length)` (only last valid token position is non-zero)\n- If `return_dict=True`: Returns dictionary containing:\n- `\"reward_tensor\"`: Reward tensor as above\n- `\"reward_extra_info\"`: Dictionary containing `\"acc\"` (accuracy list), `\"overlong_reward\"` (overlong penalty list), `\"overlong\"` (boolean list indicating if overlong), etc.\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\ndef __call__(self, data: DataProto, return_dict: bool = False):\n    \"\"\"\n    DAPO's reward manager, responsible for computing task rewards and applying overlong penalty.\n    \n    Args:\n        data: DataProto object containing:\n            - batch[\"prompts\"]: Prompt token IDs with shape (bs, prompt_len)\n            - batch[\"responses\"]: Response token IDs with shape (bs, response_len)\n            - batch[\"attention_mask\"]: Attention mask with shape (bs, total_len)\n            - non_tensor_batch[\"reward_model\"][\"ground_truth\"]: Ground truth answers\n            - non_tensor_batch[\"data_source\"]: Data source (e.g., \"gsm8k\", \"math\")\n        return_dict: Boolean, whether to return dictionary format\n    \n    Returns:\n        If return_dict=False: Sparse reward tensor with shape (batch_size, response_length)\n        If return_dict=True: Dictionary containing:\n            - \"reward_tensor\": Reward tensor as above\n            - \"reward_extra_info\": Dictionary containing \"acc\", \"overlong_reward\", \"overlong\", etc.\n    \"\"\"\n    # Extract batch data\n    batch = data.batch\n    non_tensor_batch = data.non_tensor_batch\n    \n    # Get batch size and response length\n    bs, response_len = batch[\"responses\"].shape\n    total_len = batch[\"attention_mask\"].shape[1]\n    prompt_len = total_len - response_len\n    \n    # Check if rewards are already pre-computed\n    if \"rewards\" in batch:\n        if not return_dict:\n            return batch[\"rewards\"]\n        else:\n            return {\n                \"reward_tensor\": batch[\"rewards\"],\n                \"reward_extra_info\": non_tensor_batch.get(\"reward_extra_info\", {})\n            }\n    \n    # Initialize reward tensor (all zeros)\n    reward_tensor = torch.zeros((bs, response_len), device=batch[\"responses\"].device)\n    \n    # Initialize extra information lists\n    acc_list = []\n    overlong_reward_list = []\n    overlong_list = []\n    \n    # Get ground truth answers and data source\n    ground_truths = non_tensor_batch[\"reward_model\"][\"ground_truth\"]\n    data_source = non_tensor_batch[\"data_source\"]\n    \n    # Decode tokenizer from data if available\n    tokenizer = getattr(data, \"tokenizer\", None)\n    if tokenizer is None:\n        # Try to get from non_tensor_batch\n        tokenizer = non_tensor_batch.get(\"tokenizer\")\n    \n    # Process each sample in the batch\n    for i in range(bs):\n        # Get response tokens for this sample\n        response_tokens = batch[\"responses\"][i]\n        \n        # Decode response tokens to text\n        if tokenizer is not None:\n            response_text = tokenizer.decode(response_tokens, skip_special_tokens=True)\n        else:\n            # Fallback: convert to string representation\n            response_text = str(response_tokens.cpu().tolist())\n        \n        # Get ground truth for this sample\n        ground_truth = ground_truths[i] if i < len(ground_truths) else \"\"\n        \n        # Compute task reward based on data source\n        task_reward = self._compute_task_reward(\n            response_text, ground_truth, data_source\n        )\n        \n        # Apply overlong penalty if length control is enabled\n        if self.length_control_enabled:\n            # Get response length (excluding padding)\n            attention_mask = batch[\"attention_mask\"][i]\n            # Find the last valid token position in the response\n            response_mask = attention_mask[prompt_len:]\n            valid_response_len = response_mask.sum().item()\n            \n            # Check if response is overlong\n            is_overlong = valid_response_len > self.max_response_length\n            \n            if is_overlong:\n                # Calculate linear penalty: penalty increases linearly with excess length\n                excess_length = valid_response_len - self.max_response_length\n                overlong_penalty = -self.penalty_coefficient * excess_length\n                # Apply penalty to task reward\n                task_reward += overlong_penalty\n                overlong_reward_list.append(overlong_penalty)\n            else:\n                overlong_reward_list.append(0.0)\n            \n            overlong_list.append(is_overlong)\n        else:\n            overlong_reward_list.append(0.0)\n            overlong_list.append(False)\n        \n        # Store accuracy (task_reward is 1.0 for correct, 0.0 for incorrect)\n        acc_list.append(float(task_reward))\n        \n        # Find the last valid token position in the response\n        attention_mask = batch[\"attention_mask\"][i]\n        response_mask = attention_mask[prompt_len:]\n        \n        # Find the last non-padding token position\n        if response_mask.sum() > 0:\n            # Get indices of non-zero elements\n            non_zero_indices = torch.nonzero(response_mask, as_tuple=True)[0]\n            last_valid_idx = non_zero_indices[-1].item()\n        else:\n            # If all tokens are padding, use the first position\n            last_valid_idx = 0\n        \n        # Place reward at the last valid token position\n        reward_tensor[i, last_valid_idx] = task_reward\n        \n        # Debug output for first few samples\n        if i < self.debug_sample_count:\n            self._log_debug_info(\n                i, response_text, ground_truth, task_reward,\n                last_valid_idx, data_source\n            )\n    \n    # Prepare return value\n    if not return_dict:\n        return reward_tensor\n    else:\n        return {\n            \"reward_tensor\": reward_tensor,\n            \"reward_extra_info\": {\n                \"acc\": acc_list,\n                \"overlong_reward\": overlong_reward_list,\n                \"overlong\": overlong_list,\n                \"data_source\": data_source\n            }\n        }\n```"], "usage": {"prompt_tokens": 661, "completion_tokens": 1270, "total_tokens": 1931, "per_completion": [{"prompt_tokens": 661, "completion_tokens": 1270, "total_tokens": 1931}]}, "results": [false], "pass_ratios": [0.0]}
