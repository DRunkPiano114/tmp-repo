{"function_name": "compute_grpo_outcome_advantage_split", "overview": "Computes group GRPO (Group Relative Policy Optimization) advantage estimation, with the key innovation of using only on-policy samples to compute within-group baseline.", "function_signature": "def compute_grpo_outcome_advantage_split(token_level_rewards: torch.Tensor,\neos_mask: torch.Tensor,\nindex: torch.Tensor,\non_policy_mask: torch.Tensor,\nepsilon: float = 1e-6,\nuse_std: bool = True):", "input_parameters": "- `token_level_rewards`: Token-level reward tensor with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `eos_mask`: Valid token mask with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `index`: Sample grouping indices, n responses from the same prompt share the same index. **Note: This parameter should be a Python list or indexable sequence (e.g., `[0, 0, 1, 1]`), not `torch.Tensor`, as index values will be used as dictionary keys**\n- `on_policy_mask`: Boolean mask with shape `[batch_size]`, type `torch.Tensor`, marking which samples are on-policy generated\n- `epsilon`: Numerical stability constant, default 1e-6\n- `use_std`: Whether to use standard deviation for normalization, default True", "detailed_description": "This function needs to sum token-level reward tensors along the time dimension for each response sequence to obtain sequence-level total scores.\nThe main implementation performs grouped normalization (including both on-policy and off-policy) for multiple responses from the same prompt, computing relative advantage values. During normalization, only samples marked as on-policy are selected to compute the group's mean and standard deviation. If a group has only one on-policy sample, set mean to 0 and standard deviation to 1; if there are multiple samples, compute statistics normally. For cases where standard deviation is 0, replace it with 1 to avoid division by zero errors.\nIf standard deviation normalization is enabled, compute (score - mean) / (std + epsilon); otherwise only subtract the mean. Finally, expand sequence-level advantage values to token-level advantage tensors.", "output": "- `advantages`: Token-level advantage tensor with shape `[batch_size, response_length]`\n- `returns`: Token-level return tensor with shape `[batch_size, response_length]`, same as advantages", "func_implementation": "", "test_code_path": "code/test_code/test_compute_grpo_outcome_advantage_split.py", "implementation_location": "code/luffy/verl/verl/mix_src/mix_core_alg.py:line 12-67", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_grpo_outcome_advantage_split`.\n\n**Function Overview:**\nComputes group GRPO (Group Relative Policy Optimization) advantage estimation, with the key innovation of using only on-policy samples to compute within-group baseline.\n\n**Function Signature:**\n```python\ndef compute_grpo_outcome_advantage_split(token_level_rewards: torch.Tensor,\neos_mask: torch.Tensor,\nindex: torch.Tensor,\non_policy_mask: torch.Tensor,\nepsilon: float = 1e-6,\nuse_std: bool = True):\n```\n\n**Input Parameters:**\n- `token_level_rewards`: Token-level reward tensor with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `eos_mask`: Valid token mask with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `index`: Sample grouping indices, n responses from the same prompt share the same index. **Note: This parameter should be a Python list or indexable sequence (e.g., `[0, 0, 1, 1]`), not `torch.Tensor`, as index values will be used as dictionary keys**\n- `on_policy_mask`: Boolean mask with shape `[batch_size]`, type `torch.Tensor`, marking which samples are on-policy generated\n- `epsilon`: Numerical stability constant, default 1e-6\n- `use_std`: Whether to use standard deviation for normalization, default True\n\n**Detailed Description:**\nThis function needs to sum token-level reward tensors along the time dimension for each response sequence to obtain sequence-level total scores.\nThe main implementation performs grouped normalization (including both on-policy and off-policy) for multiple responses from the same prompt, computing relative advantage values. During normalization, only samples marked as on-policy are selected to compute the group's mean and standard deviation. If a group has only one on-policy sample, set mean to 0 and standard deviation to 1; if there are multiple samples, compute statistics normally. For cases where standard deviation is 0, replace it with 1 to avoid division by zero errors.\nIf standard deviation normalization is enabled, compute (score - mean) / (std + epsilon); otherwise only subtract the mean. Finally, expand sequence-level advantage values to token-level advantage tensors.\n\n**Expected Output:**\n- `advantages`: Token-level advantage tensor with shape `[batch_size, response_length]`\n- `returns`: Token-level return tensor with shape `[batch_size, response_length]`, same as advantages\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport torch\nfrom typing import List, Union\n\ndef compute_grpo_outcome_advantage_split(token_level_rewards: torch.Tensor,\n                                         eos_mask: torch.Tensor,\n                                         index: Union[List[int], List[str]],\n                                         on_policy_mask: torch.Tensor,\n                                         epsilon: float = 1e-6,\n                                         use_std: bool = True) -> tuple:\n    \"\"\"\n    Compute group GRPO advantage estimation using only on-policy samples for within-group baseline.\n    \n    Args:\n        token_level_rewards: Token-level reward tensor with shape [batch_size, response_length]\n        eos_mask: Valid token mask with shape [batch_size, response_length]\n        index: Sample grouping indices. Responses from the same prompt share the same index.\n               Can be list of ints or strings (used as dictionary keys).\n        on_policy_mask: Boolean mask with shape [batch_size] marking on-policy samples\n        epsilon: Numerical stability constant\n        use_std: Whether to use standard deviation for normalization\n    \n    Returns:\n        advantages: Token-level advantage tensor with shape [batch_size, response_length]\n        returns: Token-level return tensor with shape [batch_size, response_length]\n    \"\"\"\n    batch_size, response_length = token_level_rewards.shape\n    \n    # Step 1: Sum token-level rewards along time dimension to get sequence-level scores\n    # Only sum over valid tokens (where eos_mask is True)\n    sequence_scores = torch.zeros(batch_size, device=token_level_rewards.device)\n    for i in range(batch_size):\n        valid_rewards = token_level_rewards[i][eos_mask[i]]\n        sequence_scores[i] = valid_rewards.sum()\n    \n    # Step 2: Group samples by index and compute statistics using only on-policy samples\n    group_stats = {}\n    \n    # First, organize indices into groups\n    groups = {}\n    for i, idx in enumerate(index):\n        if idx not in groups:\n            groups[idx] = []\n        groups[idx].append(i)\n    \n    # Compute group statistics using only on-policy samples\n    for idx, sample_indices in groups.items():\n        # Filter on-policy samples within this group\n        on_policy_indices = [i for i in sample_indices if on_policy_mask[i]]\n        \n        if len(on_policy_indices) == 0:\n            # No on-policy samples in this group, use default values\n            group_stats[idx] = {'mean': 0.0, 'std': 1.0}\n        elif len(on_policy_indices) == 1:\n            # Only one on-policy sample, set mean to 0 and std to 1\n            group_stats[idx] = {'mean': 0.0, 'std': 1.0}\n        else:\n            # Multiple on-policy samples, compute statistics normally\n            on_policy_scores = sequence_scores[on_policy_indices]\n            mean_val = on_policy_scores.mean().item()\n            std_val = on_policy_scores.std().item()\n            \n            # Handle zero standard deviation\n            if std_val < epsilon:\n                std_val = 1.0\n            \n            group_stats[idx] = {'mean': mean_val, 'std': std_val}\n    \n    # Step 3: Compute normalized advantages for each sample\n    sequence_advantages = torch.zeros_like(sequence_scores)\n    \n    for i, idx in enumerate(index):\n        stats = group_stats[idx]\n        score = sequence_scores[i]\n        \n        if use_std:\n            # Normalize with mean and std: (score - mean) / (std + epsilon)\n            sequence_advantages[i] = (score - stats['mean']) / (stats['std'] + epsilon)\n        else:\n            # Only subtract the mean\n            sequence_advantages[i] = score - stats['mean']\n    \n    # Step 4: Expand sequence-level advantages to token-level advantage tensors\n    advantages = torch.zeros_like(token_level_rewards)\n    returns = torch.zeros_like(token_level_rewards)\n    \n    for i in range(batch_size):\n        # Get the sequence advantage for this sample\n        seq_advantage = sequence_advantages[i]\n        \n        # Expand to all valid tokens in the sequence\n        valid_token_count = eos_mask[i].sum().int()\n        if valid_token_count > 0:\n            # Distribute advantage equally across all valid tokens\n            token_advantage = seq_advantage / valid_token_count\n            advantages[i][:valid_token_count] = token_advantage\n            returns[i][:valid_token_count] = token_advantage\n    \n    return advantages, returns\n```"], "usage": {"prompt_tokens": 721, "completion_tokens": 988, "total_tokens": 1709, "per_completion": [{"prompt_tokens": 721, "completion_tokens": 988, "total_tokens": 1709}]}}
{"function_name": "compute_token_on_off_policy_loss", "overview": "Computes mixed policy loss, combining on-policy PPO clip loss and off-policy importance sampling loss, supporting multiple policy shaping methods to emphasize low-probability but critical actions.", "function_signature": "def compute_token_on_off_policy_loss(\nold_log_prob,\nlog_prob,\nadvantages,\neos_mask,\ncliprange,\nclip_upper_bound,\nprefix_mask,\noff_cliprange,\noff_normalize=False,\noff_abs_cliprange=None,\noff_max_clip=None,\noff_min_clip=None,\nall_max_clip=None,\noff_policy_reshape=\"no_reshape\",\noff_policy_reshape_weight=1.0,\noff_policy_reshape_pow_exp=0.5,\non_policy_reshape=\"no_reshape\",\non_policy_reshape_weight=1.0,\non_policy_reshape_pow_exp=0.5,\ntarget_probs=None,\nloss_remove_token_mean=False,\nloss_remove_clip=False,\n):", "input_parameters": "- `old_log_prob`: Old policy log probabilities with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `log_prob`: Current policy log probabilities with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `advantages`: Advantage estimates with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `eos_mask`: Valid token mask with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `cliprange`: PPO clip range, default 0.2\n- `clip_upper_bound`: PPO clip upper bound, default 100.0\n- `prefix_mask`: Mask marking off-policy tokens with shape `[batch_size, response_length]`, **type must be `torch.Tensor` with dtype `torch.bool`, as the code uses bitwise NOT operator `~prefix_mask` to obtain on-policy token mask**\n- `off_cliprange`: off-policy clip range (not used in current implementation)\n- `off_normalize`: Whether to normalize off-policy ratio, default False\n- `off_abs_cliprange`: off-policy absolute value clip range (not used in current implementation)\n- `off_max_clip`: off-policy ratio upper limit, None means no limit\n- `off_min_clip`: off-policy ratio lower limit, None means no limit\n- `all_max_clip`: Global probability upper limit, None means no limit\n- `off_policy_reshape`: off-policy policy shaping method name, default 'no_reshape'\n- `off_policy_reshape_weight`: Policy shaping weight parameter, default 1.0\n- `off_policy_reshape_pow_exp`: Power transformation exponent, default 0.5\n- `on_policy_reshape`: on-policy policy shaping method name, default 'no_reshape'\n- `on_policy_reshape_weight`: on-policy policy shaping weight, default 1.0\n- `on_policy_reshape_pow_exp`: on-policy power transformation exponent, default 0.5\n- `target_probs`: Optional target probabilities (for explicit behavior policy), default None\n- `loss_remove_token_mean`: Whether to remove token-level averaging, default False\n- `loss_remove_clip`: Whether to remove clip operation, default False", "detailed_description": "This function computes mixed policy gradient loss, with the core idea of simultaneously handling both on-policy and off-policy data. For on-policy data, it uses standard PPO algorithm to compute importance sampling ratio and perform clipping; for off-policy data, it directly uses current policy probabilities (or ratio relative to target probabilities) to compute loss, without relying on old policy.\nThe function distinguishes which tokens are off-policy and which are on-policy through prefix_mask, then combines the two losses with weighting. Additionally, the function should support multiple probability reshaping strategies and clipping mechanisms.", "output": "Returns dictionary containing the following key-value pairs:\n- `pg_loss`: Total policy gradient loss (scalar)\n- `off_pg_loss`: off-policy loss component (scalar)\n- `on_pg_loss`: on-policy loss component (scalar)\n- `off_pg_clipfrac`: off-policy clip fraction (scalar, currently 0 in implementation)\n- `on_pg_clipfrac`: on-policy clip fraction (scalar)\n- `ppo_kl`: PPO KL divergence (scalar)\n- `off_policy_prob`: Average probability of off-policy samples (scalar)\n- `on_policy_prob`: Average probability of on-policy samples (scalar)\n- `off_ratio_mean`: off-policy average ratio (scalar)\n- `off_ratio_max_clip_frac`: Fraction reaching upper clip limit (scalar)\n- `off_ratio_min_clip_frac`: Fraction reaching lower clip limit (scalar)", "func_implementation": "", "test_code_path": "code/test_code/test_compute_token_on_off_policy_loss.py", "implementation_location": "code/luffy/verl/verl/mix_src/mix_core_alg.py:line 69-251", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_token_on_off_policy_loss`.\n\n**Function Overview:**\nComputes mixed policy loss, combining on-policy PPO clip loss and off-policy importance sampling loss, supporting multiple policy shaping methods to emphasize low-probability but critical actions.\n\n**Function Signature:**\n```python\ndef compute_token_on_off_policy_loss(\nold_log_prob,\nlog_prob,\nadvantages,\neos_mask,\ncliprange,\nclip_upper_bound,\nprefix_mask,\noff_cliprange,\noff_normalize=False,\noff_abs_cliprange=None,\noff_max_clip=None,\noff_min_clip=None,\nall_max_clip=None,\noff_policy_reshape=\"no_reshape\",\noff_policy_reshape_weight=1.0,\noff_policy_reshape_pow_exp=0.5,\non_policy_reshape=\"no_reshape\",\non_policy_reshape_weight=1.0,\non_policy_reshape_pow_exp=0.5,\ntarget_probs=None,\nloss_remove_token_mean=False,\nloss_remove_clip=False,\n):\n```\n\n**Input Parameters:**\n- `old_log_prob`: Old policy log probabilities with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `log_prob`: Current policy log probabilities with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `advantages`: Advantage estimates with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `eos_mask`: Valid token mask with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `cliprange`: PPO clip range, default 0.2\n- `clip_upper_bound`: PPO clip upper bound, default 100.0\n- `prefix_mask`: Mask marking off-policy tokens with shape `[batch_size, response_length]`, **type must be `torch.Tensor` with dtype `torch.bool`, as the code uses bitwise NOT operator `~prefix_mask` to obtain on-policy token mask**\n- `off_cliprange`: off-policy clip range (not used in current implementation)\n- `off_normalize`: Whether to normalize off-policy ratio, default False\n- `off_abs_cliprange`: off-policy absolute value clip range (not used in current implementation)\n- `off_max_clip`: off-policy ratio upper limit, None means no limit\n- `off_min_clip`: off-policy ratio lower limit, None means no limit\n- `all_max_clip`: Global probability upper limit, None means no limit\n- `off_policy_reshape`: off-policy policy shaping method name, default 'no_reshape'\n- `off_policy_reshape_weight`: Policy shaping weight parameter, default 1.0\n- `off_policy_reshape_pow_exp`: Power transformation exponent, default 0.5\n- `on_policy_reshape`: on-policy policy shaping method name, default 'no_reshape'\n- `on_policy_reshape_weight`: on-policy policy shaping weight, default 1.0\n- `on_policy_reshape_pow_exp`: on-policy power transformation exponent, default 0.5\n- `target_probs`: Optional target probabilities (for explicit behavior policy), default None\n- `loss_remove_token_mean`: Whether to remove token-level averaging, default False\n- `loss_remove_clip`: Whether to remove clip operation, default False\n\n**Detailed Description:**\nThis function computes mixed policy gradient loss, with the core idea of simultaneously handling both on-policy and off-policy data. For on-policy data, it uses standard PPO algorithm to compute importance sampling ratio and perform clipping; for off-policy data, it directly uses current policy probabilities (or ratio relative to target probabilities) to compute loss, without relying on old policy.\nThe function distinguishes which tokens are off-policy and which are on-policy through prefix_mask, then combines the two losses with weighting. Additionally, the function should support multiple probability reshaping strategies and clipping mechanisms.\n\n**Expected Output:**\nReturns dictionary containing the following key-value pairs:\n- `pg_loss`: Total policy gradient loss (scalar)\n- `off_pg_loss`: off-policy loss component (scalar)\n- `on_pg_loss`: on-policy loss component (scalar)\n- `off_pg_clipfrac`: off-policy clip fraction (scalar, currently 0 in implementation)\n- `on_pg_clipfrac`: on-policy clip fraction (scalar)\n- `ppo_kl`: PPO KL divergence (scalar)\n- `off_policy_prob`: Average probability of off-policy samples (scalar)\n- `on_policy_prob`: Average probability of on-policy samples (scalar)\n- `off_ratio_mean`: off-policy average ratio (scalar)\n- `off_ratio_max_clip_frac`: Fraction reaching upper clip limit (scalar)\n- `off_ratio_min_clip_frac`: Fraction reaching lower clip limit (scalar)\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport torch\nimport torch.nn.functional as F\nfrom typing import Optional, Dict\n\ndef compute_token_on_off_policy_loss(\n    old_log_prob: torch.Tensor,\n    log_prob: torch.Tensor,\n    advantages: torch.Tensor,\n    eos_mask: torch.Tensor,\n    cliprange: float = 0.2,\n    clip_upper_bound: float = 100.0,\n    prefix_mask: torch.Tensor = None,\n    off_cliprange: float = None,\n    off_normalize: bool = False,\n    off_abs_cliprange: Optional[float] = None,\n    off_max_clip: Optional[float] = None,\n    off_min_clip: Optional[float] = None,\n    all_max_clip: Optional[float] = None,\n    off_policy_reshape: str = \"no_reshape\",\n    off_policy_reshape_weight: float = 1.0,\n    off_policy_reshape_pow_exp: float = 0.5,\n    on_policy_reshape: str = \"no_reshape\",\n    on_policy_reshape_weight: float = 1.0,\n    on_policy_reshape_pow_exp: float = 0.5,\n    target_probs: Optional[torch.Tensor] = None,\n    loss_remove_token_mean: bool = False,\n    loss_remove_clip: bool = False,\n) -> Dict[str, torch.Tensor]:\n    \"\"\"\n    Compute mixed policy loss combining on-policy PPO clip loss and off-policy importance sampling loss.\n    \n    Args:\n        old_log_prob: Old policy log probabilities [batch_size, response_length]\n        log_prob: Current policy log probabilities [batch_size, response_length]\n        advantages: Advantage estimates [batch_size, response_length]\n        eos_mask: Valid token mask [batch_size, response_length]\n        cliprange: PPO clip range\n        clip_upper_bound: PPO clip upper bound\n        prefix_mask: Mask marking off-policy tokens [batch_size, response_length], dtype=torch.bool\n        off_cliprange: off-policy clip range (unused)\n        off_normalize: Whether to normalize off-policy ratio\n        off_abs_cliprange: off-policy absolute value clip range (unused)\n        off_max_clip: off-policy ratio upper limit\n        off_min_clip: off-policy ratio lower limit\n        all_max_clip: Global probability upper limit\n        off_policy_reshape: off-policy policy shaping method\n        off_policy_reshape_weight: Policy shaping weight parameter\n        off_policy_reshape_pow_exp: Power transformation exponent\n        on_policy_reshape: on-policy policy shaping method\n        on_policy_reshape_weight: on-policy policy shaping weight\n        on_policy_reshape_pow_exp: on-policy power transformation exponent\n        target_probs: Optional target probabilities for explicit behavior policy\n        loss_remove_token_mean: Whether to remove token-level averaging\n        loss_remove_clip: Whether to remove clip operation\n        \n    Returns:\n        Dictionary containing loss components and statistics\n    \"\"\"\n    \n    # Ensure prefix_mask is boolean tensor\n    if prefix_mask is not None:\n        assert prefix_mask.dtype == torch.bool, \"prefix_mask must be torch.bool\"\n    \n    # Apply eos_mask to all tensors\n    eos_mask = eos_mask.bool()\n    old_log_prob = old_log_prob.masked_fill(~eos_mask, 0.0)\n    log_prob = log_prob.masked_fill(~eos_mask, 0.0)\n    advantages = advantages.masked_fill(~eos_mask, 0.0)\n    \n    # Split data into on-policy and off-policy based on prefix_mask\n    if prefix_mask is not None:\n        # prefix_mask=True indicates off-policy tokens\n        off_policy_mask = prefix_mask & eos_mask\n        on_policy_mask = (~prefix_mask) & eos_mask\n    else:\n        # If no prefix_mask, all tokens are on-policy\n        off_policy_mask = torch.zeros_like(eos_mask, dtype=torch.bool)\n        on_policy_mask = eos_mask\n    \n    # Get counts for normalization\n    off_policy_count = off_policy_mask.sum().float()\n    on_policy_count = on_policy_mask.sum().float()\n    total_valid_count = eos_mask.sum().float()\n    \n    # Initialize statistics\n    stats = {}\n    \n    # ==================== OFF-POLICY LOSS COMPUTATION ====================\n    if off_policy_count > 0:\n        # Get off-policy log probabilities\n        off_log_prob = log_prob[off_policy_mask]\n        off_advantages = advantages[off_policy_mask]\n        \n        # Compute ratio for off-policy samples\n        if target_probs is not None:\n            # Use explicit behavior policy if provided\n            target_log_prob = torch.log(target_probs[off_policy_mask] + 1e-10)\n            off_ratio = torch.exp(off_log_prob - target_log_prob)\n        else:\n            # No old policy for off-policy data\n            off_ratio = torch.exp(off_log_prob)\n        \n        # Apply global clipping if specified\n        if all_max_clip is not None:\n            off_ratio = torch.clamp(off_ratio, max=all_max_clip)\n        \n        # Apply off-policy specific clipping\n        if off_max_clip is not None:\n            off_ratio_clipped_max = torch.clamp(off_ratio, max=off_max_clip)\n            stats['off_ratio_max_clip_frac'] = (off_ratio > off_max_clip).float().mean()\n        else:\n            off_ratio_clipped_max = off_ratio\n            stats['off_ratio_max_clip_frac'] = torch.tensor(0.0, device=off_ratio.device)\n        \n        if off_min_clip is not None:\n            off_ratio_clipped = torch.clamp(off_ratio_clipped_max, min=off_min_clip)\n            stats['off_ratio_min_clip_frac'] = (off_ratio_clipped_max < off_min_clip).float().mean()\n        else:\n            off_ratio_clipped = off_ratio_clipped_max\n            stats['off_ratio_min_clip_frac'] = torch.tensor(0.0, device=off_ratio.device)\n        \n        # Normalize if requested\n        if off_normalize:\n            off_ratio_clipped = off_ratio_clipped / (off_ratio_clipped.mean() + 1e-8)\n        \n        # Apply policy reshaping for off-policy\n        if off_policy_reshape == \"pow\":\n            # Power transformation: r^exp\n            off_ratio_clipped = torch.pow(off_ratio_clipped, off_policy_reshape_pow_exp)\n        elif off_policy_reshape == \"weighted\":\n            # Weighted transformation: w * r\n            off_ratio_clipped = off_policy_reshape_weight * off_ratio_clipped\n        elif off_policy_reshape == \"weighted_pow\":\n            # Combined transformation: w * r^exp\n            off_ratio_clipped = off_policy_reshape_weight * torch.pow(\n                off_ratio_clipped, off_policy_reshape_pow_exp\n            )\n        # \"no_reshape\" does nothing\n        \n        # Compute off-policy loss\n        off_pg_loss = -off_ratio_clipped * off_advantages\n        \n        # Compute statistics\n        stats['off_policy_prob'] = torch.exp(off_log_prob).mean()\n        stats['off_ratio_mean'] = off_ratio.mean()\n        stats['off_pg_clipfrac'] = torch.tensor(0.0, device=off_pg_loss.device)  # Currently 0\n        \n        # Aggregate off-policy loss\n        if loss_remove_token_mean:\n            off_pg_loss = off_pg_loss.sum()\n        else:\n            off_pg_loss = off_pg_loss.mean()\n    else:\n        off_pg_loss = torch.tensor(0.0, device=log_prob.device)\n        stats['off_policy_prob'] = torch.tensor(0.0, device=log_prob.device)\n        stats['off_ratio_mean'] = torch.tensor(0.0, device=log_prob.device)\n        stats['off_pg_clipfrac'] = torch.tensor(0.0, device=log_prob.device)\n        stats['off_ratio_max_clip_frac'] = torch.tensor(0.0, device=log_prob.device)\n        stats['off_ratio_min_clip_frac'] = torch.tensor(0.0, device=log_prob.device)\n    \n    # ==================== ON-POLICY LOSS COMPUTATION ====================\n    if on_policy_count > 0:\n        # Get on-policy data\n        on_old_log_prob = old_log_prob[on_policy_mask]\n        on_log_prob = log_prob[on_policy_mask]\n        on_advantages = advantages[on_policy_mask]\n        \n        # Compute importance sampling ratio\n        ratio = torch.exp(on_log_prob - on_old_log_prob)\n        \n        # Apply global clipping if specified\n        if all_max_clip is not None:\n            ratio = torch.clamp(ratio, max=all_max_clip)\n        \n        # Apply PPO clipping\n        if loss_remove_clip:\n            ratio_clipped = ratio\n        else:\n            # Standard PPO clipping\n            ratio_clipped = torch.clamp(ratio, 1.0 - cliprange, 1.0 + cliprange)\n            ratio_clipped = torch.clamp(ratio_clipped, max=clip_upper_bound)\n        \n        # Apply policy reshaping for on-policy\n        if on_policy_reshape == \"pow\":\n            # Power transformation: r^exp\n            ratio = torch.pow(ratio, on_policy_reshape_pow_exp)\n            ratio_clipped = torch.pow(ratio_clipped, on_policy_reshape_pow_exp)\n        elif on_policy_reshape == \"weighted\":\n            # Weighted transformation: w * r\n            ratio = on_policy_reshape_weight * ratio\n            ratio_clipped = on_policy_reshape_weight * ratio_clipped\n        elif on_policy_reshape == \"weighted_pow\":\n            # Combined transformation: w * r^exp\n            ratio = on_policy_reshape_weight * torch.pow(ratio, on_policy_reshape_pow_exp)\n            ratio_clipped = on_policy_reshape_weight * torch.pow(\n                ratio_clipped, on_policy_reshape_pow_exp\n            )\n        # \"no_reshape\" does nothing\n        \n        # Compute PPO surrogate loss\n        surr1 = ratio * on_advantages\n        surr2 = ratio_clipped * on_advantages\n        on_pg_loss = -torch.min(surr1, surr2)\n        \n        # Compute statistics\n        stats['on_policy_prob'] = torch.exp(on_log_prob).mean()\n        stats['on_pg_clipfrac'] = ((ratio > (1.0 + cliprange)) | \n                                   (ratio < (1.0 - cliprange))).float().mean()\n        \n        # Compute KL divergence\n        kl = on_old_log_prob - on_log_prob\n        stats['ppo_kl'] = kl.mean()\n        \n        # Aggregate on-policy loss\n        if loss_remove_token_mean:\n            on_pg_loss = on_pg_loss.sum()\n        else:\n            on_pg_loss = on_pg_loss.mean()\n    else:\n        on_pg_loss = torch.tensor(0.0, device=log_prob.device)\n        stats['on_policy_prob'] = torch.tensor(0.0, device=log_prob.device)\n        stats['on_pg_clipfrac'] = torch.tensor(0.0, device=log_prob.device)\n        stats['ppo_kl'] = torch.tensor(0.0, device=log_prob.device)\n    \n    # ==================== COMBINE LOSSES ====================\n    # Weight losses by their respective sample counts\n    if total_valid_count > 0:\n        off_weight = off_policy_count / total_valid_count\n        on_weight = on_policy_count / total_valid_count\n        pg_loss = off_weight * off_pg_loss + on_weight * on_pg_loss\n    else:\n        pg_loss = torch.tensor(0.0, device=log_prob.device)\n    \n    # Prepare final output dictionary\n    result = {\n        'pg_loss': pg_loss,\n        'off_pg_loss': off_pg_loss,\n        'on_pg_loss': on_pg_loss,\n        'off_pg_clipfrac': stats['off_pg_clipfrac'],\n        'on_pg_clipfrac': stats['on_pg_clipfrac'],\n        'ppo_kl': stats['ppo_kl'],\n        'off_policy_prob': stats['off_policy_prob'],\n        'on_policy_prob': stats['on_policy_prob'],\n        'off_ratio_mean': stats.get('off_ratio_mean', torch.tensor(0.0, device=log_prob.device)),\n        'off_ratio_max_clip_frac': stats.get('off_ratio_max_clip_frac', torch.tensor(0.0, device=log_prob.device)),\n        'off_ratio_min_clip_frac': stats.get('off_ratio_min_clip_frac', torch.tensor(0.0, device=log_prob.device)),\n    }\n    \n    return result\n```"], "usage": {"prompt_tokens": 1239, "completion_tokens": 2929, "total_tokens": 4168, "per_completion": [{"prompt_tokens": 1239, "completion_tokens": 2929, "total_tokens": 4168}]}}
{"function_name": "compute_sft_pure_loss", "overview": "Computes pure SFT (Supervised Fine-Tuning) loss, i.e., negative log likelihood loss, used for supervised learning on off-policy samples in multi-task learning.", "function_signature": "", "input_parameters": "- `log_prob`: Model-predicted log probabilities with shape `[batch_size, response_length]`\n- `eos_mask`: Valid token mask with shape `[batch_size, response_length]`", "detailed_description": "Computes negative log probability for each token as loss value. Uses eos_mask to mask the loss, computing loss only at valid token positions. Averages the masked loss to obtain batch-level SFT loss.", "output": "- `sft_loss`: Scalar representing average SFT loss", "func_implementation": "", "test_code_path": "code/test_code/test_compute_sft_pure_loss.py", "implementation_location": "code/luffy/verl/verl/mix_src/mix_core_alg.py:line 7-10", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_sft_pure_loss`.\n\n**Function Overview:**\nComputes pure SFT (Supervised Fine-Tuning) loss, i.e., negative log likelihood loss, used for supervised learning on off-policy samples in multi-task learning.\n\n**Function Signature:**\n```python\n\n```\n\n**Input Parameters:**\n- `log_prob`: Model-predicted log probabilities with shape `[batch_size, response_length]`\n- `eos_mask`: Valid token mask with shape `[batch_size, response_length]`\n\n**Detailed Description:**\nComputes negative log probability for each token as loss value. Uses eos_mask to mask the loss, computing loss only at valid token positions. Averages the masked loss to obtain batch-level SFT loss.\n\n**Expected Output:**\n- `sft_loss`: Scalar representing average SFT loss\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\ndef compute_sft_pure_loss(log_prob: torch.Tensor, eos_mask: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Computes pure SFT (Supervised Fine-Tuning) loss, i.e., negative log likelihood loss,\n    used for supervised learning on off-policy samples in multi-task learning.\n    \n    Args:\n        log_prob: Model-predicted log probabilities with shape [batch_size, response_length]\n        eos_mask: Valid token mask with shape [batch_size, response_length]\n                 (1 for valid tokens, 0 for padding/invalid tokens)\n    \n    Returns:\n        sft_loss: Scalar representing average SFT loss\n    \"\"\"\n    # Negative log probability for each token position\n    # Since log_prob already contains log probabilities, we take negative to get loss\n    token_loss = -log_prob\n    \n    # Mask the loss: keep only valid token positions (where eos_mask == 1)\n    masked_loss = token_loss * eos_mask\n    \n    # Sum the loss across all valid tokens\n    total_loss = masked_loss.sum()\n    \n    # Count the number of valid tokens\n    num_valid_tokens = eos_mask.sum()\n    \n    # Avoid division by zero\n    if num_valid_tokens > 0:\n        # Average the loss over valid tokens\n        sft_loss = total_loss / num_valid_tokens\n    else:\n        # If no valid tokens, return zero loss\n        sft_loss = torch.tensor(0.0, device=log_prob.device)\n    \n    return sft_loss\n```"], "usage": {"prompt_tokens": 357, "completion_tokens": 332, "total_tokens": 689, "per_completion": [{"prompt_tokens": 357, "completion_tokens": 332, "total_tokens": 689}]}}
