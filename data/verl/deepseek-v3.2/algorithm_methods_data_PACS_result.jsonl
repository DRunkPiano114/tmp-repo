{"function_name": "compute_reward", "overview": "Computes \"reward\" for policy improvement, measuring log probability change of current policy relative to reference policy.", "function_signature": "def compute_reward(\nold_log_prob,\nlog_prob,\nresponse_mask,\nbeta=1.0,\nreward_method=\"1\",\n):", "input_parameters": "- `old_log_prob`: Reference policy token-level log probabilities, shape `[batch_size, response_length]`\n- `log_prob`: Current policy token-level log probabilities, shape `[batch_size, response_length]`\n- `response_mask`: Response portion mask, shape `[batch_size, response_length]`\n- `beta`: Reward scaling factor, controls aggressiveness of policy updates (default 1.0)\n- `reward_method`: Reward computation method, '1' or '2'", "detailed_description": "Function computes reward values in reinforcement learning based on specified method.\nMethod 1 computes sum of new-old log probability differences, measuring policy improvement magnitude.\nMethod 2 computes normalized sum of current log probabilities.\nIf an invalid method number is specified, function raises an exception.", "output": "- `reward`: Policy improvement reward, shape `[batch_size, 1]`", "func_implementation": "", "test_code_path": "code/test_code/test_compute_reward.py", "implementation_location": "code/src/pacs/pacs_core_algos.py:line 7-21", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_reward`.\n\n**Function Overview:**\nComputes \"reward\" for policy improvement, measuring log probability change of current policy relative to reference policy.\n\n**Function Signature:**\n```python\ndef compute_reward(\nold_log_prob,\nlog_prob,\nresponse_mask,\nbeta=1.0,\nreward_method=\"1\",\n):\n```\n\n**Input Parameters:**\n- `old_log_prob`: Reference policy token-level log probabilities, shape `[batch_size, response_length]`\n- `log_prob`: Current policy token-level log probabilities, shape `[batch_size, response_length]`\n- `response_mask`: Response portion mask, shape `[batch_size, response_length]`\n- `beta`: Reward scaling factor, controls aggressiveness of policy updates (default 1.0)\n- `reward_method`: Reward computation method, '1' or '2'\n\n**Detailed Description:**\nFunction computes reward values in reinforcement learning based on specified method.\nMethod 1 computes sum of new-old log probability differences, measuring policy improvement magnitude.\nMethod 2 computes normalized sum of current log probabilities.\nIf an invalid method number is specified, function raises an exception.\n\n**Expected Output:**\n- `reward`: Policy improvement reward, shape `[batch_size, 1]`\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport torch\n\ndef compute_reward(\n    old_log_prob: torch.Tensor,\n    log_prob: torch.Tensor,\n    response_mask: torch.Tensor,\n    beta: float = 1.0,\n    reward_method: str = \"1\",\n) -> torch.Tensor:\n    \"\"\"\n    Computes reward for policy improvement by measuring log probability change\n    of current policy relative to reference policy.\n\n    Args:\n        old_log_prob: Reference policy token-level log probabilities,\n                      shape [batch_size, response_length]\n        log_prob: Current policy token-level log probabilities,\n                  shape [batch_size, response_length]\n        response_mask: Response portion mask,\n                       shape [batch_size, response_length]\n        beta: Reward scaling factor, controls aggressiveness of policy updates\n        reward_method: Reward computation method, '1' or '2'\n\n    Returns:\n        reward: Policy improvement reward, shape [batch_size, 1]\n\n    Raises:\n        ValueError: If an invalid reward_method is specified\n    \"\"\"\n    # Ensure tensors are on the same device\n    device = old_log_prob.device\n    log_prob = log_prob.to(device)\n    response_mask = response_mask.to(device)\n\n    # Apply mask to focus only on response tokens\n    masked_old_log_prob = old_log_prob * response_mask\n    masked_log_prob = log_prob * response_mask\n\n    if reward_method == \"1\":\n        # Method 1: Sum of new-old log probability differences\n        # Measures policy improvement magnitude\n        log_prob_diff = masked_log_prob - masked_old_log_prob\n        # Sum over response length dimension\n        reward = torch.sum(log_prob_diff, dim=1, keepdim=True)\n        # Scale by beta factor\n        reward = reward * beta\n\n    elif reward_method == \"2\":\n        # Method 2: Normalized sum of current log probabilities\n        # Sum current log probabilities over response length\n        reward = torch.sum(masked_log_prob, dim=1, keepdim=True)\n        # Normalize by number of response tokens\n        num_response_tokens = torch.sum(response_mask, dim=1, keepdim=True)\n        # Avoid division by zero\n        num_response_tokens = torch.clamp(num_response_tokens, min=1.0)\n        reward = reward / num_response_tokens\n        # Scale by beta factor\n        reward = reward * beta\n\n    else:\n        raise ValueError(\n            f\"Invalid reward_method: {reward_method}. \"\n            \"Must be either '1' or '2'.\"\n        )\n\n    return reward\n```"], "usage": {"prompt_tokens": 440, "completion_tokens": 539, "total_tokens": 979, "per_completion": [{"prompt_tokens": 440, "completion_tokens": 539, "total_tokens": 979}]}, "results": [false], "pass_ratios": [0.0]}
{"function_name": "compute_weight", "overview": "Computes sample weights to handle class imbalance problem (uneven positive/negative sample ratio).", "function_signature": "def compute_weight(score, rollout_n, mode=\"question\"):", "input_parameters": "- `score`: Verifiable rewards (labels), shape `[batch_size, 1]`\n- `rollout_n`: Number of responses generated per prompt\n- `mode`: Weight computation mode, 'question'/'only_positive'/'only_negative'", "detailed_description": "Function assigns training weights to samples based on different modes. In \"question\" mode, function groups samples by rollout_n size, using class-balanced weights for groups with label imbalance to make positive and negative samples contribute equally to training. In \"only_positive\" mode, only positive samples (label 1) are assigned weight 1.0, negative samples get weight 0. In \"only_negative\" mode, the opposite applies: only negative samples (label 0) are assigned weight 1.0. If an invalid mode is specified, function raises an exception.", "output": "- `weight`: Sample weights, shape `[batch_size, 1]`", "func_implementation": "", "test_code_path": "code/test_code/test_compute_weight.py", "implementation_location": "code/src/pacs/pacs_core_algos.py:line 59-86", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_weight`.\n\n**Function Overview:**\nComputes sample weights to handle class imbalance problem (uneven positive/negative sample ratio).\n\n**Function Signature:**\n```python\ndef compute_weight(score, rollout_n, mode=\"question\"):\n```\n\n**Input Parameters:**\n- `score`: Verifiable rewards (labels), shape `[batch_size, 1]`\n- `rollout_n`: Number of responses generated per prompt\n- `mode`: Weight computation mode, 'question'/'only_positive'/'only_negative'\n\n**Detailed Description:**\nFunction assigns training weights to samples based on different modes. In \"question\" mode, function groups samples by rollout_n size, using class-balanced weights for groups with label imbalance to make positive and negative samples contribute equally to training. In \"only_positive\" mode, only positive samples (label 1) are assigned weight 1.0, negative samples get weight 0. In \"only_negative\" mode, the opposite applies: only negative samples (label 0) are assigned weight 1.0. If an invalid mode is specified, function raises an exception.\n\n**Expected Output:**\n- `weight`: Sample weights, shape `[batch_size, 1]`\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport torch\n\ndef compute_weight(score, rollout_n, mode=\"question\"):\n    \"\"\"\n    Computes sample weights to handle class imbalance problem.\n    \n    Args:\n        score: Verifiable rewards (labels), shape [batch_size, 1]\n        rollout_n: Number of responses generated per prompt\n        mode: Weight computation mode, 'question'/'only_positive'/'only_negative'\n    \n    Returns:\n        weight: Sample weights, shape [batch_size, 1]\n    \n    Raises:\n        ValueError: If an invalid mode is specified\n    \"\"\"\n    # Ensure score is a tensor\n    if not isinstance(score, torch.Tensor):\n        score = torch.tensor(score)\n    \n    batch_size = score.shape[0]\n    \n    # Initialize weights tensor with zeros\n    weight = torch.zeros_like(score, dtype=torch.float32)\n    \n    if mode == \"only_positive\":\n        # Only positive samples (label 1) get weight 1.0\n        weight[score == 1] = 1.0\n        \n    elif mode == \"only_negative\":\n        # Only negative samples (label 0) get weight 1.0\n        weight[score == 0] = 1.0\n        \n    elif mode == \"question\":\n        # Group samples by rollout_n size and compute class-balanced weights\n        \n        # Validate that batch_size is divisible by rollout_n\n        if batch_size % rollout_n != 0:\n            raise ValueError(f\"Batch size {batch_size} must be divisible by rollout_n {rollout_n}\")\n        \n        num_groups = batch_size // rollout_n\n        \n        # Reshape score to [num_groups, rollout_n, 1]\n        score_reshaped = score.view(num_groups, rollout_n, 1)\n        \n        # Initialize weights for each group\n        group_weights = torch.zeros((num_groups, rollout_n, 1), dtype=torch.float32)\n        \n        for i in range(num_groups):\n            group_scores = score_reshaped[i].squeeze()\n            \n            # Count positive and negative samples in the group\n            num_positives = (group_scores == 1).sum().item()\n            num_negatives = (group_scores == 0).sum().item()\n            \n            # Skip if no samples in the group\n            if num_positives + num_negatives == 0:\n                continue\n            \n            # Compute class-balanced weights\n            if num_positives > 0 and num_negatives > 0:\n                # Both classes present: assign weights to make them contribute equally\n                weight_positive = 1.0 / (2 * num_positives) if num_positives > 0 else 0\n                weight_negative = 1.0 / (2 * num_negatives) if num_negatives > 0 else 0\n                \n                # Assign weights to samples in the group\n                for j in range(rollout_n):\n                    if group_scores[j] == 1:\n                        group_weights[i, j, 0] = weight_positive\n                    elif group_scores[j] == 0:\n                        group_weights[i, j, 0] = weight_negative\n            else:\n                # Only one class present: assign equal weight to all samples\n                group_weights[i, :, 0] = 1.0 / rollout_n\n        \n        # Reshape back to original batch size\n        weight = group_weights.view(batch_size, 1)\n        \n    else:\n        raise ValueError(f\"Invalid mode '{mode}'. Must be one of: 'question', 'only_positive', 'only_negative'\")\n    \n    return weight\n```"], "usage": {"prompt_tokens": 437, "completion_tokens": 786, "total_tokens": 1223, "per_completion": [{"prompt_tokens": 437, "completion_tokens": 786, "total_tokens": 1223}]}, "results": [false], "pass_ratios": [0.0]}
{"function_name": "compute_pacs_loss", "overview": "Computes PACS's core loss function, using advantage function as logit and verifiable rewards as labels with binary cross-entropy loss.", "function_signature": "def compute_pacs_loss(\nprompts,\nold_log_prob,\nlog_prob,\ntoken_level_scores,\nresponse_mask,\nrollout_n,\nalgorithm_config=None,\n):", "input_parameters": "- `prompts`: Input prompts, shape `[batch_size, prompt_length]`\n- `old_log_prob`: Reference policy log probabilities, shape `[batch_size, response_length]`\n- `log_prob`: Current policy log probabilities, shape `[batch_size, response_length]`\n- `token_level_scores`: Verifiable rewards (labels), shape `[batch_size, response_length]`\n- `response_mask`: Response mask, shape `[batch_size, response_length]`\n- `rollout_n`: Number of responses generated per prompt\n- `algorithm_config`: Algorithm configuration object containing `beta`, `reward_method`, `adv_estimator`, `use_weight`, `weight_mode`, etc.", "detailed_description": "Function needs to verify that every rollout_n samples correspond to the same prompt, then compute reward values and advantage values (based on specified advantage estimator). Finally, function sums token-level scores and uses binary cross-entropy loss with advantage values as supervision signal and scores as predicted values to compute loss. If sample weighting is enabled, balanced weights are computed based on score labels to handle sample imbalance.", "output": "- `pacs_loss`: PACS loss value, scalar", "func_implementation": "", "test_code_path": "code/test_code/test_compute_pacs_loss.py", "implementation_location": "code/src/pacs/pacs_core_algos.py:line 89-138", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_pacs_loss`.\n\n**Function Overview:**\nComputes PACS's core loss function, using advantage function as logit and verifiable rewards as labels with binary cross-entropy loss.\n\n**Function Signature:**\n```python\ndef compute_pacs_loss(\nprompts,\nold_log_prob,\nlog_prob,\ntoken_level_scores,\nresponse_mask,\nrollout_n,\nalgorithm_config=None,\n):\n```\n\n**Input Parameters:**\n- `prompts`: Input prompts, shape `[batch_size, prompt_length]`\n- `old_log_prob`: Reference policy log probabilities, shape `[batch_size, response_length]`\n- `log_prob`: Current policy log probabilities, shape `[batch_size, response_length]`\n- `token_level_scores`: Verifiable rewards (labels), shape `[batch_size, response_length]`\n- `response_mask`: Response mask, shape `[batch_size, response_length]`\n- `rollout_n`: Number of responses generated per prompt\n- `algorithm_config`: Algorithm configuration object containing `beta`, `reward_method`, `adv_estimator`, `use_weight`, `weight_mode`, etc.\n\n**Detailed Description:**\nFunction needs to verify that every rollout_n samples correspond to the same prompt, then compute reward values and advantage values (based on specified advantage estimator). Finally, function sums token-level scores and uses binary cross-entropy loss with advantage values as supervision signal and scores as predicted values to compute loss. If sample weighting is enabled, balanced weights are computed based on score labels to handle sample imbalance.\n\n**Expected Output:**\n- `pacs_loss`: PACS loss value, scalar\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport torch\nimport torch.nn.functional as F\nfrom typing import Optional, Dict, Any\n\ndef compute_pacs_loss(\n    prompts: torch.Tensor,\n    old_log_prob: torch.Tensor,\n    log_prob: torch.Tensor,\n    token_level_scores: torch.Tensor,\n    response_mask: torch.Tensor,\n    rollout_n: int,\n    algorithm_config: Optional[Dict[str, Any]] = None,\n) -> torch.Tensor:\n    \"\"\"\n    Compute PACS's core loss function.\n    \n    Uses advantage function as logit and verifiable rewards as labels with binary cross-entropy loss.\n    \n    Args:\n        prompts: Input prompts, shape [batch_size, prompt_length]\n        old_log_prob: Reference policy log probabilities, shape [batch_size, response_length]\n        log_prob: Current policy log probabilities, shape [batch_size, response_length]\n        token_level_scores: Verifiable rewards (labels), shape [batch_size, response_length]\n        response_mask: Response mask, shape [batch_size, response_length]\n        rollout_n: Number of responses generated per prompt\n        algorithm_config: Algorithm configuration object containing:\n            - beta: KL penalty coefficient\n            - reward_method: Method for computing rewards\n            - adv_estimator: Advantage estimator type\n            - use_weight: Whether to use sample weighting\n            - weight_mode: Weight computation mode\n    \n    Returns:\n        pacs_loss: PACS loss value, scalar\n    \"\"\"\n    \n    # Set default algorithm config if not provided\n    if algorithm_config is None:\n        algorithm_config = {\n            'beta': 0.1,\n            'reward_method': 'sum',\n            'adv_estimator': 'gae',\n            'use_weight': False,\n            'weight_mode': 'balanced'\n        }\n    \n    # Extract config parameters\n    beta = algorithm_config.get('beta', 0.1)\n    reward_method = algorithm_config.get('reward_method', 'sum')\n    adv_estimator = algorithm_config.get('adv_estimator', 'gae')\n    use_weight = algorithm_config.get('use_weight', False)\n    weight_mode = algorithm_config.get('weight_mode', 'balanced')\n    \n    batch_size = prompts.size(0)\n    response_length = old_log_prob.size(1)\n    \n    # Verify that every rollout_n samples correspond to the same prompt\n    if batch_size % rollout_n != 0:\n        raise ValueError(f\"Batch size {batch_size} must be divisible by rollout_n {rollout_n}\")\n    \n    # Reshape tensors to group responses by prompt\n    # New shape: [num_prompts, rollout_n, response_length]\n    num_prompts = batch_size // rollout_n\n    old_log_prob_reshaped = old_log_prob.view(num_prompts, rollout_n, response_length)\n    log_prob_reshaped = log_prob.view(num_prompts, rollout_n, response_length)\n    token_level_scores_reshaped = token_level_scores.view(num_prompts, rollout_n, response_length)\n    response_mask_reshaped = response_mask.view(num_prompts, rollout_n, response_length)\n    \n    # Compute KL divergence between current and reference policy\n    kl_div = (log_prob - old_log_prob) * response_mask\n    kl_penalty = beta * kl_div.mean()\n    \n    # Compute reward values based on specified method\n    if reward_method == 'sum':\n        # Sum token-level scores for each response\n        rewards = (token_level_scores_reshaped * response_mask_reshaped).sum(dim=-1)\n    elif reward_method == 'mean':\n        # Mean token-level scores for each response\n        mask_sum = response_mask_reshaped.sum(dim=-1, keepdim=True)\n        mask_sum = torch.where(mask_sum > 0, mask_sum, torch.ones_like(mask_sum))\n        rewards = (token_level_scores_reshaped * response_mask_reshaped).sum(dim=-1) / mask_sum.squeeze(-1)\n    else:\n        raise ValueError(f\"Unknown reward_method: {reward_method}\")\n    \n    # Compute advantage values based on specified advantage estimator\n    if adv_estimator == 'gae':\n        # Generalized Advantage Estimation\n        # For simplicity, we use a basic advantage computation\n        # In practice, this would involve temporal difference and lambda parameter\n        advantages = rewards - rewards.mean(dim=1, keepdim=True)\n    elif adv_estimator == 'vtrace':\n        # V-trace advantage estimation\n        # Simplified version - actual implementation would be more complex\n        advantages = rewards - rewards.mean(dim=1, keepdim=True)\n    else:\n        raise ValueError(f\"Unknown adv_estimator: {adv_estimator}\")\n    \n    # Reshape advantages to match token-level shape\n    advantages_expanded = advantages.unsqueeze(-1).expand(-1, -1, response_length)\n    advantages_flat = advantages_expanded.reshape(batch_size, response_length)\n    \n    # Apply response mask to advantages\n    advantages_masked = advantages_flat * response_mask\n    \n    # Compute binary cross-entropy loss\n    # Advantages as logits, token_level_scores as labels\n    # Note: BCEWithLogitsLoss expects values between 0 and 1 for labels\n    # We assume token_level_scores are already in [0, 1] range\n    \n    if use_weight:\n        # Compute balanced weights based on score labels\n        if weight_mode == 'balanced':\n            # Count positive and negative samples\n            pos_mask = (token_level_scores > 0.5).float()\n            neg_mask = (token_level_scores <= 0.5).float()\n            \n            pos_count = (pos_mask * response_mask).sum()\n            neg_count = (neg_mask * response_mask).sum()\n            \n            # Avoid division by zero\n            pos_weight = neg_count / (pos_count + 1e-8)\n            neg_weight = pos_count / (neg_count + 1e-8)\n            \n            # Create weight tensor\n            weights = pos_weight * pos_mask + neg_weight * neg_mask\n            weights = weights * response_mask\n        else:\n            raise ValueError(f\"Unknown weight_mode: {weight_mode}\")\n        \n        # Weighted binary cross-entropy loss\n        bce_loss = F.binary_cross_entropy_with_logits(\n            advantages_masked,\n            token_level_scores,\n            weight=weights,\n            reduction='sum'\n        ) / (response_mask.sum() + 1e-8)\n    else:\n        # Standard binary cross-entropy loss\n        bce_loss = F.binary_cross_entropy_with_logits(\n            advantages_masked,\n            token_level_scores,\n            reduction='sum'\n        ) / (response_mask.sum() + 1e-8)\n    \n    # Combine BCE loss with KL penalty\n    pacs_loss = bce_loss + kl_penalty\n    \n    return pacs_loss\n```"], "usage": {"prompt_tokens": 517, "completion_tokens": 1465, "total_tokens": 1982, "per_completion": [{"prompt_tokens": 517, "completion_tokens": 1465, "total_tokens": 1982}]}, "results": [false], "pass_ratios": [0.0]}
