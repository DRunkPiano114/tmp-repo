{"function_name": "ProcessRewardModelWorker._forward_micro_batch", "overview": "Computes token-level scores from the Process Reward Model (PRM) and performs approximate weighting for min-form credit assignment based on configuration.", "function_signature": "def _forward_micro_batch(self, micro_batch):", "input_parameters": "- `micro_batch`: Micro-batch data containing `input_ids`, `attention_mask`, `position_ids`, `responses`, `reward_mask`, etc.\n- `response_length`: Response sequence length (inferred from batch tensors)\n- `temperature`: Temperature for approximate min-form (when `reward_model.credit_assignment` is a float)", "detailed_description": "Performs forward pass on the Process Reward Model (PRM), computing token-level scores via softmax to obtain the positive-negative class probability difference. Uses masking to retain only valid scores from the response portion.\nIf depadding or sequence parallelism optimization is enabled, padding must be removed before computation and restored afterward.\nCredit assignment weighting executes different strategies based on configuration mode: min-form, strict min-form, or gamma-decay. Lower-scoring tokens should receive higher weights.", "output": "- `rm_score`: Token-level process reward scores (shape `[batch_size, response_length]`)", "func_implementation": "", "test_code_path": "code/test_code/test_forward_micro_batch.py", "implementation_location": "code/verl/workers/fsdp_workers.py:line 1462-1539", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `ProcessRewardModelWorker._forward_micro_batch`.\n\n**Function Overview:**\nComputes token-level scores from the Process Reward Model (PRM) and performs approximate weighting for min-form credit assignment based on configuration.\n\n**Function Signature:**\n```python\ndef _forward_micro_batch(self, micro_batch):\n```\n\n**Input Parameters:**\n- `micro_batch`: Micro-batch data containing `input_ids`, `attention_mask`, `position_ids`, `responses`, `reward_mask`, etc.\n- `response_length`: Response sequence length (inferred from batch tensors)\n- `temperature`: Temperature for approximate min-form (when `reward_model.credit_assignment` is a float)\n\n**Detailed Description:**\nPerforms forward pass on the Process Reward Model (PRM), computing token-level scores via softmax to obtain the positive-negative class probability difference. Uses masking to retain only valid scores from the response portion.\nIf depadding or sequence parallelism optimization is enabled, padding must be removed before computation and restored afterward.\nCredit assignment weighting executes different strategies based on configuration mode: min-form, strict min-form, or gamma-decay. Lower-scoring tokens should receive higher weights.\n\n**Expected Output:**\n- `rm_score`: Token-level process reward scores (shape `[batch_size, response_length]`)\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\ndef _forward_micro_batch(self, micro_batch):\n    \"\"\"\n    Compute token-level scores from Process Reward Model (PRM) with \n    approximate weighting for min-form credit assignment.\n    \n    Args:\n        micro_batch: Dictionary containing:\n            - input_ids: Token IDs [batch_size, seq_len]\n            - attention_mask: Attention mask [batch_size, seq_len]\n            - position_ids: Position IDs [batch_size, seq_len]\n            - responses: Response tokens [batch_size, response_length]\n            - reward_mask: Mask for response portion [batch_size, seq_len]\n            - (other optional fields)\n        response_length: Inferred from batch tensors\n        temperature: Temperature for approximate min-form (when credit_assignment is float)\n    \n    Returns:\n        rm_score: Token-level process reward scores [batch_size, response_length]\n    \"\"\"\n    # Extract batch data\n    input_ids = micro_batch[\"input_ids\"]\n    attention_mask = micro_batch[\"attention_mask\"]\n    position_ids = micro_batch.get(\"position_ids\", None)\n    reward_mask = micro_batch[\"reward_mask\"]\n    \n    # Get response length from responses tensor\n    response_length = micro_batch[\"responses\"].shape[1]\n    \n    # Get temperature from config if available\n    temperature = getattr(self.config.reward_model, 'temperature', 1.0)\n    \n    # Handle depadding/sequence parallelism optimization\n    if hasattr(self.config, 'depadding_enabled') and self.config.depadding_enabled:\n        # Remove padding before forward pass\n        non_pad_mask = attention_mask.bool()\n        input_ids = input_ids[non_pad_mask].unsqueeze(0) if input_ids.dim() > 1 else input_ids[non_pad_mask]\n        if position_ids is not None:\n            position_ids = position_ids[non_pad_mask].unsqueeze(0) if position_ids.dim() > 1 else position_ids[non_pad_mask]\n        attention_mask = attention_mask[non_pad_mask].unsqueeze(0) if attention_mask.dim() > 1 else attention_mask[non_pad_mask]\n        reward_mask = reward_mask[non_pad_mask].unsqueeze(0) if reward_mask.dim() > 1 else reward_mask[non_pad_mask]\n    \n    # Forward pass through Process Reward Model\n    with torch.no_grad():\n        # Get logits from PRM [batch_size, seq_len, 2] (positive/negative classes)\n        prm_output = self.reward_model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids\n        )\n        logits = prm_output.logits  # Shape: [batch_size, seq_len, 2]\n    \n    # Compute token-level scores via softmax class probability difference\n    # Softmax over class dimension (positive vs negative)\n    probs = torch.softmax(logits, dim=-1)  # Shape: [batch_size, seq_len, 2]\n    # Difference between positive and negative class probabilities\n    token_scores = probs[..., 0] - probs[..., 1]  # Shape: [batch_size, seq_len]\n    \n    # Apply reward mask to retain only valid response portion scores\n    # reward_mask is 1 for response tokens, 0 elsewhere\n    masked_scores = token_scores * reward_mask  # Shape: [batch_size, seq_len]\n    \n    # Extract response portion scores\n    # First find the response start positions\n    batch_size = masked_scores.shape[0]\n    response_scores = []\n    \n    for i in range(batch_size):\n        # Find indices where reward_mask is 1 (response tokens)\n        response_indices = torch.where(reward_mask[i] == 1)[0]\n        # Take first response_length tokens (or all available)\n        response_indices = response_indices[:response_length]\n        # Extract scores for these positions\n        batch_scores = masked_scores[i, response_indices]\n        # Pad or truncate to exact response_length\n        if len(batch_scores) < response_length:\n            padding = torch.zeros(response_length - len(batch_scores), \n                                 device=batch_scores.device)\n            batch_scores = torch.cat([batch_scores, padding])\n        else:\n            batch_scores = batch_scores[:response_length]\n        response_scores.append(batch_scores)\n    \n    response_scores = torch.stack(response_scores, dim=0)  # Shape: [batch_size, response_length]\n    \n    # Apply credit assignment weighting based on configuration\n    credit_assignment = getattr(self.config.reward_model, 'credit_assignment', 'min_form')\n    \n    if credit_assignment == 'min_form' or isinstance(credit_assignment, float):\n        # Min-form credit assignment: lower-scoring tokens get higher weights\n        # For approximate min-form with temperature\n        if isinstance(credit_assignment, float):\n            temp = credit_assignment  # Use provided temperature\n        else:\n            temp = temperature\n        \n        # Apply temperature-scaled softmin (softmax on negative scores)\n        # Lower scores -> higher weights after softmax on negatives\n        weights = torch.softmax(-response_scores / temp, dim=-1)\n        weighted_scores = response_scores * weights\n        \n    elif credit_assignment == 'strict_min_form':\n        # Strict min-form: weight = 1 for minimum score token, 0 elsewhere\n        min_indices = torch.argmin(response_scores, dim=-1)\n        weights = torch.zeros_like(response_scores)\n        for i in range(batch_size):\n            weights[i, min_indices[i]] = 1.0\n        weighted_scores = response_scores * weights\n        \n    elif credit_assignment == 'gamma_decay':\n        # Gamma-decay: exponential weighting based on score ranking\n        # Sort scores in ascending order (lower scores first)\n        sorted_scores, sorted_indices = torch.sort(response_scores, dim=-1)\n        gamma = getattr(self.config.reward_model, 'gamma', 0.9)\n        \n        # Create gamma-decay weights: gamma^0, gamma^1, gamma^2, ...\n        decay_weights = torch.pow(gamma, torch.arange(response_length, device=response_scores.device))\n        decay_weights = decay_weights / decay_weights.sum()  # Normalize\n        \n        # Apply weights to sorted scores\n        sorted_weighted = sorted_scores * decay_weights\n        \n        # Restore original order\n        weights = torch.zeros_like(response_scores)\n        for i in range(batch_size):\n            weights[i, sorted_indices[i]] = decay_weights\n        weighted_scores = response_scores * weights\n        \n    else:\n        # No credit assignment weighting\n        weighted_scores = response_scores\n    \n    # Restore padding if depadding was applied\n    if hasattr(self.config, 'depadding_enabled') and self.config.depadding_enabled:\n        # In depadding mode, we already have the correct response_length scores\n        rm_score = weighted_scores\n    else:\n        rm_score = weighted_scores\n    \n    return rm_score  # Shape: [batch_size, response_length]\n```"], "usage": {"prompt_tokens": 455, "completion_tokens": 1544, "total_tokens": 1999, "per_completion": [{"prompt_tokens": 455, "completion_tokens": 1544, "total_tokens": 1999}]}}
{"function_name": "ProcessRewardModelWorker.compute_rm_score", "overview": "Batch-constructs PRM inputs (splitting by steps and appending newlines after each step), performs inference with (optional) variable-length sharding/backfilling, and aggregates to obtain batch-level token-level `rm_scores`.", "function_signature": "def compute_rm_score(self, data:DataProto):", "input_parameters": "- `data`: `DataProto` containing `responses`, `reward_mask`, etc.;", "detailed_description": "Splits response sequences by steps to construct input format for the process reward model.\nIn dynamic batch mode, reorganizes data into multiple mini-batches based on total token count limit and records original order; in fixed batch mode, evenly splits by sample count.\nCalls forward function for each mini-batch to compute token-level scores, concatenating results from all batches. In dynamic batch mode, restores results to original order based on recorded indices.", "output": "- `DataProto`: `batch['rm_scores']` with shape `[batch_size, response_length]`", "func_implementation": "", "test_code_path": "code/test_code/test_compute_rm_score.py", "implementation_location": "code/verl/workers/fsdp_workers.py:line 1541-1594", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `ProcessRewardModelWorker.compute_rm_score`.\n\n**Function Overview:**\nBatch-constructs PRM inputs (splitting by steps and appending newlines after each step), performs inference with (optional) variable-length sharding/backfilling, and aggregates to obtain batch-level token-level `rm_scores`.\n\n**Function Signature:**\n```python\ndef compute_rm_score(self, data:DataProto):\n```\n\n**Input Parameters:**\n- `data`: `DataProto` containing `responses`, `reward_mask`, etc.;\n\n**Detailed Description:**\nSplits response sequences by steps to construct input format for the process reward model.\nIn dynamic batch mode, reorganizes data into multiple mini-batches based on total token count limit and records original order; in fixed batch mode, evenly splits by sample count.\nCalls forward function for each mini-batch to compute token-level scores, concatenating results from all batches. In dynamic batch mode, restores results to original order based on recorded indices.\n\n**Expected Output:**\n- `DataProto`: `batch['rm_scores']` with shape `[batch_size, response_length]`\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\ndef compute_rm_score(self, data: DataProto):\n    \"\"\"\n    Batch-constructs PRM inputs by splitting responses by steps, \n    performs inference with optional dynamic batching, and aggregates\n    token-level reward model scores.\n    \n    Args:\n        data: DataProto containing:\n            - responses: List of response sequences\n            - reward_mask: Mask indicating valid reward positions\n            - Other necessary fields for PRM input construction\n    \n    Returns:\n        DataProto with added 'rm_scores' field of shape [batch_size, response_length]\n    \"\"\"\n    # Extract necessary data from DataProto\n    responses = data.responses\n    batch_size = len(responses)\n    \n    # Step 1: Construct PRM inputs by splitting responses by steps\n    prm_inputs = []\n    step_indices = []  # Track step boundaries for each response\n    \n    for response in responses:\n        # Split response by steps (assuming steps are separated by special tokens or markers)\n        # Implementation depends on specific step delimiter format\n        steps = self._split_response_by_steps(response)\n        \n        # Append newline after each step as per PRM input format\n        prm_input = self._construct_prm_input(steps)\n        prm_inputs.append(prm_input)\n        \n        # Record step boundaries for later aggregation\n        step_indices.append(self._get_step_boundaries(steps))\n    \n    # Step 2: Batch construction based on batching mode\n    if self.dynamic_batching:\n        # Dynamic batch mode: reorganize by total token count limit\n        mini_batches, batch_indices = self._create_dynamic_batches(\n            prm_inputs, self.max_tokens_per_batch\n        )\n    else:\n        # Fixed batch mode: evenly split by sample count\n        mini_batches, batch_indices = self._create_fixed_batches(\n            prm_inputs, self.batch_size\n        )\n    \n    # Step 3: Perform forward pass for each mini-batch\n    all_scores = []\n    for batch_idx, mini_batch in enumerate(mini_batches):\n        # Call forward function for the current mini-batch\n        batch_scores = self._forward_prm(mini_batch)\n        all_scores.append(batch_scores)\n    \n    # Step 4: Aggregate results from all batches\n    if self.dynamic_batching:\n        # In dynamic batch mode, restore to original order\n        aggregated_scores = self._restore_dynamic_order(\n            all_scores, batch_indices, batch_size\n        )\n    else:\n        # In fixed batch mode, simply concatenate\n        aggregated_scores = torch.cat(all_scores, dim=0)\n    \n    # Step 5: Reshape to token-level scores for each response\n    rm_scores = []\n    for i in range(batch_size):\n        # Use step_indices to map aggregated scores back to token positions\n        token_scores = self._map_to_token_level(\n            aggregated_scores[i], step_indices[i], len(responses[i])\n        )\n        rm_scores.append(token_scores)\n    \n    # Convert to tensor and add to data\n    data.rm_scores = torch.stack(rm_scores, dim=0)\n    \n    return data\n\n# Helper methods (implementation details would depend on specific requirements)\n\ndef _split_response_by_steps(self, response):\n    \"\"\"Split response sequence into individual steps.\"\"\"\n    # Implementation depends on how steps are delimited in the response\n    # Example: splitting by special token or step markers\n    steps = response.split(self.step_delimiter)\n    return steps\n\ndef _construct_prm_input(self, steps):\n    \"\"\"Construct PRM input by appending newline after each step.\"\"\"\n    prm_input = []\n    for step in steps:\n        prm_input.extend(step)\n        prm_input.append(self.newline_token)\n    return prm_input\n\ndef _get_step_boundaries(self, steps):\n    \"\"\"Get token indices for step boundaries.\"\"\"\n    boundaries = []\n    current_idx = 0\n    for step in steps:\n        boundaries.append((current_idx, current_idx + len(step)))\n        current_idx += len(step) + 1  # +1 for newline token\n    return boundaries\n\ndef _create_dynamic_batches(self, inputs, max_tokens):\n    \"\"\"\n    Create dynamic batches based on token count limit.\n    Returns batches and indices for restoring original order.\n    \"\"\"\n    batches = []\n    batch_indices = []\n    \n    current_batch = []\n    current_indices = []\n    current_token_count = 0\n    \n    for idx, inp in enumerate(inputs):\n        token_count = len(inp)\n        \n        if current_token_count + token_count > max_tokens and current_batch:\n            # Start new batch\n            batches.append(current_batch)\n            batch_indices.append(current_indices)\n            current_batch = [inp]\n            current_indices = [idx]\n            current_token_count = token_count\n        else:\n            # Add to current batch\n            current_batch.append(inp)\n            current_indices.append(idx)\n            current_token_count += token_count\n    \n    # Add the last batch\n    if current_batch:\n        batches.append(current_batch)\n        batch_indices.append(current_indices)\n    \n    return batches, batch_indices\n\ndef _create_fixed_batches(self, inputs, batch_size):\n    \"\"\"Create fixed-size batches.\"\"\"\n    batches = []\n    batch_indices = []\n    \n    for i in range(0, len(inputs), batch_size):\n        batch = inputs[i:i + batch_size]\n        indices = list(range(i, min(i + batch_size, len(inputs))))\n        batches.append(batch)\n        batch_indices.append(indices)\n    \n    return batches, batch_indices\n\ndef _forward_prm(self, batch_inputs):\n    \"\"\"Forward pass through the process reward model.\"\"\"\n    # Implementation depends on the specific PRM model\n    with torch.no_grad():\n        outputs = self.reward_model(batch_inputs)\n    return outputs.scores\n\ndef _restore_dynamic_order(self, all_scores, batch_indices, batch_size):\n    \"\"\"Restore scores to original order after dynamic batching.\"\"\"\n    restored_scores = torch.zeros(batch_size, dtype=torch.float32)\n    \n    for batch_scores, indices in zip(all_scores, batch_indices):\n        for score, idx in zip(batch_scores, indices):\n            restored_scores[idx] = score\n    \n    return restored_scores\n\ndef _map_to_token_level(self, aggregated_score, step_boundaries, response_length):\n    \"\"\"Map aggregated step scores to token-level scores.\"\"\"\n    token_scores = torch.zeros(response_length, dtype=torch.float32)\n    \n    for (start, end), step_score in zip(step_boundaries, aggregated_score):\n        # Assign the same step score to all tokens in that step\n        token_scores[start:end] = step_score\n    \n    return token_scores\n```"], "usage": {"prompt_tokens": 423, "completion_tokens": 1475, "total_tokens": 1898, "per_completion": [{"prompt_tokens": 423, "completion_tokens": 1475, "total_tokens": 1898}]}}
{"function_name": "compute_return", "overview": "Computes token-level return sequences based on `method`", "function_signature": "def compute_return(token_level_rewards, eos_mask, method='sum', gamma=1.0):", "input_parameters": "- `token_level_rewards`: `[batch_size, response_length]`\n- `eos_mask`: `[batch_size, response_length]`\n- `method`: `'sum'|'min'`\n- `gamma`: Discount factor (applicable in `'sum'` mode)", "detailed_description": "Computes token-level return sequences based on method, using masking to ensure only valid positions are processed.\n\"method\" includes \"sum\" and \"min\".", "output": "- `returns`: `[batch_size, response_length]`", "func_implementation": "", "test_code_path": "code/test_code/test_compute_return.py", "implementation_location": "code/verl/trainer/ppo/core_algos.py:line 70-88", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_return`.\n\n**Function Overview:**\nComputes token-level return sequences based on `method`\n\n**Function Signature:**\n```python\ndef compute_return(token_level_rewards, eos_mask, method='sum', gamma=1.0):\n```\n\n**Input Parameters:**\n- `token_level_rewards`: `[batch_size, response_length]`\n- `eos_mask`: `[batch_size, response_length]`\n- `method`: `'sum'|'min'`\n- `gamma`: Discount factor (applicable in `'sum'` mode)\n\n**Detailed Description:**\nComputes token-level return sequences based on method, using masking to ensure only valid positions are processed.\n\"method\" includes \"sum\" and \"min\".\n\n**Expected Output:**\n- `returns`: `[batch_size, response_length]`\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport torch\n\ndef compute_return(token_level_rewards, eos_mask, method='sum', gamma=1.0):\n    \"\"\"\n    Computes token-level return sequences based on specified method.\n    \n    Args:\n        token_level_rewards: Tensor of shape [batch_size, response_length]\n            containing rewards for each token position.\n        eos_mask: Tensor of shape [batch_size, response_length]\n            where True/1 indicates end-of-sequence tokens (EOS).\n        method: String specifying the computation method.\n            'sum': Sum of future discounted rewards (including current).\n            'min': Minimum reward from current position to EOS.\n        gamma: Discount factor for 'sum' method (default: 1.0).\n    \n    Returns:\n        returns: Tensor of shape [batch_size, response_length]\n            containing computed returns for each token position.\n    \"\"\"\n    batch_size, response_length = token_level_rewards.shape\n    \n    # Initialize returns tensor with zeros\n    returns = torch.zeros_like(token_level_rewards)\n    \n    # Process each sequence in the batch\n    for b in range(batch_size):\n        # Find the position of the first EOS token in this sequence\n        eos_positions = torch.where(eos_mask[b])[0]\n        \n        if len(eos_positions) == 0:\n            # If no EOS token found, use the entire sequence\n            valid_length = response_length\n        else:\n            # Use the first EOS position as the end of valid sequence\n            valid_length = eos_positions[0].item() + 1  # Include EOS token\n        \n        # Extract valid rewards for this sequence\n        valid_rewards = token_level_rewards[b, :valid_length]\n        \n        if method == 'sum':\n            # Compute discounted sum of future rewards (including current)\n            # We'll compute this by iterating backward from the end\n            discounted_sum = torch.zeros_like(valid_rewards)\n            future_sum = 0.0\n            \n            # Traverse from the last valid token to the first\n            for t in range(valid_length - 1, -1, -1):\n                # Current reward + discounted future sum\n                future_sum = valid_rewards[t] + gamma * future_sum\n                discounted_sum[t] = future_sum\n            \n            # Pad with zeros for positions beyond valid_length\n            returns[b, :valid_length] = discounted_sum\n            \n        elif method == 'min':\n            # Compute minimum reward from current position to EOS\n            min_rewards = torch.zeros_like(valid_rewards)\n            \n            # For each position, find the minimum reward from current to end\n            for t in range(valid_length):\n                # Extract rewards from current position to the end\n                future_rewards = valid_rewards[t:]\n                min_rewards[t] = torch.min(future_rewards)\n            \n            # Pad with zeros for positions beyond valid_length\n            returns[b, :valid_length] = min_rewards\n            \n        else:\n            raise ValueError(f\"Unknown method: {method}. Supported methods: 'sum', 'min'\")\n    \n    return returns\n```"], "usage": {"prompt_tokens": 357, "completion_tokens": 654, "total_tokens": 1011, "per_completion": [{"prompt_tokens": 357, "completion_tokens": 654, "total_tokens": 1011}]}}
