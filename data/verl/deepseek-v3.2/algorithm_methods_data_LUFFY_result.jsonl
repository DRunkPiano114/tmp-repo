{"function_name": "compute_grpo_outcome_advantage_split", "overview": "Computes group GRPO (Group Relative Policy Optimization) advantage estimation, with the key innovation of using only on-policy samples to compute within-group baseline.", "function_signature": "def compute_grpo_outcome_advantage_split(token_level_rewards: torch.Tensor,\neos_mask: torch.Tensor,\nindex: torch.Tensor,\non_policy_mask: torch.Tensor,\nepsilon: float = 1e-6,\nuse_std: bool = True):", "input_parameters": "- `token_level_rewards`: Token-level reward tensor with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `eos_mask`: Valid token mask with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `index`: Sample grouping indices, n responses from the same prompt share the same index. **Note: This parameter should be a Python list or indexable sequence (e.g., `[0, 0, 1, 1]`), not `torch.Tensor`, as index values will be used as dictionary keys**\n- `on_policy_mask`: Boolean mask with shape `[batch_size]`, type `torch.Tensor`, marking which samples are on-policy generated\n- `epsilon`: Numerical stability constant, default 1e-6\n- `use_std`: Whether to use standard deviation for normalization, default True", "detailed_description": "This function needs to sum token-level reward tensors along the time dimension for each response sequence to obtain sequence-level total scores.\nThe main implementation performs grouped normalization (including both on-policy and off-policy) for multiple responses from the same prompt, computing relative advantage values. During normalization, only samples marked as on-policy are selected to compute the group's mean and standard deviation. If a group has only one on-policy sample, set mean to 0 and standard deviation to 1; if there are multiple samples, compute statistics normally. For cases where standard deviation is 0, replace it with 1 to avoid division by zero errors.\nIf standard deviation normalization is enabled, compute (score - mean) / (std + epsilon); otherwise only subtract the mean. Finally, expand sequence-level advantage values to token-level advantage tensors.", "output": "- `advantages`: Token-level advantage tensor with shape `[batch_size, response_length]`\n- `returns`: Token-level return tensor with shape `[batch_size, response_length]`, same as advantages", "func_implementation": "", "test_code_path": "code/test_code/test_compute_grpo_outcome_advantage_split.py", "implementation_location": "code/luffy/verl/verl/mix_src/mix_core_alg.py:line 12-67", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_grpo_outcome_advantage_split`.\n\n**Function Overview:**\nComputes group GRPO (Group Relative Policy Optimization) advantage estimation, with the key innovation of using only on-policy samples to compute within-group baseline.\n\n**Function Signature:**\n```python\ndef compute_grpo_outcome_advantage_split(token_level_rewards: torch.Tensor,\neos_mask: torch.Tensor,\nindex: torch.Tensor,\non_policy_mask: torch.Tensor,\nepsilon: float = 1e-6,\nuse_std: bool = True):\n```\n\n**Input Parameters:**\n- `token_level_rewards`: Token-level reward tensor with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `eos_mask`: Valid token mask with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `index`: Sample grouping indices, n responses from the same prompt share the same index. **Note: This parameter should be a Python list or indexable sequence (e.g., `[0, 0, 1, 1]`), not `torch.Tensor`, as index values will be used as dictionary keys**\n- `on_policy_mask`: Boolean mask with shape `[batch_size]`, type `torch.Tensor`, marking which samples are on-policy generated\n- `epsilon`: Numerical stability constant, default 1e-6\n- `use_std`: Whether to use standard deviation for normalization, default True\n\n**Detailed Description:**\nThis function needs to sum token-level reward tensors along the time dimension for each response sequence to obtain sequence-level total scores.\nThe main implementation performs grouped normalization (including both on-policy and off-policy) for multiple responses from the same prompt, computing relative advantage values. During normalization, only samples marked as on-policy are selected to compute the group's mean and standard deviation. If a group has only one on-policy sample, set mean to 0 and standard deviation to 1; if there are multiple samples, compute statistics normally. For cases where standard deviation is 0, replace it with 1 to avoid division by zero errors.\nIf standard deviation normalization is enabled, compute (score - mean) / (std + epsilon); otherwise only subtract the mean. Finally, expand sequence-level advantage values to token-level advantage tensors.\n\n**Expected Output:**\n- `advantages`: Token-level advantage tensor with shape `[batch_size, response_length]`\n- `returns`: Token-level return tensor with shape `[batch_size, response_length]`, same as advantages\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport torch\nfrom typing import Tuple\n\ndef compute_grpo_outcome_advantage_split(\n    token_level_rewards: torch.Tensor,\n    eos_mask: torch.Tensor,\n    index: list,\n    on_policy_mask: torch.Tensor,\n    epsilon: float = 1e-6,\n    use_std: bool = True\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Compute group GRPO advantage estimation using only on-policy samples for within-group baseline.\n    \n    Args:\n        token_level_rewards: Token-level reward tensor with shape [batch_size, response_length]\n        eos_mask: Valid token mask with shape [batch_size, response_length] (1 for valid, 0 for invalid)\n        index: Sample grouping indices as a list (e.g., [0, 0, 1, 1])\n        on_policy_mask: Boolean mask with shape [batch_size] marking on-policy samples\n        epsilon: Numerical stability constant\n        use_std: Whether to use standard deviation for normalization\n        \n    Returns:\n        advantages: Token-level advantage tensor with shape [batch_size, response_length]\n        returns: Token-level return tensor with shape [batch_size, response_length] (same as advantages)\n    \"\"\"\n    # Step 1: Compute sequence-level total scores by summing token-level rewards\n    # Only sum over valid tokens (where eos_mask == 1)\n    sequence_scores = (token_level_rewards * eos_mask).sum(dim=1)  # [batch_size]\n    \n    # Step 2: Group samples by index and compute statistics using only on-policy samples\n    group_stats = {}\n    \n    # First, organize samples by group index\n    for i, group_idx in enumerate(index):\n        if group_idx not in group_stats:\n            group_stats[group_idx] = {\n                'indices': [],\n                'on_policy_indices': [],\n                'on_policy_scores': []\n            }\n        group_stats[group_idx]['indices'].append(i)\n        if on_policy_mask[i]:\n            group_stats[group_idx]['on_policy_indices'].append(i)\n            group_stats[group_idx]['on_policy_scores'].append(sequence_scores[i])\n    \n    # Step 3: Compute mean and std for each group using only on-policy samples\n    group_means = {}\n    group_stds = {}\n    \n    for group_idx, stats in group_stats.items():\n        on_policy_scores = stats['on_policy_scores']\n        \n        if len(on_policy_scores) == 0:\n            # No on-policy samples in this group, use default values\n            group_means[group_idx] = 0.0\n            group_stds[group_idx] = 1.0\n        elif len(on_policy_scores) == 1:\n            # Single on-policy sample: mean=0, std=1\n            group_means[group_idx] = 0.0\n            group_stds[group_idx] = 1.0\n        else:\n            # Multiple on-policy samples: compute actual statistics\n            scores_tensor = torch.stack(on_policy_scores)\n            mean = scores_tensor.mean()\n            std = scores_tensor.std()\n            \n            # Handle zero standard deviation\n            if std == 0:\n                std = 1.0\n                \n            group_means[group_idx] = mean\n            group_stds[group_idx] = std\n    \n    # Step 4: Compute sequence-level advantages for all samples\n    sequence_advantages = torch.zeros_like(sequence_scores)\n    \n    for i, group_idx in enumerate(index):\n        mean = group_means[group_idx]\n        std = group_stds[group_idx]\n        \n        if use_std:\n            sequence_advantages[i] = (sequence_scores[i] - mean) / (std + epsilon)\n        else:\n            sequence_advantages[i] = sequence_scores[i] - mean\n    \n    # Step 5: Expand sequence-level advantages to token-level\n    # Repeat the sequence advantage for each token position\n    advantages = sequence_advantages.unsqueeze(1).expand_as(token_level_rewards)\n    \n    # Apply eos_mask to ensure invalid tokens have zero advantage\n    advantages = advantages * eos_mask\n    \n    # Returns are the same as advantages in this implementation\n    returns = advantages.clone()\n    \n    return advantages, returns\n```"], "usage": {"prompt_tokens": 721, "completion_tokens": 2717, "total_tokens": 3438, "per_completion": [{"prompt_tokens": 721, "completion_tokens": 2717, "total_tokens": 3438}]}, "results": [false], "pass_ratios": [0.0]}
{"function_name": "compute_token_on_off_policy_loss", "overview": "Computes mixed policy loss, combining on-policy PPO clip loss and off-policy importance sampling loss, supporting multiple policy shaping methods to emphasize low-probability but critical actions.", "function_signature": "def compute_token_on_off_policy_loss(\nold_log_prob,\nlog_prob,\nadvantages,\neos_mask,\ncliprange,\nclip_upper_bound,\nprefix_mask,\noff_cliprange,\noff_normalize=False,\noff_abs_cliprange=None,\noff_max_clip=None,\noff_min_clip=None,\nall_max_clip=None,\noff_policy_reshape=\"no_reshape\",\noff_policy_reshape_weight=1.0,\noff_policy_reshape_pow_exp=0.5,\non_policy_reshape=\"no_reshape\",\non_policy_reshape_weight=1.0,\non_policy_reshape_pow_exp=0.5,\ntarget_probs=None,\nloss_remove_token_mean=False,\nloss_remove_clip=False,\n):", "input_parameters": "- `old_log_prob`: Old policy log probabilities with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `log_prob`: Current policy log probabilities with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `advantages`: Advantage estimates with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `eos_mask`: Valid token mask with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `cliprange`: PPO clip range, default 0.2\n- `clip_upper_bound`: PPO clip upper bound, default 100.0\n- `prefix_mask`: Mask marking off-policy tokens with shape `[batch_size, response_length]`, **type must be `torch.Tensor` with dtype `torch.bool`, as the code uses bitwise NOT operator `~prefix_mask` to obtain on-policy token mask**\n- `off_cliprange`: off-policy clip range (not used in current implementation)\n- `off_normalize`: Whether to normalize off-policy ratio, default False\n- `off_abs_cliprange`: off-policy absolute value clip range (not used in current implementation)\n- `off_max_clip`: off-policy ratio upper limit, None means no limit\n- `off_min_clip`: off-policy ratio lower limit, None means no limit\n- `all_max_clip`: Global probability upper limit, None means no limit\n- `off_policy_reshape`: off-policy policy shaping method name, default 'no_reshape'\n- `off_policy_reshape_weight`: Policy shaping weight parameter, default 1.0\n- `off_policy_reshape_pow_exp`: Power transformation exponent, default 0.5\n- `on_policy_reshape`: on-policy policy shaping method name, default 'no_reshape'\n- `on_policy_reshape_weight`: on-policy policy shaping weight, default 1.0\n- `on_policy_reshape_pow_exp`: on-policy power transformation exponent, default 0.5\n- `target_probs`: Optional target probabilities (for explicit behavior policy), default None\n- `loss_remove_token_mean`: Whether to remove token-level averaging, default False\n- `loss_remove_clip`: Whether to remove clip operation, default False", "detailed_description": "This function computes mixed policy gradient loss, with the core idea of simultaneously handling both on-policy and off-policy data. For on-policy data, it uses standard PPO algorithm to compute importance sampling ratio and perform clipping; for off-policy data, it directly uses current policy probabilities (or ratio relative to target probabilities) to compute loss, without relying on old policy.\nThe function distinguishes which tokens are off-policy and which are on-policy through prefix_mask, then combines the two losses with weighting. Additionally, the function should support multiple probability reshaping strategies and clipping mechanisms.", "output": "Returns dictionary containing the following key-value pairs:\n- `pg_loss`: Total policy gradient loss (scalar)\n- `off_pg_loss`: off-policy loss component (scalar)\n- `on_pg_loss`: on-policy loss component (scalar)\n- `off_pg_clipfrac`: off-policy clip fraction (scalar, currently 0 in implementation)\n- `on_pg_clipfrac`: on-policy clip fraction (scalar)\n- `ppo_kl`: PPO KL divergence (scalar)\n- `off_policy_prob`: Average probability of off-policy samples (scalar)\n- `on_policy_prob`: Average probability of on-policy samples (scalar)\n- `off_ratio_mean`: off-policy average ratio (scalar)\n- `off_ratio_max_clip_frac`: Fraction reaching upper clip limit (scalar)\n- `off_ratio_min_clip_frac`: Fraction reaching lower clip limit (scalar)", "func_implementation": "", "test_code_path": "code/test_code/test_compute_token_on_off_policy_loss.py", "implementation_location": "code/luffy/verl/verl/mix_src/mix_core_alg.py:line 69-251", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_token_on_off_policy_loss`.\n\n**Function Overview:**\nComputes mixed policy loss, combining on-policy PPO clip loss and off-policy importance sampling loss, supporting multiple policy shaping methods to emphasize low-probability but critical actions.\n\n**Function Signature:**\n```python\ndef compute_token_on_off_policy_loss(\nold_log_prob,\nlog_prob,\nadvantages,\neos_mask,\ncliprange,\nclip_upper_bound,\nprefix_mask,\noff_cliprange,\noff_normalize=False,\noff_abs_cliprange=None,\noff_max_clip=None,\noff_min_clip=None,\nall_max_clip=None,\noff_policy_reshape=\"no_reshape\",\noff_policy_reshape_weight=1.0,\noff_policy_reshape_pow_exp=0.5,\non_policy_reshape=\"no_reshape\",\non_policy_reshape_weight=1.0,\non_policy_reshape_pow_exp=0.5,\ntarget_probs=None,\nloss_remove_token_mean=False,\nloss_remove_clip=False,\n):\n```\n\n**Input Parameters:**\n- `old_log_prob`: Old policy log probabilities with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `log_prob`: Current policy log probabilities with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `advantages`: Advantage estimates with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `eos_mask`: Valid token mask with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `cliprange`: PPO clip range, default 0.2\n- `clip_upper_bound`: PPO clip upper bound, default 100.0\n- `prefix_mask`: Mask marking off-policy tokens with shape `[batch_size, response_length]`, **type must be `torch.Tensor` with dtype `torch.bool`, as the code uses bitwise NOT operator `~prefix_mask` to obtain on-policy token mask**\n- `off_cliprange`: off-policy clip range (not used in current implementation)\n- `off_normalize`: Whether to normalize off-policy ratio, default False\n- `off_abs_cliprange`: off-policy absolute value clip range (not used in current implementation)\n- `off_max_clip`: off-policy ratio upper limit, None means no limit\n- `off_min_clip`: off-policy ratio lower limit, None means no limit\n- `all_max_clip`: Global probability upper limit, None means no limit\n- `off_policy_reshape`: off-policy policy shaping method name, default 'no_reshape'\n- `off_policy_reshape_weight`: Policy shaping weight parameter, default 1.0\n- `off_policy_reshape_pow_exp`: Power transformation exponent, default 0.5\n- `on_policy_reshape`: on-policy policy shaping method name, default 'no_reshape'\n- `on_policy_reshape_weight`: on-policy policy shaping weight, default 1.0\n- `on_policy_reshape_pow_exp`: on-policy power transformation exponent, default 0.5\n- `target_probs`: Optional target probabilities (for explicit behavior policy), default None\n- `loss_remove_token_mean`: Whether to remove token-level averaging, default False\n- `loss_remove_clip`: Whether to remove clip operation, default False\n\n**Detailed Description:**\nThis function computes mixed policy gradient loss, with the core idea of simultaneously handling both on-policy and off-policy data. For on-policy data, it uses standard PPO algorithm to compute importance sampling ratio and perform clipping; for off-policy data, it directly uses current policy probabilities (or ratio relative to target probabilities) to compute loss, without relying on old policy.\nThe function distinguishes which tokens are off-policy and which are on-policy through prefix_mask, then combines the two losses with weighting. Additionally, the function should support multiple probability reshaping strategies and clipping mechanisms.\n\n**Expected Output:**\nReturns dictionary containing the following key-value pairs:\n- `pg_loss`: Total policy gradient loss (scalar)\n- `off_pg_loss`: off-policy loss component (scalar)\n- `on_pg_loss`: on-policy loss component (scalar)\n- `off_pg_clipfrac`: off-policy clip fraction (scalar, currently 0 in implementation)\n- `on_pg_clipfrac`: on-policy clip fraction (scalar)\n- `ppo_kl`: PPO KL divergence (scalar)\n- `off_policy_prob`: Average probability of off-policy samples (scalar)\n- `on_policy_prob`: Average probability of on-policy samples (scalar)\n- `off_ratio_mean`: off-policy average ratio (scalar)\n- `off_ratio_max_clip_frac`: Fraction reaching upper clip limit (scalar)\n- `off_ratio_min_clip_frac`: Fraction reaching lower clip limit (scalar)\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport torch\nimport torch.nn.functional as F\n\ndef compute_token_on_off_policy_loss(\n    old_log_prob,\n    log_prob,\n    advantages,\n    eos_mask,\n    cliprange,\n    clip_upper_bound,\n    prefix_mask,\n    off_cliprange,\n    off_normalize=False,\n    off_abs_cliprange=None,\n    off_max_clip=None,\n    off_min_clip=None,\n    all_max_clip=None,\n    off_policy_reshape=\"no_reshape\",\n    off_policy_reshape_weight=1.0,\n    off_policy_reshape_pow_exp=0.5,\n    on_policy_reshape=\"no_reshape\",\n    on_policy_reshape_weight=1.0,\n    on_policy_reshape_pow_exp=0.5,\n    target_probs=None,\n    loss_remove_token_mean=False,\n    loss_remove_clip=False,\n):\n    \"\"\"\n    Compute mixed policy loss combining on-policy PPO clip loss and off-policy importance sampling loss.\n    \n    Args:\n        old_log_prob: Old policy log probabilities [batch_size, response_length]\n        log_prob: Current policy log probabilities [batch_size, response_length]\n        advantages: Advantage estimates [batch_size, response_length]\n        eos_mask: Valid token mask [batch_size, response_length]\n        cliprange: PPO clip range\n        clip_upper_bound: PPO clip upper bound\n        prefix_mask: Off-policy token mask [batch_size, response_length], dtype=torch.bool\n        off_cliprange: Off-policy clip range (unused)\n        off_normalize: Whether to normalize off-policy ratio\n        off_abs_cliprange: Off-policy absolute clip range (unused)\n        off_max_clip: Off-policy ratio upper limit\n        off_min_clip: Off-policy ratio lower limit\n        all_max_clip: Global probability upper limit\n        off_policy_reshape: Off-policy policy shaping method\n        off_policy_reshape_weight: Off-policy shaping weight\n        off_policy_reshape_pow_exp: Off-policy power exponent\n        on_policy_reshape: On-policy policy shaping method\n        on_policy_reshape_weight: On-policy shaping weight\n        on_policy_reshape_pow_exp: On-policy power exponent\n        target_probs: Target probabilities for explicit behavior policy\n        loss_remove_token_mean: Whether to remove token-level averaging\n        loss_remove_clip: Whether to remove clip operation\n        \n    Returns:\n        Dictionary containing loss components and statistics\n    \"\"\"\n    # Ensure prefix_mask is boolean for bitwise operations\n    if not prefix_mask.dtype == torch.bool:\n        raise ValueError(\"prefix_mask must have dtype torch.bool\")\n    \n    # Create masks for on-policy and off-policy tokens\n    on_mask = ~prefix_mask  # On-policy tokens\n    off_mask = prefix_mask  # Off-policy tokens\n    \n    # Apply valid token mask\n    on_valid_mask = on_mask & eos_mask\n    off_valid_mask = off_mask & eos_mask\n    \n    # Initialize statistics\n    batch_size = old_log_prob.shape[0]\n    device = old_log_prob.device\n    \n    # ==================== ON-POLICY LOSS COMPUTATION ====================\n    if on_valid_mask.any():\n        # Extract on-policy data\n        on_old_log_prob = old_log_prob[on_valid_mask]\n        on_log_prob = log_prob[on_valid_mask]\n        on_advantages = advantages[on_valid_mask]\n        \n        # Compute probability ratio\n        ratio = torch.exp(on_log_prob - on_old_log_prob)\n        \n        # Apply on-policy policy shaping\n        if on_policy_reshape != \"no_reshape\":\n            ratio = apply_policy_reshaping(\n                ratio, on_policy_reshape, on_policy_reshape_weight, on_policy_reshape_pow_exp\n            )\n        \n        # Apply global probability clipping if specified\n        if all_max_clip is not None:\n            ratio = torch.clamp(ratio, max=all_max_clip)\n        \n        # Compute PPO clipped loss\n        if loss_remove_clip:\n            # Without clipping\n            pg_losses1 = -on_advantages * ratio\n            pg_losses2 = -on_advantages * ratio\n        else:\n            # With clipping\n            clipped_ratio = torch.clamp(\n                ratio, 1.0 - cliprange, 1.0 + cliprange\n            )\n            if clip_upper_bound is not None:\n                clipped_ratio = torch.clamp(clipped_ratio, max=clip_upper_bound)\n            \n            pg_losses1 = -on_advantages * ratio\n            pg_losses2 = -on_advantages * clipped_ratio\n        \n        # Take minimum of clipped and unclipped loss\n        on_pg_loss = torch.max(pg_losses1, pg_losses2)\n        \n        # Compute on-policy clip fraction\n        if loss_remove_clip:\n            on_pg_clipfrac = torch.tensor(0.0, device=device)\n        else:\n            clipped = (ratio < (1.0 - cliprange)) | (ratio > (1.0 + cliprange))\n            on_pg_clipfrac = clipped.float().mean()\n        \n        # Compute KL divergence\n        ppo_kl = (on_old_log_prob - on_log_prob).mean()\n        \n        # Compute on-policy probability\n        on_policy_prob = torch.exp(on_log_prob).mean()\n        \n        # Apply token-level averaging if not removed\n        if not loss_remove_token_mean:\n            on_pg_loss = on_pg_loss.mean()\n        else:\n            on_pg_loss = on_pg_loss.sum() / batch_size\n    else:\n        # No on-policy tokens\n        on_pg_loss = torch.tensor(0.0, device=device)\n        on_pg_clipfrac = torch.tensor(0.0, device=device)\n        ppo_kl = torch.tensor(0.0, device=device)\n        on_policy_prob = torch.tensor(0.0, device=device)\n    \n    # ==================== OFF-POLICY LOSS COMPUTATION ====================\n    if off_valid_mask.any():\n        # Extract off-policy data\n        off_log_prob = log_prob[off_valid_mask]\n        off_advantages = advantages[off_valid_mask]\n        \n        # Compute off-policy ratio\n        if target_probs is not None:\n            # Use explicit behavior policy\n            off_target_log_prob = target_probs[off_valid_mask]\n            off_ratio = torch.exp(off_log_prob - off_target_log_prob)\n        else:\n            # Use current policy directly (no old policy reference)\n            off_ratio = torch.exp(off_log_prob)\n        \n        # Apply off-policy policy shaping\n        if off_policy_reshape != \"no_reshape\":\n            off_ratio = apply_policy_reshaping(\n                off_ratio, off_policy_reshape, off_policy_reshape_weight, off_policy_reshape_pow_exp\n            )\n        \n        # Normalize off-policy ratio if requested\n        if off_normalize:\n            off_ratio = off_ratio / (off_ratio.mean().detach() + 1e-8)\n        \n        # Apply clipping bounds\n        if off_max_clip is not None:\n            off_ratio = torch.clamp(off_ratio, max=off_max_clip)\n        if off_min_clip is not None:\n            off_ratio = torch.clamp(off_ratio, min=off_min_clip)\n        if all_max_clip is not None:\n            off_ratio = torch.clamp(off_ratio, max=all_max_clip)\n        \n        # Compute off-policy loss\n        off_pg_loss = -off_advantages * off_ratio\n        \n        # Compute off-policy statistics\n        off_policy_prob = torch.exp(off_log_prob).mean()\n        off_ratio_mean = off_ratio.mean()\n        \n        # Compute clip fractions\n        if off_max_clip is not None:\n            off_ratio_max_clip_frac = (off_ratio >= off_max_clip).float().mean()\n        else:\n            off_ratio_max_clip_frac = torch.tensor(0.0, device=device)\n        \n        if off_min_clip is not None:\n            off_ratio_min_clip_frac = (off_ratio <= off_min_clip).float().mean()\n        else:\n            off_ratio_min_clip_frac = torch.tensor(0.0, device=device)\n        \n        # Apply token-level averaging if not removed\n        if not loss_remove_token_mean:\n            off_pg_loss = off_pg_loss.mean()\n        else:\n            off_pg_loss = off_pg_loss.sum() / batch_size\n    else:\n        # No off-policy tokens\n        off_pg_loss = torch.tensor(0.0, device=device)\n        off_policy_prob = torch.tensor(0.0, device=device)\n        off_ratio_mean = torch.tensor(0.0, device=device)\n        off_ratio_max_clip_frac = torch.tensor(0.0, device=device)\n        off_ratio_min_clip_frac = torch.tensor(0.0, device=device)\n    \n    # Off-policy clip fraction is currently 0 as per specification\n    off_pg_clipfrac = torch.tensor(0.0, device=device)\n    \n    # ==================== COMBINE LOSSES ====================\n    # Combine on-policy and off-policy losses\n    pg_loss = on_pg_loss + off_pg_loss\n    \n    # Return all statistics\n    return {\n        \"pg_loss\": pg_loss,\n        \"off_pg_loss\": off_pg_loss,\n        \"on_pg_loss\": on_pg_loss,\n        \"off_pg_clipfrac\": off_pg_clipfrac,\n        \"on_pg_clipfrac\": on_pg_clipfrac,\n        \"ppo_kl\": ppo_kl,\n        \"off_policy_prob\": off_policy_prob,\n        \"on_policy_prob\": on_policy_prob,\n        \"off_ratio_mean\": off_ratio_mean,\n        \"off_ratio_max_clip_frac\": off_ratio_max_clip_frac,\n        \"off_ratio_min_clip_frac\": off_ratio_min_clip_frac,\n    }\n\n\ndef apply_policy_reshaping(ratio, reshape_method, weight, pow_exp):\n    \"\"\"\n    Apply policy shaping transformation to emphasize low-probability actions.\n    \n    Args:\n        ratio: Probability ratio tensor\n        reshape_method: Shaping method name\n        weight: Shaping weight parameter\n        pow_exp: Power transformation exponent\n        \n    Returns:\n        Reshaped ratio tensor\n    \"\"\"\n    if reshape_method == \"no_reshape\":\n        return ratio\n    elif reshape_method == \"pow\":\n        # Power transformation: ratio^pow_exp\n        return torch.pow(ratio, pow_exp)\n    elif reshape_method == \"exp\":\n        # Exponential transformation: exp(weight * ratio)\n        return torch.exp(weight * ratio)\n    elif reshape_method == \"log\":\n        # Logarithmic transformation: log(1 + weight * ratio)\n        return torch.log1p(weight * ratio)\n    elif reshape_method == \"sigmoid\":\n        # Sigmoid transformation: sigmoid(weight * ratio)\n        return torch.sigmoid(weight * ratio)\n    elif reshape_method == \"softplus\":\n        # Softplus transformation: softplus(weight * ratio)\n        return F.softplus(weight * ratio)\n    else:\n        raise ValueError(f\"Unknown policy reshaping method: {reshape_method}\")\n```"], "usage": {"prompt_tokens": 1239, "completion_tokens": 7402, "total_tokens": 8641, "per_completion": [{"prompt_tokens": 1239, "completion_tokens": 7402, "total_tokens": 8641}]}, "results": [false], "pass_ratios": [0.0]}
{"function_name": "compute_sft_pure_loss", "overview": "Computes pure SFT (Supervised Fine-Tuning) loss, i.e., negative log likelihood loss, used for supervised learning on off-policy samples in multi-task learning.", "function_signature": "", "input_parameters": "- `log_prob`: Model-predicted log probabilities with shape `[batch_size, response_length]`\n- `eos_mask`: Valid token mask with shape `[batch_size, response_length]`", "detailed_description": "Computes negative log probability for each token as loss value. Uses eos_mask to mask the loss, computing loss only at valid token positions. Averages the masked loss to obtain batch-level SFT loss.", "output": "- `sft_loss`: Scalar representing average SFT loss", "func_implementation": "", "test_code_path": "code/test_code/test_compute_sft_pure_loss.py", "implementation_location": "code/luffy/verl/verl/mix_src/mix_core_alg.py:line 7-10", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_sft_pure_loss`.\n\n**Function Overview:**\nComputes pure SFT (Supervised Fine-Tuning) loss, i.e., negative log likelihood loss, used for supervised learning on off-policy samples in multi-task learning.\n\n**Function Signature:**\n```python\n\n```\n\n**Input Parameters:**\n- `log_prob`: Model-predicted log probabilities with shape `[batch_size, response_length]`\n- `eos_mask`: Valid token mask with shape `[batch_size, response_length]`\n\n**Detailed Description:**\nComputes negative log probability for each token as loss value. Uses eos_mask to mask the loss, computing loss only at valid token positions. Averages the masked loss to obtain batch-level SFT loss.\n\n**Expected Output:**\n- `sft_loss`: Scalar representing average SFT loss\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\ndef compute_sft_pure_loss(log_prob, eos_mask):\n    \"\"\"\n    Computes pure SFT (Supervised Fine-Tuning) loss, i.e., negative log likelihood loss.\n    \n    Args:\n        log_prob: Model-predicted log probabilities with shape [batch_size, response_length]\n        eos_mask: Valid token mask with shape [batch_size, response_length]\n    \n    Returns:\n        sft_loss: Scalar representing average SFT loss\n    \"\"\"\n    # Compute negative log probability for each token\n    loss = -log_prob\n    \n    # Mask the loss using eos_mask (only compute loss at valid token positions)\n    masked_loss = loss * eos_mask\n    \n    # Sum the masked loss and divide by total number of valid tokens\n    total_valid_tokens = eos_mask.sum()\n    if total_valid_tokens > 0:\n        sft_loss = masked_loss.sum() / total_valid_tokens\n    else:\n        # Avoid division by zero if no valid tokens\n        sft_loss = masked_loss.sum() * 0.0\n    \n    return sft_loss\n```"], "usage": {"prompt_tokens": 357, "completion_tokens": 2275, "total_tokens": 2632, "per_completion": [{"prompt_tokens": 357, "completion_tokens": 2275, "total_tokens": 2632}]}, "results": [false], "pass_ratios": [0.0]}
