{"function_name": "DataParallelPRIMERewardModel._forward_micro_batch", "overview": "Computes token-level reward scores for implicit process reward model", "function_signature": "def _forward_micro_batch(self, micro_batch, prompt_length):", "input_parameters": "- `micro_batch`: Micro-batch data {input_ids, attention_mask, position_ids, acc}\n- `prompt_length`: Prompt sequence length", "detailed_description": "First selects computation path based on whether padding removal optimization is enabled. For reward model and reference model, performs forward pass to compute output logits, then extracts log probabilities for corresponding tokens.\nImplicit rewards are obtained by computing log probability differences between the two models on response sequences.\nProcess reward generation supports two modes: direct mode and GAE mode.\nIn token granularity, each position receives corresponding reward value; in whole granularity, all rewards accumulate to the last valid token position.", "output": "- `token_level_scores`: Token-level reward scores (torch.Tensor, shape: [batch_size, seq_len])\n- `q_values`: Raw log probability differences (torch.Tensor, shape: [batch_size, seq_len])", "func_implementation": "", "test_code_path": "code/test_code/test_forward_micro_batch.py", "implementation_location": "code/recipe/prime/prime_dp_rm.py:line 51-228", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `DataParallelPRIMERewardModel._forward_micro_batch`.\n\n**Function Overview:**\nComputes token-level reward scores for implicit process reward model\n\n**Function Signature:**\n```python\ndef _forward_micro_batch(self, micro_batch, prompt_length):\n```\n\n**Input Parameters:**\n- `micro_batch`: Micro-batch data {input_ids, attention_mask, position_ids, acc}\n- `prompt_length`: Prompt sequence length\n\n**Detailed Description:**\nFirst selects computation path based on whether padding removal optimization is enabled. For reward model and reference model, performs forward pass to compute output logits, then extracts log probabilities for corresponding tokens.\nImplicit rewards are obtained by computing log probability differences between the two models on response sequences.\nProcess reward generation supports two modes: direct mode and GAE mode.\nIn token granularity, each position receives corresponding reward value; in whole granularity, all rewards accumulate to the last valid token position.\n\n**Expected Output:**\n- `token_level_scores`: Token-level reward scores (torch.Tensor, shape: [batch_size, seq_len])\n- `q_values`: Raw log probability differences (torch.Tensor, shape: [batch_size, seq_len])\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\ndef _forward_micro_batch(self, micro_batch, prompt_length):\n    \"\"\"\n    Computes token-level reward scores for implicit process reward model.\n    \n    Args:\n        micro_batch: Micro-batch data containing:\n            - input_ids: Token IDs [batch_size, seq_len]\n            - attention_mask: Attention mask [batch_size, seq_len]\n            - position_ids: Position IDs [batch_size, seq_len]\n            - acc: Additional context information\n        prompt_length: Prompt sequence length\n    \n    Returns:\n        token_level_scores: Token-level reward scores [batch_size, seq_len]\n        q_values: Raw log probability differences [batch_size, seq_len]\n    \"\"\"\n    # Extract input data\n    input_ids = micro_batch[\"input_ids\"]\n    attention_mask = micro_batch[\"attention_mask\"]\n    position_ids = micro_batch[\"position_ids\"]\n    acc = micro_batch[\"acc\"]\n    \n    batch_size, seq_len = input_ids.shape\n    \n    # Determine computation path based on padding removal optimization\n    if self.remove_padding:\n        # Remove padding tokens for efficiency\n        # Flatten batch and sequence dimensions, keep only non-padding tokens\n        non_padding_mask = attention_mask.view(-1) == 1\n        flat_input_ids = input_ids.view(-1)[non_padding_mask]\n        flat_position_ids = position_ids.view(-1)[non_padding_mask]\n        \n        # Forward pass through reward model\n        reward_logits = self.reward_model(\n            input_ids=flat_input_ids.unsqueeze(0),\n            position_ids=flat_position_ids.unsqueeze(0),\n            attention_mask=None  # Not needed when padding removed\n        )\n        \n        # Forward pass through reference model\n        reference_logits = self.reference_model(\n            input_ids=flat_input_ids.unsqueeze(0),\n            position_ids=flat_position_ids.unsqueeze(0),\n            attention_mask=None\n        )\n        \n        # Extract log probabilities for corresponding tokens\n        # Shift input_ids to align with logits (predict next token)\n        target_ids = flat_input_ids[1:] if seq_len > 1 else flat_input_ids\n        \n        reward_log_probs = torch.log_softmax(reward_logits[:, :-1], dim=-1)\n        reference_log_probs = torch.log_softmax(reference_logits[:, :-1], dim=-1)\n        \n        # Gather log probabilities for target tokens\n        reward_token_log_probs = reward_log_probs.gather(\n            dim=-1, \n            index=target_ids.unsqueeze(0).unsqueeze(-1)\n        ).squeeze(-1).squeeze(0)\n        \n        reference_token_log_probs = reference_log_probs.gather(\n            dim=-1,\n            index=target_ids.unsqueeze(0).unsqueeze(-1)\n        ).squeeze(-1).squeeze(0)\n        \n        # Compute log probability differences\n        log_prob_diff = reward_token_log_probs - reference_token_log_probs\n        \n        # Reconstruct full sequence with padding\n        q_values_flat = torch.zeros(batch_size * seq_len, device=input_ids.device)\n        # Place computed differences at appropriate positions\n        # We need to handle the offset due to shifting for next token prediction\n        valid_positions = non_padding_mask.nonzero(as_tuple=True)[0]\n        # Only assign to positions where we have computed differences\n        q_values_flat[valid_positions[1:]] = log_prob_diff\n        \n        # Reshape back to original dimensions\n        q_values = q_values_flat.view(batch_size, seq_len)\n    else:\n        # Standard forward pass without padding removal\n        # Forward pass through reward model\n        reward_logits = self.reward_model(\n            input_ids=input_ids,\n            position_ids=position_ids,\n            attention_mask=attention_mask\n        )\n        \n        # Forward pass through reference model\n        reference_logits = self.reference_model(\n            input_ids=input_ids,\n            position_ids=position_ids,\n            attention_mask=attention_mask\n        )\n        \n        # Extract log probabilities for corresponding tokens\n        # Shift input_ids to align with logits (predict next token)\n        target_ids = input_ids[:, 1:] if seq_len > 1 else input_ids\n        \n        reward_log_probs = torch.log_softmax(reward_logits[:, :-1], dim=-1)\n        reference_log_probs = torch.log_softmax(reference_logits[:, :-1], dim=-1)\n        \n        # Gather log probabilities for target tokens\n        reward_token_log_probs = reward_log_probs.gather(\n            dim=-1,\n            index=target_ids.unsqueeze(-1)\n        ).squeeze(-1)\n        \n        reference_token_log_probs = reference_log_probs.gather(\n            dim=-1,\n            index=target_ids.unsqueeze(-1)\n        ).squeeze(-1)\n        \n        # Compute log probability differences\n        log_prob_diff = reward_token_log_probs - reference_token_log_probs\n        \n        # Pad with zeros at the beginning to maintain sequence length\n        q_values = torch.zeros(batch_size, seq_len, device=input_ids.device)\n        q_values[:, 1:] = log_prob_diff\n    \n    # Generate implicit rewards from log probability differences\n    # Process reward generation supports two modes: direct mode and GAE mode\n    if self.process_reward_mode == \"direct\":\n        # Direct mode: use log probability differences directly as rewards\n        token_level_scores = q_values\n    elif self.process_reward_mode == \"gae\":\n        # GAE mode: apply Generalized Advantage Estimation\n        # For simplicity, we'll compute a simple advantage estimate\n        # In practice, this would involve more complex temporal difference calculations\n        with torch.no_grad():\n            # Compute value estimates (could be from a separate value head)\n            # Here we use the log prob differences as a proxy\n            values = q_values.cumsum(dim=-1)\n            \n            # Compute advantages using GAE\n            advantages = torch.zeros_like(q_values)\n            gamma = self.gae_gamma\n            lam = self.gae_lambda\n            \n            # Reverse computation for GAE\n            next_value = 0\n            for t in reversed(range(seq_len)):\n                delta = q_values[:, t] + gamma * next_value - values[:, t]\n                advantages[:, t] = delta + gamma * lam * next_value\n                next_value = values[:, t]\n            \n            token_level_scores = advantages\n    else:\n        raise ValueError(f\"Unknown process reward mode: {self.process_reward_mode}\")\n    \n    # Handle reward granularity\n    if self.reward_granularity == \"whole\":\n        # Whole granularity: accumulate all rewards to the last valid token position\n        # Create mask for response tokens (after prompt)\n        response_mask = torch.zeros_like(attention_mask)\n        response_mask[:, prompt_length:] = attention_mask[:, prompt_length:]\n        \n        # Find the last valid token position in each sequence\n        last_token_positions = (response_mask.cumsum(dim=-1).argmax(dim=-1))\n        \n        # Create mask for last token positions\n        last_token_mask = torch.zeros_like(token_level_scores)\n        for i in range(batch_size):\n            if last_token_positions[i] > 0:\n                last_token_mask[i, last_token_positions[i]] = 1\n        \n        # Accumulate rewards to last token positions\n        # Sum all response rewards for each sequence\n        response_rewards = token_level_scores * response_mask\n        total_rewards = response_rewards.sum(dim=-1, keepdim=True)\n        \n        # Place total rewards at last token positions\n        token_level_scores = torch.zeros_like(token_level_scores)\n        token_level_scores = token_level_scores + last_token_mask * total_rewards\n    \n    # Ensure token-level scores have the correct shape\n    assert token_level_scores.shape == (batch_size, seq_len), \\\n        f\"Token level scores shape mismatch: {token_level_scores.shape} != {(batch_size, seq_len)}\"\n    assert q_values.shape == (batch_size, seq_len), \\\n        f\"Q values shape mismatch: {q_values.shape} != {(batch_size, seq_len)}\"\n    \n    return token_level_scores, q_values\n```"], "usage": {"prompt_tokens": 437, "completion_tokens": 1767, "total_tokens": 2204, "per_completion": [{"prompt_tokens": 437, "completion_tokens": 1767, "total_tokens": 2204}]}}
{"function_name": "compute_ce_dpo_loss_rm", "overview": "Computes cross-entropy style DPO loss", "function_signature": "def compute_ce_dpo_loss_rm(token_level_scores, acc, response_mask, beta):", "input_parameters": "- `token_level_scores`: Token-level rewards\n- `acc`: Binary accuracy labels\n- `response_mask`: Response mask\n- `beta`: Temperature parameter", "detailed_description": "Computes sequence-level scores, converts them to probabilities, and finally computes cross-entropy loss.", "output": "Scalar loss value", "func_implementation": "", "test_code_path": "code/test_code/test_ce_dpo_loss.py", "implementation_location": "code/recipe/prime/prime_core_algos.py:line 82-85", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_ce_dpo_loss_rm`.\n\n**Function Overview:**\nComputes cross-entropy style DPO loss\n\n**Function Signature:**\n```python\ndef compute_ce_dpo_loss_rm(token_level_scores, acc, response_mask, beta):\n```\n\n**Input Parameters:**\n- `token_level_scores`: Token-level rewards\n- `acc`: Binary accuracy labels\n- `response_mask`: Response mask\n- `beta`: Temperature parameter\n\n**Detailed Description:**\nComputes sequence-level scores, converts them to probabilities, and finally computes cross-entropy loss.\n\n**Expected Output:**\nScalar loss value\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport torch\nimport torch.nn.functional as F\n\n\ndef compute_ce_dpo_loss_rm(token_level_scores, acc, response_mask, beta):\n    \"\"\"\n    Computes cross-entropy style DPO loss.\n\n    Args:\n        token_level_scores: Token-level rewards of shape (batch_size, seq_len)\n        acc: Binary accuracy labels of shape (batch_size,)\n        response_mask: Response mask of shape (batch_size, seq_len)\n        beta: Temperature parameter (scalar)\n\n    Returns:\n        Scalar loss value\n    \"\"\"\n    # Step 1: Compute sequence-level scores by averaging token-level rewards\n    # Only consider tokens where response_mask is True\n    # We use masked mean to handle variable"], "usage": {"prompt_tokens": 314, "completion_tokens": 153, "total_tokens": 467, "per_completion": [{"prompt_tokens": 314, "completion_tokens": 153, "total_tokens": 467}]}}
{"function_name": "compute_detach_dpo_loss_rm", "overview": "Computes detached DPO loss, supporting BoN (Best-of-N) mode", "function_signature": "def compute_detach_dpo_loss_rm(token_level_scores, acc, Q_bc, acc_bc, response_mask, beta, bon_mode=\"none\"):", "input_parameters": "- Base: `token_level_scores`, `acc`, `response_mask`, `beta`\n- Contrast: `Q_bc` (batch Q-value matrix), `acc_bc` (batch accuracy matrix)\n- `bon_mode`: Weight mode {\"none\", \"bon_rm\", \"bon_acc\"}", "detailed_description": "Computes detached DPO loss based on contrastive samples, selecting samples with opposite accuracy labels within batch as contrastive baseline, and supports assigning higher training weights to high-quality samples through BoN weight mechanism.\nBoN weights include three modes: none mode, bon_rm mode, and bon_acc mode.", "output": "Weighted DPO loss value", "func_implementation": "", "test_code_path": "code/test_code/test_detach_dpo_loss.py", "implementation_location": "code/recipe/prime/prime_core_algos.py:line 88-116", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_detach_dpo_loss_rm`.\n\n**Function Overview:**\nComputes detached DPO loss, supporting BoN (Best-of-N) mode\n\n**Function Signature:**\n```python\ndef compute_detach_dpo_loss_rm(token_level_scores, acc, Q_bc, acc_bc, response_mask, beta, bon_mode=\"none\"):\n```\n\n**Input Parameters:**\n- Base: `token_level_scores`, `acc`, `response_mask`, `beta`\n- Contrast: `Q_bc` (batch Q-value matrix), `acc_bc` (batch accuracy matrix)\n- `bon_mode`: Weight mode {\"none\", \"bon_rm\", \"bon_acc\"}\n\n**Detailed Description:**\nComputes detached DPO loss based on contrastive samples, selecting samples with opposite accuracy labels within batch as contrastive baseline, and supports assigning higher training weights to high-quality samples through BoN weight mechanism.\nBoN weights include three modes: none mode, bon_rm mode, and bon_acc mode.\n\n**Expected Output:**\nWeighted DPO loss value\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport torch\nimport torch.nn.functional as F\n\ndef compute_detach_dpo_loss_rm(token_level_scores, acc, Q_bc, acc_bc, response_mask, beta, bon_mode=\"none\"):\n    \"\"\"\n    Computes detached DPO loss, supporting BoN (Best-of-N) mode.\n    \n    Args:\n        token_level_scores: Base token-level scores (batch_size, seq_len)\n        acc: Base accuracy labels (batch_size,)\n        Q_bc: Batch Q-value matrix for contrastive samples (batch_size, num_contrast)\n        acc_bc: Batch accuracy matrix for contrastive samples (batch_size, num_contrast)\n        response_mask: Mask indicating response tokens (batch_size, seq_len)\n        beta: Temperature parameter for DPO loss\n        bon_mode: Weight mode {\"none\", \"bon_rm\", \"bon_acc\"}\n    \n    Returns:\n        Weighted DPO loss value (scalar tensor)\n    \"\"\"\n    batch_size = token_level_scores.size(0)\n    num_contrast = Q_bc.size(1)\n    \n    # 1. Compute base response-level scores\n    # Mask out non-response tokens and sum over sequence dimension\n    masked_scores = token_level_scores * response_mask\n    base_scores = masked_scores.sum(dim=1)  # (batch_size,)\n    \n    # 2. Find contrastive samples with opposite accuracy labels\n    # Expand base accuracy for comparison\n    acc_expanded = acc.unsqueeze(1).expand(-1, num_contrast)  # (batch_size, num_contrast)\n    \n    # Find indices where accuracy labels are opposite\n    opposite_mask = (acc_expanded != acc_bc).float()  # (batch_size, num_contrast)\n    \n    # If no opposite samples exist for a batch item, use all samples as fallback\n    opposite_mask_sum = opposite_mask.sum(dim=1, keepdim=True)\n    contrast_mask = torch.where(opposite_mask_sum > 0, opposite_mask, \n                                torch.ones_like(opposite_mask))\n    \n    # 3. Compute contrastive baseline\n    # Apply mask to Q-values\n    masked_Q_bc = Q_bc * contrast_mask\n    \n    # For each batch item, compute average of valid contrastive Q-values\n    # Add small epsilon to avoid division by zero\n    valid_count = contrast_mask.sum(dim=1, keepdim=True) + 1e-8\n    contrast_baseline = masked_Q_bc.sum(dim=1) / valid_count.squeeze(1)  # (batch_size,)\n    \n    # 4. Compute DPO loss components\n    # Detach the contrastive baseline to stop gradient flow\n    contrast_baseline_detached = contrast_baseline.detach()\n    \n    # Compute advantage: base_score - contrast_baseline\n    advantage = base_scores - contrast_baseline_detached\n    \n    # Compute log probability ratios\n    log_prob_ratio = advantage / beta\n    \n    # Compute DPO loss using sigmoid cross-entropy\n    # For DPO, we want to maximize the probability that preferred samples have higher scores\n    # We use accuracy labels as preference indicators (1 for preferred, 0 for dispreferred)\n    dpo_loss = F.binary_cross_entropy_with_logits(\n        log_prob_ratio,\n        acc.float(),\n        reduction='none'\n    )  # (batch_size,)\n    \n    # 5. Compute BoN weights\n    if bon_mode == \"none\":\n        weights = torch.ones_like(dpo_loss)\n    elif bon_mode == \"bon_rm\":\n        # Weight by response mask sum (longer responses get higher weight)\n        weights = response_mask.sum(dim=1).float()\n    elif bon_mode == \"bon_acc\":\n        # Weight by accuracy (higher accuracy samples get higher weight)\n        weights = acc.float()\n    else:\n        raise ValueError(f\"Unknown bon_mode: {bon_mode}. Must be one of ['none', 'bon_rm', 'bon_acc']\")\n    \n    # Normalize weights to have mean 1.0\n    weights = weights / (weights.mean() + 1e-8)\n    \n    # 6. Compute weighted loss\n    weighted_loss = (dpo_loss * weights).mean()\n    \n    return weighted_loss\n```"], "usage": {"prompt_tokens": 409, "completion_tokens": 936, "total_tokens": 1345, "per_completion": [{"prompt_tokens": 409, "completion_tokens": 936, "total_tokens": 1345}]}}
{"function_name": "compute_dpo_accuracy", "overview": "Computes pairwise comparison DPO accuracy", "function_signature": "def compute_dpo_accuracy(token_level_scores, acc, response_mask, n_samples):", "input_parameters": "- `token_level_scores`: Token-level reward scores\n- `acc`: Accuracy labels\n- `response_mask`: Response mask\n- `n_samples`: Sample count per group", "detailed_description": "Uses pairwise comparison to evaluate reward model accuracy. Aggregates token-level scores to sequence-level scores, constructs sample pairs for comparison. Judges directional consistency by computing score differences and accuracy differences.\nUses weighted average to compute accuracy, with higher weights for sample pairs with larger accuracy differences. Returns 0.5 when all samples have the same accuracy.", "output": "Weighted pairwise comparison accuracy", "func_implementation": "", "test_code_path": "code/test_code/test_dpo_accuracy.py", "implementation_location": "code/recipe/prime/prime_core_algos.py:line 119-143", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_dpo_accuracy`.\n\n**Function Overview:**\nComputes pairwise comparison DPO accuracy\n\n**Function Signature:**\n```python\ndef compute_dpo_accuracy(token_level_scores, acc, response_mask, n_samples):\n```\n\n**Input Parameters:**\n- `token_level_scores`: Token-level reward scores\n- `acc`: Accuracy labels\n- `response_mask`: Response mask\n- `n_samples`: Sample count per group\n\n**Detailed Description:**\nUses pairwise comparison to evaluate reward model accuracy. Aggregates token-level scores to sequence-level scores, constructs sample pairs for comparison. Judges directional consistency by computing score differences and accuracy differences.\nUses weighted average to compute accuracy, with higher weights for sample pairs with larger accuracy differences. Returns 0.5 when all samples have the same accuracy.\n\n**Expected Output:**\nWeighted pairwise comparison accuracy\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport torch\n\ndef compute_dpo_accuracy(token_level_scores, acc, response_mask, n_samples):\n    \"\"\"\n    Computes pairwise comparison DPO accuracy.\n    \n    Uses pairwise comparison to evaluate reward model accuracy. Aggregates token-level scores \n    to sequence-level scores, constructs sample pairs for comparison. Judges directional \n    consistency by computing score differences and accuracy differences.\n    Uses weighted average to compute accuracy, with higher weights for sample pairs with \n    larger accuracy differences. Returns 0.5 when all samples have the same accuracy.\n    \n    Args:\n        token_level_scores (torch.Tensor): Token-level reward scores of shape [batch_size, seq_len]\n        acc (torch.Tensor): Accuracy labels of shape [batch_size]\n        response_mask (torch.Tensor): Response mask of shape [batch_size, seq_len]\n        n_samples (int): Sample count per group\n    \n    Returns:\n        float: Weighted pairwise comparison accuracy\n    \"\"\"\n    # Ensure inputs are tensors\n    if not isinstance(token_level_scores, torch.Tensor):\n        token_level_scores = torch.tensor(token_level_scores)\n    if not isinstance(acc, torch.Tensor):\n        acc = torch.tensor(acc)\n    if not isinstance(response_mask, torch.Tensor):\n        response_mask = torch.tensor(response_mask)\n    \n    # Get batch size\n    batch_size = token_level_scores.shape[0]\n    \n    # Aggregate token-level scores to sequence-level scores\n    # Apply response mask to focus only on response tokens\n    masked_scores = token_level_scores * response_mask\n    # Sum scores across sequence dimension\n    seq_scores = masked_scores.sum(dim=1)\n    \n    # Reshape to groups of n_samples\n    # Each group contains n_samples that should be compared pairwise\n    n_groups = batch_size // n_samples\n    if n_groups == 0:\n        # If batch size is smaller than n_samples, treat entire batch as one group\n        n_groups = 1\n        n_samples = batch_size\n    \n    # Reshape scores and accuracy labels\n    seq_scores_grouped = seq_scores.view(n_groups, n_samples)\n    acc_grouped = acc.view(n_groups, n_samples)\n    \n    total_weighted_correct = 0.0\n    total_weight = 0.0\n    \n    # Process each group separately\n    for group_idx in range(n_groups):\n        group_scores = seq_scores_grouped[group_idx]\n        group_acc = acc_grouped[group_idx]\n        \n        # Generate all unique pairs within the group\n        for i in range(n_samples):\n            for j in range(i + 1, n_samples):\n                score_i = group_scores[i]\n                score_j = group_scores[j]\n                acc_i = group_acc[i]\n                acc_j = group_acc[j]\n                \n                # Compute score difference\n                score_diff = score_i - score_j\n                \n                # Compute accuracy difference\n                acc_diff = acc_i - acc_j\n                \n                # Determine if the score difference direction matches accuracy difference direction\n                # Both positive or both negative means correct direction\n                if score_diff * acc_diff > 0:\n                    # Direction matches\n                    correct = 1.0\n                elif score_diff * acc_diff < 0:\n                    # Direction opposite\n                    correct = 0.0\n                else:\n                    # One or both differences are zero\n                    # If both differences are zero, it's ambiguous - treat as 0.5\n                    if score_diff == 0 and acc_diff == 0:\n                        correct = 0.5\n                    elif score_diff == 0:\n                        # Score difference zero but accuracy difference non-zero\n                        # Can't determine direction - treat as 0.5\n                        correct = 0.5\n                    else:\n                        # Accuracy difference zero but score difference non-zero\n                        # Models disagree but ground truth doesn't - treat as 0.5\n                        correct = 0.5\n                \n                # Weight by absolute accuracy difference\n                weight = torch.abs(acc_diff).item()\n                \n                total_weighted_correct += correct * weight\n                total_weight += weight\n    \n    # Compute weighted accuracy\n    if total_weight > 0:\n        weighted_accuracy = total_weighted_correct / total_weight\n    else:\n        # If all weights are zero (all accuracy differences are zero)\n        weighted_accuracy = 0.5\n    \n    return weighted_accuracy\n```"], "usage": {"prompt_tokens": 362, "completion_tokens": 947, "total_tokens": 1309, "per_completion": [{"prompt_tokens": 362, "completion_tokens": 947, "total_tokens": 1309}]}}
{"function_name": "compute_dpo_abs_accuracy", "overview": "Computes absolute accuracy metric", "function_signature": "def compute_dpo_abs_accuracy(token_level_scores, acc, response_mask, n_samples):", "input_parameters": "- `token_level_scores`: Token-level reward scores\n- `acc`: Accuracy labels\n- `response_mask`: Response mask", "detailed_description": "Converts aggregated token-level scores to predicted labels, compares with true labels to compute absolute accuracy.", "output": "Absolute classification accuracy", "func_implementation": "", "test_code_path": "code/test_code/test_dpo_abs_accuracy.py", "implementation_location": "code/recipe/prime/prime_core_algos.py:line 146-147", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_dpo_abs_accuracy`.\n\n**Function Overview:**\nComputes absolute accuracy metric\n\n**Function Signature:**\n```python\ndef compute_dpo_abs_accuracy(token_level_scores, acc, response_mask, n_samples):\n```\n\n**Input Parameters:**\n- `token_level_scores`: Token-level reward scores\n- `acc`: Accuracy labels\n- `response_mask`: Response mask\n\n**Detailed Description:**\nConverts aggregated token-level scores to predicted labels, compares with true labels to compute absolute accuracy.\n\n**Expected Output:**\nAbsolute classification accuracy\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\ndef compute_dpo_abs_accuracy(token_level_scores, acc, response_mask, n_samples):\n    \"\"\"\n    Compute absolute classification accuracy for DPO (Direct Preference Optimization).\n    \n    This function converts aggregated token-level reward scores to predicted labels,\n    then compares them with true accuracy labels to compute absolute accuracy.\n    \n    Args:\n        token_level_scores: Tensor of shape [batch_size, seq_len] containing \n                           token-level reward scores.\n        acc: Tensor of shape [batch_size] containing true accuracy labels \n             (0 for incorrect, 1 for correct).\n        response_mask: Tensor of shape [batch_size, seq_len] indicating which \n                      tokens belong to the response (1 for response tokens, \n                      0 for padding/context tokens).\n        n_samples: Number of samples in the batch (for normalization).\n    \n    Returns:\n        float: Absolute classification accuracy.\n    \"\"\"\n    import torch\n    \n    # Ensure inputs are torch tensors\n    if not isinstance(token_level_scores, torch.Tensor):\n        token_level_scores = torch.tensor(token_level_scores)\n    if not isinstance(acc, torch.Tensor):\n        acc = torch.tensor(acc)\n    if not isinstance(response_mask, torch.Tensor):\n        response_mask = torch.tensor(response_mask)\n    \n    # Apply response mask to token-level scores\n    # Only consider tokens that are part of the response\n    masked_scores = token_level_scores * response_mask\n    \n    # Aggregate token-level scores to sequence-level scores\n    # Sum over sequence dimension for each sample in the batch\n    # response_mask.sum(dim=1) gives the number of response tokens per sample\n    seq_scores = masked_scores.sum(dim=1) / response_mask.sum(dim=1).clamp(min=1)\n    \n    # Convert sequence-level scores to binary predictions\n    # Positive scores predict class 1 (correct), negative scores predict class 0 (incorrect)\n    pred_labels = (seq_scores > 0).float()\n    \n    # Compare predictions with true labels\n    correct_predictions = (pred_labels == acc).float()\n    \n    # Compute absolute accuracy\n    accuracy = correct_predictions.sum().item() / n_samples\n    \n    return accuracy\n```"], "usage": {"prompt_tokens": 301, "completion_tokens": 471, "total_tokens": 772, "per_completion": [{"prompt_tokens": 301, "completion_tokens": 471, "total_tokens": 772}]}}
{"function_name": "compute_rloo_advantage_return", "overview": "Implements RLOO advantage estimation, supporting multiple reward source fusion", "function_signature": "def compute_rloo_advantage_return(data: verl.DataProto, response_mask: torch.Tensor, n_samples, config):", "input_parameters": "- `data`: Data protocol object containing DataProto.batch fields\n- `data.batch[\"rm_scores\"]`: Implicit reward scores\n- `data.batch[\"acc\"]`: Binary accuracy labels\n- `data.batch[\"prompts\"]`: Input prompt sequences\n- `data.batch[\"attention_mask\"]`: Attention mask\n- `response_mask`: Response mask indicating valid token positions\n- `n_samples`: Sample count per group, used for RLOO grouping computation\n- `config`: Reward weight configuration, hierarchical configuration object\n- `config.algorithm.reward_dpo_coef`: DPO reward weight coefficient, default 5.0\n- `config.algorithm.reward_gt_coef`: Accuracy reward weight coefficient, default 5.0", "detailed_description": "Function supports advantage computation with multi-source reward fusion. Reward sources include reward model scores and accuracy rewards.\nGroups multiple responses generated from the same prompt by n_samples intervals.\nUses RLOO to compute advantage function for each reward source and fuses multiple reward sources, computes cumulative return values after reward fusion and applies whitening.", "output": "- `advantages`: Whitened advantage function (torch.Tensor, shape: [batch_size, seq_len])\n- `returns`: Cumulative return values (torch.Tensor, shape: [batch_size, seq_len])", "func_implementation": "", "test_code_path": "code/test_code/test_rloo_advantage_return.py", "implementation_location": "code/recipe/prime/prime_core_algos.py:line 21-79", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_rloo_advantage_return`.\n\n**Function Overview:**\nImplements RLOO advantage estimation, supporting multiple reward source fusion\n\n**Function Signature:**\n```python\ndef compute_rloo_advantage_return(data: verl.DataProto, response_mask: torch.Tensor, n_samples, config):\n```\n\n**Input Parameters:**\n- `data`: Data protocol object containing DataProto.batch fields\n- `data.batch[\"rm_scores\"]`: Implicit reward scores\n- `data.batch[\"acc\"]`: Binary accuracy labels\n- `data.batch[\"prompts\"]`: Input prompt sequences\n- `data.batch[\"attention_mask\"]`: Attention mask\n- `response_mask`: Response mask indicating valid token positions\n- `n_samples`: Sample count per group, used for RLOO grouping computation\n- `config`: Reward weight configuration, hierarchical configuration object\n- `config.algorithm.reward_dpo_coef`: DPO reward weight coefficient, default 5.0\n- `config.algorithm.reward_gt_coef`: Accuracy reward weight coefficient, default 5.0\n\n**Detailed Description:**\nFunction supports advantage computation with multi-source reward fusion. Reward sources include reward model scores and accuracy rewards.\nGroups multiple responses generated from the same prompt by n_samples intervals.\nUses RLOO to compute advantage function for each reward source and fuses multiple reward sources, computes cumulative return values after reward fusion and applies whitening.\n\n**Expected Output:**\n- `advantages`: Whitened advantage function (torch.Tensor, shape: [batch_size, seq_len])\n- `returns`: Cumulative return values (torch.Tensor, shape: [batch_size, seq_len])\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport torch\nimport torch.nn.functional as F\nfrom typing import Tuple\n\ndef compute_rloo_advantage_return(data: verl.DataProto, response_mask: torch.Tensor, n_samples: int, config) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Implements RLOO advantage estimation with multiple reward source fusion.\n    \n    Args:\n        data: Data protocol object containing batch fields\n        response_mask: Response mask indicating valid token positions (shape: [batch_size, seq_len])\n        n_samples: Sample count per group for RLOO grouping computation\n        config: Configuration object containing reward weight coefficients\n        \n    Returns:\n        advantages: Whitened advantage function (shape: [batch_size, seq_len])\n        returns: Cumulative return values (shape: [batch_size, seq_len])\n    \"\"\"\n    batch = data.batch\n    \n    # Extract required tensors from batch\n    rm_scores = batch[\"rm_scores\"]  # Reward model scores\n    acc = batch[\"acc\"]  # Binary accuracy labels\n    prompts = batch[\"prompts\"]  # Input prompt sequences\n    attention_mask = batch[\"attention_mask\"]  # Attention mask\n    \n    # Get reward weight coefficients from config\n    dpo_coef = getattr(config.algorithm, \"reward_dpo_coef\", 5.0)\n    gt_coef = getattr(config.algorithm, \"reward_gt_coef\", 5.0)\n    \n    batch_size = rm_scores.size(0)\n    seq_len = rm_scores.size(1)\n    \n    # Ensure n_samples is valid\n    if n_samples <= 0:\n        raise ValueError(f\"n_samples must be positive, got {n_samples}\")\n    \n    if batch_size % n_samples != 0:\n        raise ValueError(f\"batch_size {batch_size} must be divisible by n_samples {n_samples}\")\n    \n    # Reshape tensors for group processing\n    num_groups = batch_size // n_samples\n    \n    # Reshape to [num_groups, n_samples, seq_len]\n    rm_scores_grouped = rm_scores.view(num_groups, n_samples, seq_len)\n    acc_grouped = acc.view(num_groups, n_samples, seq_len)\n    response_mask_grouped = response_mask.view(num_groups, n_samples, seq_len)\n    \n    # Initialize advantage and return tensors\n    advantages = torch.zeros_like(rm_scores)\n    returns = torch.zeros_like(rm_scores)\n    \n    # Process each group separately\n    for group_idx in range(num_groups):\n        # Extract current group data\n        group_rm_scores = rm_scores_grouped[group_idx]  # [n_samples, seq_len]\n        group_acc = acc_grouped[group_idx]  # [n_samples, seq_len]\n        group_response_mask = response_mask_grouped[group_idx]  # [n_samples, seq_len]\n        \n        # Compute advantages for each sample in the group using RLOO\n        # RLOO: For each sample i, compute advantage using all other samples in the group\n        group_advantages = torch.zeros_like(group_rm_scores)\n        \n        for i in range(n_samples):\n            # Create mask for all samples except i\n            mask = torch.ones(n_samples, dtype=torch.bool, device=rm_scores.device)\n            mask[i] = False\n            \n            # Get other samples' rewards\n            other_rm_scores = group_rm_scores[mask]  # [n_samples-1, seq_len]\n            other_acc = group_acc[mask]  # [n_samples-1, seq_len]\n            \n            # Compute mean rewards from other samples\n            mean_other_rm_scores = other_rm_scores.mean(dim=0, keepdim=True)  # [1, seq_len]\n            mean_other_acc = other_acc.mean(dim=0, keepdim=True)  # [1, seq_len]\n            \n            # Get current sample's rewards\n            current_rm_score = group_rm_scores[i:i+1]  # [1, seq_len]\n            current_acc = group_acc[i:i+1]  # [1, seq_len]\n            \n            # Compute advantage for each reward source\n            # Advantage = current_reward - mean_other_rewards\n            rm_advantage = current_rm_score - mean_other_rm_scores\n            acc_advantage = current_acc - mean_other_acc\n            \n            # Fuse advantages from multiple reward sources\n            fused_advantage = dpo_coef * rm_advantage + gt_coef * acc_advantage\n            \n            # Apply response mask to keep only valid token positions\n            fused_advantage = fused_advantage * group_response_mask[i:i+1]\n            \n            group_advantages[i] = fused_advantage.squeeze(0)\n        \n        # Compute fused rewards for return calculation\n        fused_rewards = dpo_coef * group_rm_scores + gt_coef * group_acc\n        \n        # Apply response mask to rewards\n        fused_rewards = fused_rewards * group_response_mask\n        \n        # Compute returns (cumulative sum of rewards)\n        # We'll compute returns along the sequence dimension\n        group_returns = torch.zeros_like(fused_rewards)\n        \n        # Compute cumulative sum from the end of sequence\n        # This assumes rewards are given per token and we want to accumulate future rewards\n        cum_sum = torch.zeros_like(fused_rewards[:, 0])\n        \n        for t in reversed(range(seq_len)):\n            cum_sum = fused_rewards[:, t] + cum_sum\n            group_returns[:, t] = cum_sum\n        \n        # Store results back to the full batch tensors\n        start_idx = group_idx * n_samples\n        end_idx = start_idx + n_samples\n        \n        advantages[start_idx:end_idx] = group_advantages\n        returns[start_idx:end_idx] = group_returns\n    \n    # Apply whitening to advantages (normalize to zero mean and unit variance)\n    # Only whiten over valid positions indicated by response_mask\n    valid_advantages = advantages[response_mask.bool()]\n    \n    if valid_advantages.numel() > 0:\n        mean = valid_advantages.mean()\n        std = valid_advantages.std()\n        \n        # Avoid division by zero\n        if std > 1e-8:\n            advantages = (advantages - mean) / std\n        else:\n            advantages = advantages - mean\n    \n    # Apply response mask to final outputs\n    advantages = advantages * response_mask\n    returns = returns * response_mask\n    \n    return advantages, returns\n```"], "usage": {"prompt_tokens": 533, "completion_tokens": 1403, "total_tokens": 1936, "per_completion": [{"prompt_tokens": 533, "completion_tokens": 1403, "total_tokens": 1936}]}}
{"function_name": "RayPRIMETrainer.filter_and_downsample", "overview": "Implements intelligent sample filtering and priority downsampling for PRIME algorithm", "function_signature": "def filter_and_downsample(self, scores, batch: DataProto):", "input_parameters": "- `scores`: Verification score list, binary accuracy labels\n- `batch`: Data batch object\n- `config`: Filtering parameter configuration", "detailed_description": "Filters high-quality samples from oversampled candidate responses. Reconstructs verification scores into 2D matrix, performs dual filtering: accuracy filtering and length filtering.\nAccuracy filtering retains sample groups with average accuracy within configured range; length filtering removes sample groups exceeding maximum length threshold.\nSample groups meeting filtering conditions are retained with priority, ultimately retaining 1/oversample_factor proportion of samples. If all groups fail to meet filtering conditions, algorithm still retains corresponding proportion of samples to ensure training continues.", "output": "Filtered high-quality data batch", "func_implementation": "", "test_code_path": "code/test_code/test_filter_and_downsample.py", "implementation_location": "code/recipe/prime/prime_ray_trainer.py:line 540-572", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `RayPRIMETrainer.filter_and_downsample`.\n\n**Function Overview:**\nImplements intelligent sample filtering and priority downsampling for PRIME algorithm\n\n**Function Signature:**\n```python\ndef filter_and_downsample(self, scores, batch: DataProto):\n```\n\n**Input Parameters:**\n- `scores`: Verification score list, binary accuracy labels\n- `batch`: Data batch object\n- `config`: Filtering parameter configuration\n\n**Detailed Description:**\nFilters high-quality samples from oversampled candidate responses. Reconstructs verification scores into 2D matrix, performs dual filtering: accuracy filtering and length filtering.\nAccuracy filtering retains sample groups with average accuracy within configured range; length filtering removes sample groups exceeding maximum length threshold.\nSample groups meeting filtering conditions are retained with priority, ultimately retaining 1/oversample_factor proportion of samples. If all groups fail to meet filtering conditions, algorithm still retains corresponding proportion of samples to ensure training continues.\n\n**Expected Output:**\nFiltered high-quality data batch\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\ndef filter_and_downsample(self, scores, batch: DataProto):\n    \"\"\"\n    Implements intelligent sample filtering and priority downsampling for PRIME algorithm.\n    \n    Filters high-quality samples from oversampled candidate responses. Reconstructs verification \n    scores into 2D matrix, performs dual filtering: accuracy filtering and length filtering.\n    \n    Args:\n        scores: Verification score list, binary accuracy labels\n        batch: Data batch object containing responses and metadata\n        config: Filtering parameter configuration (accessed via self.config)\n        \n    Returns:\n        DataProto: Filtered high-quality data batch\n    \"\"\"\n    # Get configuration parameters\n    config = self.config\n    oversample_factor = config.oversample_factor\n    min_accuracy_threshold = config.min_accuracy_threshold\n    max_accuracy_threshold = config.max_accuracy_threshold\n    max_length_threshold = config.max_length_threshold\n    \n    # Reconstruct scores into 2D matrix: [num_samples, oversample_factor]\n    num_samples = len(batch) // oversample_factor\n    score_matrix = scores.reshape(num_samples, oversample_factor)\n    \n    # Calculate average accuracy for each sample group\n    group_accuracies = score_matrix.mean(axis=1)\n    \n    # Calculate response lengths for each sample\n    # Assuming batch has 'responses' field containing text responses\n    response_lengths = [len(resp) for resp in batch.responses]\n    length_matrix = np.array(response_lengths).reshape(num_samples, oversample_factor)\n    group_max_lengths = length_matrix.max(axis=1)\n    \n    # Perform dual filtering\n    accuracy_mask = (group_accuracies >= min_accuracy_threshold) & \\\n                    (group_accuracies <= max_accuracy_threshold)\n    length_mask = group_max_lengths <= max_length_threshold\n    \n    # Combined filtering mask\n    combined_mask = accuracy_mask & length_mask\n    \n    # Calculate number of groups to retain (1/oversample_factor proportion)\n    groups_to_retain = max(1, num_samples // oversample_factor)\n    \n    # Select groups with priority: first those meeting all conditions,\n    # then fill remaining slots with other groups if needed\n    if combined_mask.sum() >= groups_to_retain:\n        # Enough groups meet all conditions\n        selected_group_indices = np.where(combined_mask)[0][:groups_to_retain]\n    else:\n        # Not enough groups meet all conditions, include some that don't meet all criteria\n        selected_group_indices = np.where(combined_mask)[0]\n        remaining_slots = groups_to_retain - len(selected_group_indices)\n        \n        if remaining_slots > 0:\n            # Select additional groups based on accuracy score (highest first)\n            other_groups = np.where(~combined_mask)[0]\n            other_group_accuracies = group_accuracies[other_groups]\n            # Sort by accuracy in descending order\n            sorted_indices = np.argsort(-other_group_accuracies)\n            additional_indices = other_groups[sorted_indices[:remaining_slots]]\n            selected_group_indices = np.concatenate([selected_group_indices, additional_indices])\n    \n    # Ensure we have at least one group selected\n    if len(selected_group_indices) == 0:\n        # Fallback: select groups with highest accuracy\n        selected_group_indices = np.argsort(-group_accuracies)[:groups_to_retain]\n    \n    # Flatten selected indices to get individual sample indices\n    selected_sample_indices = []\n    for group_idx in selected_group_indices:\n        start_idx = group_idx * oversample_factor\n        end_idx = start_idx + oversample_factor\n        selected_sample_indices.extend(range(start_idx, end_idx))\n    \n    # Create filtered batch\n    filtered_batch = DataProto()\n    \n    # Copy selected samples to new batch\n    # Assuming DataProto has a method to create subset or we need to manually copy fields\n    for idx in selected_sample_indices:\n        # Copy each field from original batch to filtered batch\n        # This depends on the actual structure of DataProto\n        # Here's a generic implementation assuming batch is indexable\n        filtered_batch.append(batch[idx])\n    \n    return filtered_batch\n```"], "usage": {"prompt_tokens": 390, "completion_tokens": 896, "total_tokens": 1286, "per_completion": [{"prompt_tokens": 390, "completion_tokens": 896, "total_tokens": 1286}]}}
