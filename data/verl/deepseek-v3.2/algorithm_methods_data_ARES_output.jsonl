{"function_name": "compute_score", "overview": "Computes ARES hierarchical entropy rewards, dynamically adjusting reward signals based on sample difficulty, accuracy, and High Window Entropy (HWE) token count.", "function_signature": "def compute_score(reward_inputs: List[Dict[str, Any]], format_weight: float = 0.1, alpha_length_bonus: float = 0.5, alpha_entropy: float = 0.5) -> List[Dict[str, float]]:", "input_parameters": "- `reward_inputs`: List of reward input dictionaries, each containing:\n- `response`: Model-generated response text\n- `ground_truth`: Ground truth answer\n- `difficulty`: Sample difficulty label (\"easy\" / \"medium\" / \"hard\")\n- `high_entropy_token_num`: Number of HWE tokens in response\n- `target_high_entropy_token_num`: Target HWE token count (difficulty-related)\n- `alpha_entropy`: Entropy reward coefficient (difficulty-related)\n- `accuracy`: Accuracy label (optional, if not provided, extracted from response and evaluated)\n- `format_weight`: Format reward weight, default 0.1\n- `alpha_length_bonus`: Length reward coefficient (legacy parameter, actual usage is alpha_entropy)\n- `alpha_entropy`: Default entropy reward coefficient, default 0.5 (used if sample doesn't specify)", "detailed_description": "This function implements ARES's core reward shaping mechanism, adaptively adjusting reasoning resource allocation based on task difficulty and model performance. The function computes format reward, accuracy reward, and entropy reward. For easy questions, when the answer is correct, it penalizes overly long reasoning chains, while when the answer is incorrect, it provides moderate positive rewards to encourage the model to explore more possibilities. Medium-difficulty questions require reasoning length closer to the target value; both excessive and insufficient reasoning are penalized when the answer is correct, but longer reasoning is still encouraged when the answer is incorrect. Hard questions adopt the opposite strategy: when the answer is correct, reasoning chains close to or exceeding the target are rewarded, and penalties are only applied when reasoning is significantly insufficient; when the answer is incorrect, longer reasoning is also encouraged to find the correct path.\nThe entire reward mechanism uses Huber loss function to smoothly handle penalty terms, Sigmoid function to achieve reward saturation effects, and sets different tolerance boundaries and reward caps for different difficulties.\nDifficulty-related hyperparameters that may be used:\n- `MARGIN_FRAC`: Tolerance band width (easy: 15%, medium: 25%, hard: 35%)\n- `KAPPA`: Huber function inflection point (easy: 2.0, medium: 3.0, hard: 4.0)\n- `TEMP`: Sigmoid temperature parameter (easy: 2.0, medium: 2.5, hard: 3.0)\n- `CAP_SCALE`: Entropy reward cap coefficient (easy: 1.0, medium: 1.0, hard: 1.2)", "output": "- `scores`: List of score dictionaries, each containing:\n- `overall`: Total score = accuracy + entropy_score\n- `accuracy`: Accuracy score (0.0 or 1.0)\n- `format`: Format score (0.0 or 1.0)\n- `high_entropy_token_num_score`: Entropy reward score", "func_implementation": "", "test_code_path": "code/test_code/test_compute_score.py", "implementation_location": "code/examples/reward_function/aepo.py:line 58-133", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_score`.\n\n**Function Overview:**\nComputes ARES hierarchical entropy rewards, dynamically adjusting reward signals based on sample difficulty, accuracy, and High Window Entropy (HWE) token count.\n\n**Function Signature:**\n```python\ndef compute_score(reward_inputs: List[Dict[str, Any]], format_weight: float = 0.1, alpha_length_bonus: float = 0.5, alpha_entropy: float = 0.5) -> List[Dict[str, float]]:\n```\n\n**Input Parameters:**\n- `reward_inputs`: List of reward input dictionaries, each containing:\n- `response`: Model-generated response text\n- `ground_truth`: Ground truth answer\n- `difficulty`: Sample difficulty label (\"easy\" / \"medium\" / \"hard\")\n- `high_entropy_token_num`: Number of HWE tokens in response\n- `target_high_entropy_token_num`: Target HWE token count (difficulty-related)\n- `alpha_entropy`: Entropy reward coefficient (difficulty-related)\n- `accuracy`: Accuracy label (optional, if not provided, extracted from response and evaluated)\n- `format_weight`: Format reward weight, default 0.1\n- `alpha_length_bonus`: Length reward coefficient (legacy parameter, actual usage is alpha_entropy)\n- `alpha_entropy`: Default entropy reward coefficient, default 0.5 (used if sample doesn't specify)\n\n**Detailed Description:**\nThis function implements ARES's core reward shaping mechanism, adaptively adjusting reasoning resource allocation based on task difficulty and model performance. The function computes format reward, accuracy reward, and entropy reward. For easy questions, when the answer is correct, it penalizes overly long reasoning chains, while when the answer is incorrect, it provides moderate positive rewards to encourage the model to explore more possibilities. Medium-difficulty questions require reasoning length closer to the target value; both excessive and insufficient reasoning are penalized when the answer is correct, but longer reasoning is still encouraged when the answer is incorrect. Hard questions adopt the opposite strategy: when the answer is correct, reasoning chains close to or exceeding the target are rewarded, and penalties are only applied when reasoning is significantly insufficient; when the answer is incorrect, longer reasoning is also encouraged to find the correct path.\nThe entire reward mechanism uses Huber loss function to smoothly handle penalty terms, Sigmoid function to achieve reward saturation effects, and sets different tolerance boundaries and reward caps for different difficulties.\nDifficulty-related hyperparameters that may be used:\n- `MARGIN_FRAC`: Tolerance band width (easy: 15%, medium: 25%, hard: 35%)\n- `KAPPA`: Huber function inflection point (easy: 2.0, medium: 3.0, hard: 4.0)\n- `TEMP`: Sigmoid temperature parameter (easy: 2.0, medium: 2.5, hard: 3.0)\n- `CAP_SCALE`: Entropy reward cap coefficient (easy: 1.0, medium: 1.0, hard: 1.2)\n\n**Expected Output:**\n- `scores`: List of score dictionaries, each containing:\n- `overall`: Total score = accuracy + entropy_score\n- `accuracy`: Accuracy score (0.0 or 1.0)\n- `format`: Format score (0.0 or 1.0)\n- `high_entropy_token_num_score`: Entropy reward score\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport math\nfrom typing import List, Dict, Any\n\n\ndef compute_score(\n    reward_inputs: List[Dict[str, Any]],\n    format_weight: float = 0.1,\n    alpha_length_bonus: float = 0.5,\n    alpha_entropy: float = 0.5\n) -> List[Dict[str, float]]:\n    \"\"\"\n    Computes ARES hierarchical entropy rewards, dynamically adjusting reward signals\n    based on sample difficulty, accuracy, and High Window Entropy (HWE) token count.\n    \n    Args:\n        reward_inputs: List of reward input dictionaries\n        format_weight: Format reward weight, default 0.1\n        alpha_length_bonus: Length reward coefficient (legacy parameter, actual usage is alpha_entropy)\n        alpha_entropy: Default entropy reward coefficient, default 0.5\n        \n    Returns:\n        List of score dictionaries containing overall, accuracy, format, and high_entropy_token_num_score\n    \"\"\"\n    \n    # Difficulty-related hyperparameters\n    DIFFICULTY_PARAMS = {\n        \"easy\": {\n            \"MARGIN_FRAC\": 0.15,  # 15% tolerance band\n            \"KAPPA\": 2.0,         # Huber function inflection point\n            \"TEMP\": 2.0,          # Sigmoid temperature\n            \"CAP_SCALE\": 1.0      # Entropy reward cap coefficient\n        },\n        \"medium\": {\n            \"MARGIN_FRAC\": 0.25,  # 25% tolerance band\n            \"KAPPA\": 3.0,         # Huber function inflection point\n            \"TEMP\": 2.5,          # Sigmoid temperature\n            \"CAP_SCALE\": 1.0      # Entropy reward cap coefficient\n        },\n        \"hard\": {\n            \"MARGIN_FRAC\": 0.35,  # 35% tolerance band\n            \"KAPPA\": 4.0,         # Huber function inflection point\n            \"TEMP\": 3.0,          # Sigmoid temperature\n            \"CAP_SCALE\": 1.2      # Entropy reward cap coefficient\n        }\n    }\n    \n    def huber_loss(x: float, kappa: float) -> float:\n        \"\"\"Huber loss function for smooth penalty handling.\"\"\"\n        if abs(x) <= kappa:\n            return 0.5 * x * x\n        else:\n            return kappa * (abs(x) - 0.5 * kappa)\n    \n    def sigmoid(x: float, temp: float) -> float:\n        \"\"\"Sigmoid function for reward saturation.\"\"\"\n        return 1.0 / (1.0 + math.exp(-x / temp))\n    \n    def compute_entropy_reward(\n        high_entropy_token_num: int,\n        target_high_entropy_token_num: int,\n        accuracy: float,\n        difficulty: str,\n        alpha_entropy: float\n    ) -> float:\n        \"\"\"\n        Compute entropy reward based on difficulty and accuracy.\n        \n        For easy questions: When correct, penalize overly long reasoning.\n                           When incorrect, provide moderate positive rewards.\n        For medium questions: When correct, penalize both excessive and insufficient reasoning.\n                              When incorrect, encourage longer reasoning.\n        For hard questions: When correct, reward reasoning close to or exceeding target.\n                            When incorrect, encourage longer reasoning.\n        \"\"\"\n        params = DIFFICULTY_PARAMS[difficulty]\n        margin = params[\"MARGIN_FRAC\"] * target_high_entropy_token_num\n        kappa = params[\"KAPPA\"]\n        temp = params[\"TEMP\"]\n        cap_scale = params[\"CAP_SCALE\"]\n        \n        # Calculate deviation from target\n        deviation = high_entropy_token_num - target_high_entropy_token_num\n        \n        if difficulty == \"easy\":\n            if accuracy > 0.5:  # Correct answer\n                # Penalize overly long reasoning for easy questions when correct\n                if deviation > margin:\n                    penalty = huber_loss(deviation - margin, kappa)\n                    reward = -penalty\n                else:\n                    reward = 0.0\n            else:  # Incorrect answer\n                # Encourage exploration for easy questions when incorrect\n                reward = sigmoid(deviation, temp) * alpha_entropy\n                \n        elif difficulty == \"medium\":\n            if accuracy > 0.5:  # Correct answer\n                # Penalize both excessive and insufficient reasoning for medium questions when correct\n                if abs(deviation) > margin:\n                    penalty = huber_loss(abs(deviation) - margin, kappa)\n                    reward = -penalty\n                else:\n                    reward = 0.0\n            else:  # Incorrect answer\n                # Encourage longer reasoning for medium questions when incorrect\n                reward = sigmoid(deviation, temp) * alpha_entropy\n                \n        else:  # hard\n            if accuracy > 0.5:  # Correct answer\n                # Reward reasoning close to or exceeding target for hard questions when correct\n                if deviation < -margin:\n                    penalty = huber_loss(-deviation - margin, kappa)\n                    reward = -penalty\n                else:\n                    reward = sigmoid(deviation, temp) * alpha_entropy\n            else:  # Incorrect answer\n                # Encourage longer reasoning for hard questions when incorrect\n                reward = sigmoid(deviation, temp) * alpha_entropy\n        \n        # Apply cap scaling\n        return reward * cap_scale\n    \n    def extract_accuracy(response: str, ground_truth: str, provided_accuracy: Any) -> float:\n        \"\"\"\n        Extract accuracy from response if not provided.\n        Simple implementation - in practice this would be more sophisticated.\n        \"\"\"\n        if provided_accuracy is not None:\n            return float(provided_accuracy)\n        \n        # Simple exact match for demonstration\n        # In practice, this would use more sophisticated evaluation\n        response_clean = response.strip().lower()\n        ground_truth_clean = ground_truth.strip().lower()\n        \n        # Check if ground truth appears in response\n        if ground_truth_clean in response_clean:\n            return 1.0\n        \n        # Check for numeric answers\n        try:\n            # Extract numbers from response\n            import re\n            response_numbers = re.findall(r'\\d+\\.?\\d*', response_clean)\n            ground_truth_numbers = re.findall(r'\\d+\\.?\\d*', ground_truth_clean)\n            \n            if response_numbers and ground_truth_numbers:\n                # Check if any response number matches ground truth\n                for resp_num in response_numbers:\n                    for gt_num in ground_truth_numbers:\n                        if abs(float(resp_num) - float(gt_num)) < 1e-6:\n                            return 1.0\n        except:\n            pass\n        \n        return 0.0\n    \n    def compute_format_reward(response: str) -> float:\n        \"\"\"\n        Compute format reward based on response structure.\n        Simple implementation - checks for basic formatting.\n        \"\"\"\n        # Check if response has reasonable structure\n        if not response or len(response.strip()) == 0:\n            return 0.0\n        \n        # Check for common reasoning indicators\n        reasoning_indicators = [\"step\", \"reason\", \"explain\", \"therefore\", \"thus\", \"so\", \"because\"]\n        response_lower = response.lower()\n        \n        has_reasoning = any(indicator in response_lower for indicator in reasoning_indicators)\n        has_answer = any(word in response_lower for word in [\"answer\", \"result\", \"solution\", \"final\"])\n        \n        if has_reasoning and has_answer:\n            return 1.0\n        elif has_reasoning or has_answer:\n            return 0.5\n        else:\n            return 0.0\n    \n    scores = []\n    \n    for input_data in reward_inputs:\n        # Extract required fields\n        response = input_data.get(\"response\", \"\")\n        ground_truth = input_data.get(\"ground_truth\", \"\")\n        difficulty = input_data.get(\"difficulty\", \"medium\").lower()\n        high_entropy_token_num = input_data.get(\"high_entropy_token_num\", 0)\n        target_high_entropy_token_num = input_data.get(\"target_high_entropy_token_num\", 0)\n        \n        # Use sample-specific alpha_entropy if provided, otherwise use default\n        sample_alpha_entropy = input_data.get(\"alpha_entropy\", alpha_entropy)\n        \n        # Extract or compute accuracy\n        provided_accuracy = input_data.get(\"accuracy\")\n        accuracy = extract_accuracy(response, ground_truth, provided_accuracy)\n        \n        # Compute format reward\n        format_reward = compute_format_reward(response)\n        \n        # Compute entropy reward\n        entropy_reward = compute_entropy_reward(\n            high_entropy_token_num=high_entropy_token_num,\n            target_high_entropy_token_num=target_high_entropy_token_num,\n            accuracy=accuracy,\n            difficulty=difficulty,\n            alpha_entropy=sample_alpha_entropy\n        )\n        \n        # Apply format weight\n        weighted_format_reward = format_reward * format_weight\n        \n        # Compute overall score\n        overall_score = accuracy + weighted_format_reward + entropy_reward\n        \n        # Create score dictionary\n        score_dict = {\n            \"overall\": overall_score,\n            \"accuracy\": accuracy,\n            \"format\": weighted_format_reward,\n            \"high_entropy_token_num_score\": entropy_reward\n        }\n        \n        scores.append(score_dict)\n    \n    return scores\n```"], "usage": {"prompt_tokens": 909, "completion_tokens": 2056, "total_tokens": 2965, "per_completion": [{"prompt_tokens": 909, "completion_tokens": 2056, "total_tokens": 2965}]}}
{"function_name": "RayPPOTrainer.update_difficulty_and_skip_gid_set", "overview": "Updates sample difficulty dictionary and adaptive entropy coefficients based on current batch rollout results, implementing online difficulty bucketing and resource budget adjustment.", "function_signature": "def update_difficulty_and_skip_gid_set(self, batch: DataProto) -> None:", "input_parameters": "- `batch`: DataProto object containing:\n- `global_id`: Sample global ID (for cross-batch tracking)\n- `accuracy`: Accuracy list (accuracy of each response)\n- `entropies`: Average entropy list (average token entropy of each response)\n- `high_entropy_token_num`: HWE token count list (HWE token count of each response)", "detailed_description": "This function is responsible for dynamically updating question difficulty classification and adaptively adjusting entropy penalty coefficients during training. The function needs to aggregate accuracy, entropy, and high-entropy token counts for each question by global identifier. Questions are categorized into easy, medium, and hard levels based on accuracy. Based on high-entropy token counts, the function needs to compute target reasoning length for each difficulty level and adjust entropy penalty coefficients based on deviation between actual generated length and target value. When adjusting entropy penalty coefficients, gradient ascent can be applied. For easy and medium questions, if reasoning is too long, increase penalty strength; for hard questions, increase encouragement when reasoning is insufficient.\nDifficulty assignment based on accuracy thresholds:\n- `acc_rate ≥ 2/3` → \"easy\"\n- `1/3 ≤ acc_rate < 2/3` → \"medium\"\n- `acc_rate < 1/3` → \"hard\"\n(Optional) The function adds samples that are \"all correct with extremely low average entropy\" to `self.skip_gid_set`, and subsequent training will skip these overly simple samples, focusing on challenging data.", "output": "- No direct return value, but updates the following class attributes:\n- `self.difficulty_dict_train`: Sample difficulty dictionary `{global_id: difficulty}`\n- `self.target_high_entropy_token_num_dict`: Target HWE token count `{difficulty: target_num}`\n- `self.alpha_entropy_dict`: Entropy coefficient dictionary `{difficulty: alpha}`\n- `self.skip_gid_set`: Skip sample set (optional)", "func_implementation": "", "test_code_path": "code/test_code/test_update_difficulty.py", "implementation_location": "code/verl/trainer/ray_trainer.py:line 775-825", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `RayPPOTrainer.update_difficulty_and_skip_gid_set`.\n\n**Function Overview:**\nUpdates sample difficulty dictionary and adaptive entropy coefficients based on current batch rollout results, implementing online difficulty bucketing and resource budget adjustment.\n\n**Function Signature:**\n```python\ndef update_difficulty_and_skip_gid_set(self, batch: DataProto) -> None:\n```\n\n**Input Parameters:**\n- `batch`: DataProto object containing:\n- `global_id`: Sample global ID (for cross-batch tracking)\n- `accuracy`: Accuracy list (accuracy of each response)\n- `entropies`: Average entropy list (average token entropy of each response)\n- `high_entropy_token_num`: HWE token count list (HWE token count of each response)\n\n**Detailed Description:**\nThis function is responsible for dynamically updating question difficulty classification and adaptively adjusting entropy penalty coefficients during training. The function needs to aggregate accuracy, entropy, and high-entropy token counts for each question by global identifier. Questions are categorized into easy, medium, and hard levels based on accuracy. Based on high-entropy token counts, the function needs to compute target reasoning length for each difficulty level and adjust entropy penalty coefficients based on deviation between actual generated length and target value. When adjusting entropy penalty coefficients, gradient ascent can be applied. For easy and medium questions, if reasoning is too long, increase penalty strength; for hard questions, increase encouragement when reasoning is insufficient.\nDifficulty assignment based on accuracy thresholds:\n- `acc_rate ≥ 2/3` → \"easy\"\n- `1/3 ≤ acc_rate < 2/3` → \"medium\"\n- `acc_rate < 1/3` → \"hard\"\n(Optional) The function adds samples that are \"all correct with extremely low average entropy\" to `self.skip_gid_set`, and subsequent training will skip these overly simple samples, focusing on challenging data.\n\n**Expected Output:**\n- No direct return value, but updates the following class attributes:\n- `self.difficulty_dict_train`: Sample difficulty dictionary `{global_id: difficulty}`\n- `self.target_high_entropy_token_num_dict`: Target HWE token count `{difficulty: target_num}`\n- `self.alpha_entropy_dict`: Entropy coefficient dictionary `{difficulty: alpha}`\n- `self.skip_gid_set`: Skip sample set (optional)\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["Here's the implementation of the `update_difficulty_and_skip_gid_set` function:\n\n```python\ndef update_difficulty_and_skip_gid_set(self, batch: DataProto) -> None:\n    \"\"\"\n    Updates sample difficulty dictionary and adaptive entropy coefficients based on current batch rollout results.\n    Implements online difficulty bucketing and resource budget adjustment.\n    \n    Args:\n        batch: DataProto object containing:\n            - global_id: Sample global ID (for cross-batch tracking)\n            - accuracy: Accuracy list (accuracy of each response)\n            - entropies: Average entropy list (average token entropy of each response)\n            - high_entropy_token_num: HWE token count list (HWE token count of each response)\n    \"\"\"\n    # Initialize dictionaries if they don't exist\n    if not hasattr(self, 'difficulty_dict_train'):\n        self.difficulty_dict_train = {}\n    if not hasattr(self, 'target_high_entropy_token_num_dict'):\n        self.target_high_entropy_token_num_dict = {'easy': 0, 'medium': 0, 'hard': 0}\n    if not hasattr(self, 'alpha_entropy_dict'):\n        self.alpha_entropy_dict = {'easy': 1.0, 'medium': 1.0, 'hard': 1.0}\n    if not hasattr(self, 'skip_gid_set'):\n        self.skip_gid_set = set()\n    \n    # Initialize temporary storage for aggregating statistics by difficulty level\n    difficulty_stats = {\n        'easy': {'hwe_counts': [], 'accuracies': [], 'entropies': []},\n        'medium': {'hwe_counts': [], 'accuracies': [], 'entropies': []},\n        'hard': {'hwe_counts': [], 'accuracies': [], 'entropies': []}\n    }\n    \n    # Process each sample in the batch\n    for i, gid in enumerate(batch.global_id):\n        # Get accuracy for this sample (average across responses)\n        acc_rate = sum(batch.accuracy[i]) / len(batch.accuracy[i]) if batch.accuracy[i] else 0.0\n        \n        # Determine difficulty based on accuracy thresholds\n        if acc_rate >= 2/3:\n            difficulty = 'easy'\n        elif acc_rate >= 1/3:\n            difficulty = 'medium'\n        else:\n            difficulty = 'hard'\n        \n        # Update difficulty dictionary\n        self.difficulty_dict_train[gid] = difficulty\n        \n        # Collect statistics for this difficulty level\n        if batch.high_entropy_token_num and i < len(batch.high_entropy_token_num):\n            hwe_count = sum(batch.high_entropy_token_num[i]) / len(batch.high_entropy_token_num[i])\n            difficulty_stats[difficulty]['hwe_counts'].append(hwe_count)\n        \n        if batch.accuracy and i < len(batch.accuracy):\n            difficulty_stats[difficulty]['accuracies'].append(acc_rate)\n        \n        if batch.entropies and i < len(batch.entropies):\n            avg_entropy = sum(batch.entropies[i]) / len(batch.entropies[i]) if batch.entropies[i] else 0.0\n            difficulty_stats[difficulty]['entropies'].append(avg_entropy)\n        \n        # Optional: Add samples that are \"all correct with extremely low average entropy\" to skip set\n        if acc_rate >= 1.0 and batch.entropies and i < len(batch.entropies):\n            avg_entropy = sum(batch.entropies[i]) / len(batch.entropies[i]) if batch.entropies[i] else 0.0\n            if avg_entropy < 0.1:  # Threshold for \"extremely low entropy\"\n                self.skip_gid_set.add(gid)\n    \n    # Update target HWE token counts for each difficulty level\n    for difficulty in ['easy', 'medium', 'hard']:\n        if difficulty_stats[difficulty]['hwe_counts']:\n            # Use median as target to be robust to outliers\n            sorted_counts = sorted(difficulty_stats[difficulty]['hwe_counts'])\n            median_idx = len(sorted_counts) // 2\n            self.target_high_entropy_token_num_dict[difficulty] = sorted_counts[median_idx]\n    \n    # Adjust entropy penalty coefficients based on deviation from target\n    learning_rate = 0.01  # Gradient ascent learning rate\n    \n    for difficulty in ['easy', 'medium', 'hard']:\n        if difficulty_stats[difficulty]['hwe_counts']:\n            avg_hwe = sum(difficulty_stats[difficulty]['hwe_counts']) / len(difficulty_stats[difficulty]['hwe_counts'])\n            target_hwe = self.target_high_entropy_token_num_dict[difficulty]\n            \n            # Calculate deviation from target\n            deviation = avg_hwe - target_hwe\n            \n            # Adjust entropy coefficient based on difficulty level and deviation\n            if difficulty in ['easy', 'medium']:\n                # For easy and medium questions: increase penalty if reasoning is too long\n                if deviation > 0:  # Actual > target (too long)\n                    # Increase penalty (decrease alpha for entropy bonus)\n                    self.alpha_entropy_dict[difficulty] -= learning_rate * deviation / target_hwe\n                else:\n                    # Decrease penalty (increase alpha for entropy bonus)\n                    self.alpha_entropy_dict[difficulty] += learning_rate * abs(deviation) / target_hwe\n            else:  # hard difficulty\n                # For hard questions: increase encouragement when reasoning is insufficient\n                if deviation < 0:  # Actual < target (too short)\n                    # Increase encouragement (increase alpha for entropy bonus)\n                    self.alpha_entropy_dict[difficulty] += learning_rate * abs(deviation) / target_hwe\n                else:\n                    # Decrease encouragement (decrease alpha for entropy bonus)\n                    self.alpha_entropy_dict[difficulty] -= learning_rate * deviation / target_hwe\n            \n            # Clamp alpha values to reasonable range [0.1, 10.0]\n            self.alpha_entropy_dict[difficulty] = max(0.1, min(10.0, self.alpha_entropy_dict[difficulty]))\n    \n    # Optional: Log statistics for monitoring\n    if hasattr(self, 'logger'):\n        self.logger.info(f\"Difficulty distribution: \"\n                        f\"Easy={len(difficulty_stats['easy']['hwe_counts'])}, \"\n                        f\"Medium={len(difficulty_stats['medium']['hwe_counts'])}, \"\n                        f\"Hard={len(difficulty_stats['hard']['hwe_counts'])}\")\n        self.logger.info(f\"Target HWE tokens: {self.target_high_entropy_token_num_dict}\")\n        self.logger.info(f\"Entropy coefficients: {self.alpha_entropy_dict}\")\n        self.logger.info(f\"Skipped samples: {len(self.skip_gid_set)}\")\n```\n\nThis implementation:\n\n1. **Processes each sample** in the batch to determine its difficulty based on accuracy thresholds\n2. **Updates difficulty dictionary** (`self.difficulty_dict_train`) with the classification\n3. **Collects statistics** for each difficulty level (HWE token counts, accuracies, entropies)\n4. **Updates target HWE token counts** using median values for robustness\n5. **Adjusts entropy coefficients** based on deviation from target:\n   - For easy/medium questions: increases penalty when reasoning is too long\n   - For hard questions: increases encouragement when reasoning is insufficient\n6. **Optionally adds samples** to skip set if they're all correct with extremely low entropy\n7. **Clamps entropy coefficients** to a reasonable range [0.1, 10.0]\n8. **Includes logging** for monitoring training progress\n\nThe function follows gradient ascent principles for coefficient adjustment and implements the adaptive resource budgeting mechanism described in the requirements."], "usage": {"prompt_tokens": 675, "completion_tokens": 1712, "total_tokens": 2387, "per_completion": [{"prompt_tokens": 675, "completion_tokens": 1712, "total_tokens": 2387}]}}
{"function_name": "TokenLevelAdaptiveKLController.update", "overview": "Implements update logic for token-level adaptive KL controller, dynamically adjusting KL coefficients using dual ascent mechanism based on deviation between actual KL divergence and target for each difficulty bucket.", "function_signature": "def update(self, current_kls: Dict[str, float], n_steps: Dict[str, int]):", "input_parameters": "- `current_kls`: Average KL divergence for each difficulty bucket at current step, dictionary format `{difficulty: kl_value}`\n- Example: `{\"easy\": 0.06, \"medium\": 0.12, \"hard\": 0.18}`\n- `n_steps`: Sample count for each difficulty bucket (used for weighting, may be simplified in actual implementation)", "detailed_description": "Token-level adaptive KL control allows using different KL budgets for different difficulty tasks, automatically adjusting KL divergence coefficients for each difficulty bucket through dual ascent mechanism. When actual KL divergence exceeds target value, increase penalty coefficient; when below target value, decrease coefficient.", "output": "- No direct return value, but updates class attributes:\n- `self.kl_coefs`: Updated KL coefficient dictionary `{difficulty: beta}`\n- `self.lambdas`: Updated Lagrange multiplier dictionary `{difficulty: lambda}`", "func_implementation": "", "test_code_path": "code/test_code/test_kl_controller.py", "implementation_location": "code/verl/trainer/core_algos.py:line 77-86", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `TokenLevelAdaptiveKLController.update`.\n\n**Function Overview:**\nImplements update logic for token-level adaptive KL controller, dynamically adjusting KL coefficients using dual ascent mechanism based on deviation between actual KL divergence and target for each difficulty bucket.\n\n**Function Signature:**\n```python\ndef update(self, current_kls: Dict[str, float], n_steps: Dict[str, int]):\n```\n\n**Input Parameters:**\n- `current_kls`: Average KL divergence for each difficulty bucket at current step, dictionary format `{difficulty: kl_value}`\n- Example: `{\"easy\": 0.06, \"medium\": 0.12, \"hard\": 0.18}`\n- `n_steps`: Sample count for each difficulty bucket (used for weighting, may be simplified in actual implementation)\n\n**Detailed Description:**\nToken-level adaptive KL control allows using different KL budgets for different difficulty tasks, automatically adjusting KL divergence coefficients for each difficulty bucket through dual ascent mechanism. When actual KL divergence exceeds target value, increase penalty coefficient; when below target value, decrease coefficient.\n\n**Expected Output:**\n- No direct return value, but updates class attributes:\n- `self.kl_coefs`: Updated KL coefficient dictionary `{difficulty: beta}`\n- `self.lambdas`: Updated Lagrange multiplier dictionary `{difficulty: lambda}`\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\ndef update(self, current_kls: Dict[str, float], n_steps: Dict[str, int]):\n    \"\"\"\n    Update token-level adaptive KL controller using dual ascent mechanism.\n    \n    For each difficulty bucket, adjust the Lagrange multiplier (lambda) based on the\n    deviation between actual KL divergence and target KL value. Then update the\n    KL coefficient (beta) accordingly.\n    \n    The update follows the dual ascent formula:\n        lambda_{t+1} = max(0, lambda_t + learning_rate * (current_kl - target_kl))\n        beta = base_beta * (1 + lambda)\n    \n    Args:\n        current_kls: Dictionary mapping difficulty levels to their average KL divergence\n                     at the current step. Example: {\"easy\": 0.06, \"medium\": 0.12, \"hard\": 0.18}\n        n_steps: Dictionary mapping difficulty levels to sample counts (used for weighting).\n                 Example: {\"easy\": 100, \"medium\": 200, \"hard\": 150}\n    \"\"\"\n    # Iterate through all difficulty buckets\n    for difficulty in self.difficulty_levels:\n        if difficulty not in current_kls:\n            # Skip if no KL data for this difficulty level\n            continue\n            \n        # Get current KL divergence and target KL for this difficulty\n        current_kl = current_kls[difficulty]\n        target_kl = self.target_kls[difficulty]\n        \n        # Get current Lagrange multiplier for this difficulty\n        current_lambda = self.lambdas[difficulty]\n        \n        # Calculate KL deviation from target\n        kl_deviation = current_kl - target_kl\n        \n        # Update Lagrange multiplier using dual ascent\n        # lambda_{t+1} = max(0, lambda_t + learning_rate * (current_kl - target_kl))\n        new_lambda = current_lambda + self.learning_rate * kl_deviation\n        new_lambda = max(0.0, new_lambda)  # Ensure non-negative\n        \n        # Update KL coefficient: beta = base_beta * (1 + lambda)\n        new_beta = self.base_beta * (1.0 + new_lambda)\n        \n        # Apply bounds to ensure numerical stability\n        new_beta = max(self.min_beta, min(self.max_beta, new_beta))\n        \n        # Store updated values\n        self.lambdas[difficulty] = new_lambda\n        self.kl_coefs[difficulty] = new_beta\n        \n        # Optional: Log the update for monitoring\n        if self.verbose:\n            print(f\"Difficulty '{difficulty}': \"\n                  f\"KL={current_kl:.4f}, target={target_kl:.4f}, \"\n                  f\"lambda={new_lambda:.4f}, beta={new_beta:.4f}\")\n```"], "usage": {"prompt_tokens": 462, "completion_tokens": 605, "total_tokens": 1067, "per_completion": [{"prompt_tokens": 462, "completion_tokens": 605, "total_tokens": 1067}]}}
{"function_name": "apply_kl_penalty", "overview": "Applies KL divergence penalty to token-level rewards and updates KL controller.", "function_signature": "def apply_kl_penalty(data: DataProto, kl_ctrl: KLController, kl_penalty=\"kl\"):", "input_parameters": "- `data`: DataProto object containing:\n- `token_level_scores`: Original token-level rewards [batch_size, response_length]\n- `old_log_probs`: Actor old policy log probabilities [batch_size, response_length]\n- `ref_log_probs`: Ref policy log probabilities [batch_size, response_length]\n- `response_mask`: Response mask [batch_size, response_length]\n- `kl_ctrl`: KL controller (FixedKLController or AdaptiveKLController)\n- `kl_penalty`: KL penalty type (\"kl\", \"abs\", \"mse\", \"low_var_kl\", \"full\")", "detailed_description": "This function implements computation and application of token-level KL penalty. It computes token-level KL divergence based on `kl_penalty` type, then subtracts KL penalty term from original rewards to obtain token-level rewards, and computes sequence-level KL divergence and batch-level KL divergence.\nThe function also needs to implement KL controller update.", "output": "- `data`: Updated DataProto where `data.batch[\"token_level_rewards\"]` has KL penalty applied\n- `metrics`: Metrics dictionary `{\"critic/kl\": current_kl, \"critic/kl_coef\": kl_ctrl.kl_coef}`", "func_implementation": "", "test_code_path": "code/test_code/test_apply_kl_penalty.py", "implementation_location": "code/verl/trainer/ray_trainer.py:line 107-124", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `apply_kl_penalty`.\n\n**Function Overview:**\nApplies KL divergence penalty to token-level rewards and updates KL controller.\n\n**Function Signature:**\n```python\ndef apply_kl_penalty(data: DataProto, kl_ctrl: KLController, kl_penalty=\"kl\"):\n```\n\n**Input Parameters:**\n- `data`: DataProto object containing:\n- `token_level_scores`: Original token-level rewards [batch_size, response_length]\n- `old_log_probs`: Actor old policy log probabilities [batch_size, response_length]\n- `ref_log_probs`: Ref policy log probabilities [batch_size, response_length]\n- `response_mask`: Response mask [batch_size, response_length]\n- `kl_ctrl`: KL controller (FixedKLController or AdaptiveKLController)\n- `kl_penalty`: KL penalty type (\"kl\", \"abs\", \"mse\", \"low_var_kl\", \"full\")\n\n**Detailed Description:**\nThis function implements computation and application of token-level KL penalty. It computes token-level KL divergence based on `kl_penalty` type, then subtracts KL penalty term from original rewards to obtain token-level rewards, and computes sequence-level KL divergence and batch-level KL divergence.\nThe function also needs to implement KL controller update.\n\n**Expected Output:**\n- `data`: Updated DataProto where `data.batch[\"token_level_rewards\"]` has KL penalty applied\n- `metrics`: Metrics dictionary `{\"critic/kl\": current_kl, \"critic/kl_coef\": kl_ctrl.kl_coef}`\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport torch\nimport torch.nn.functional as F\nfrom typing import Dict, Tuple\n\ndef apply_kl_penalty(data, kl_ctrl, kl_penalty=\"kl\"):\n    \"\"\"\n    Applies KL divergence penalty to token-level rewards and updates KL controller.\n    \n    Args:\n        data: DataProto object containing:\n            - token_level_scores: Original token-level rewards [batch_size, response_length]\n            - old_log_probs: Actor old policy log probabilities [batch_size, response_length]\n            - ref_log_probs: Ref policy log probabilities [batch_size, response_length]\n            - response_mask: Response mask [batch_size, response_length]\n        kl_ctrl: KL controller (FixedKLController or AdaptiveKLController)\n        kl_penalty: KL penalty type (\"kl\", \"abs\", \"mse\", \"low_var_kl\", \"full\")\n    \n    Returns:\n        data: Updated DataProto with KL penalty applied to token_level_rewards\n        metrics: Dictionary containing KL-related metrics\n    \"\"\"\n    # Extract tensors from data\n    token_level_scores = data.token_level_scores\n    old_log_probs = data.old_log_probs\n    ref_log_probs = data.ref_log_probs\n    response_mask = data.response_mask\n    \n    batch_size, response_length = token_level_scores.shape\n    \n    # Compute token-level KL divergence based on penalty type\n    if kl_penalty == \"kl\":\n        # Standard KL divergence: KL(policy || reference)\n        kl_div = old_log_probs - ref_log_probs\n        kl_penalty_term = kl_ctrl.kl_coef * kl_div\n        \n    elif kl_penalty == \"abs\":\n        # Absolute difference between log probabilities\n        kl_div = torch.abs(old_log_probs - ref_log_probs)\n        kl_penalty_term = kl_ctrl.kl_coef * kl_div\n        \n    elif kl_penalty == \"mse\":\n        # Mean squared error between log probabilities\n        kl_div = F.mse_loss(old_log_probs, ref_log_probs, reduction='none')\n        kl_penalty_term = kl_ctrl.kl_coef * kl_div\n        \n    elif kl_penalty == \"low_var_kl\":\n        # Low variance KL estimator: exp(policy_logprob - ref_logprob) - 1\n        log_ratio = old_log_probs - ref_log_probs\n        kl_div = torch.exp(log_ratio) - 1 - log_ratio\n        kl_penalty_term = kl_ctrl.kl_coef * kl_div\n        \n    elif kl_penalty == \"full\":\n        # Full KL divergence with both directions\n        kl_forward = old_log_probs - ref_log_probs\n        kl_reverse = ref_log_probs - old_log_probs\n        kl_div = 0.5 * (kl_forward + kl_reverse)\n        kl_penalty_term = kl_ctrl.kl_coef * kl_div\n        \n    else:\n        raise ValueError(f\"Unknown kl_penalty type: {kl_penalty}\")\n    \n    # Apply KL penalty to original rewards\n    # Note: We subtract the penalty since we want to penalize high KL divergence\n    token_level_rewards = token_level_scores - kl_penalty_term\n    \n    # Apply response mask to ignore padding tokens\n    kl_div_masked = kl_div * response_mask\n    token_level_rewards_masked = token_level_rewards * response_mask\n    \n    # Compute sequence-level KL divergence (average per sequence)\n    # Sum over tokens, then average over sequences\n    kl_seq = torch.sum(kl_div_masked, dim=1) / torch.sum(response_mask, dim=1).clamp(min=1)\n    current_kl = torch.mean(kl_seq).item()\n    \n    # Update KL controller with the computed KL divergence\n    kl_ctrl.update(current_kl, batch_size)\n    \n    # Store the updated token-level rewards in data\n    data.batch[\"token_level_rewards\"] = token_level_rewards_masked\n    \n    # Prepare metrics\n    metrics = {\n        \"critic/kl\": current_kl,\n        \"critic/kl_coef\": kl_ctrl.kl_coef\n    }\n    \n    return data, metrics\n```"], "usage": {"prompt_tokens": 510, "completion_tokens": 901, "total_tokens": 1411, "per_completion": [{"prompt_tokens": 510, "completion_tokens": 901, "total_tokens": 1411}]}}
