{"function_name": "DataParallelPRIMERewardModel._forward_micro_batch", "overview": "Computes token-level reward scores for implicit process reward model", "function_signature": "def _forward_micro_batch(self, micro_batch, prompt_length):", "input_parameters": "- `micro_batch`: Micro-batch data {input_ids, attention_mask, position_ids, acc}\n- `prompt_length`: Prompt sequence length", "detailed_description": "First selects computation path based on whether padding removal optimization is enabled. For reward model and reference model, performs forward pass to compute output logits, then extracts log probabilities for corresponding tokens.\nImplicit rewards are obtained by computing log probability differences between the two models on response sequences.\nProcess reward generation supports two modes: direct mode and GAE mode.\nIn token granularity, each position receives corresponding reward value; in whole granularity, all rewards accumulate to the last valid token position.", "output": "- `token_level_scores`: Token-level reward scores (torch.Tensor, shape: [batch_size, seq_len])\n- `q_values`: Raw log probability differences (torch.Tensor, shape: [batch_size, seq_len])", "func_implementation": "", "test_code_path": "code/test_code/test_forward_micro_batch.py", "implementation_location": "code/recipe/prime/prime_dp_rm.py:line 51-228", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `DataParallelPRIMERewardModel._forward_micro_batch`.\n\n**Function Overview:**\nComputes token-level reward scores for implicit process reward model\n\n**Function Signature:**\n```python\ndef _forward_micro_batch(self, micro_batch, prompt_length):\n```\n\n**Input Parameters:**\n- `micro_batch`: Micro-batch data {input_ids, attention_mask, position_ids, acc}\n- `prompt_length`: Prompt sequence length\n\n**Detailed Description:**\nFirst selects computation path based on whether padding removal optimization is enabled. For reward model and reference model, performs forward pass to compute output logits, then extracts log probabilities for corresponding tokens.\nImplicit rewards are obtained by computing log probability differences between the two models on response sequences.\nProcess reward generation supports two modes: direct mode and GAE mode.\nIn token granularity, each position receives corresponding reward value; in whole granularity, all rewards accumulate to the last valid token position.\n\n**Expected Output:**\n- `token_level_scores`: Token-level reward scores (torch.Tensor, shape: [batch_size, seq_len])\n- `q_values`: Raw log probability differences (torch.Tensor, shape: [batch_size, seq_len])\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def _forward_micro_batch(self, micro_batch, prompt_length):\n    \"\"\"\n    Compute token-level reward scores for implicit process reward model.\n\n    Args:\n        micro_batch: dict with keys:\n            - input_ids: (batch, seq_len) LongTensor\n            - attention_mask: (batch, seq_len) LongTensor or BoolTensor\n            - position_ids: optional (batch, seq_len) LongTensor\n            - acc: optional, used by padding-removal path (ignored if not present)\n        prompt_length: int, index where response tokens start\n\n    Returns:\n        token_level_scores: (batch, seq_len) FloatTensor\n        q_values: (batch, seq_len) FloatTensor  # raw log-prob differences (reward - ref)\n    \"\"\"\n    import torch\n    import torch.nn.functional as F\n\n    input_ids = micro_batch[\"input_ids\"]\n    attention_mask = micro_batch[\"attention_mask\"]\n    position_ids = micro_batch.get(\"position_ids\", None)\n\n    batch_size, seq_len = input_ids.shape\n    device = input_ids.device\n    dtype = next(self.reward_model.parameters()).dtype if hasattr(self, \"reward_model\") else input_ids.dtype\n\n    # Helper to safely call model and extract logits\n    def _get_logits(model, input_ids_, attention_mask_, position_ids_):\n        # Try common call signatures, return logits tensor shaped (B, L, V)\n        try:\n            out = model(input_ids=input_ids_, attention_mask=attention_mask_, position_ids=position_ids_)\n        except TypeError:\n            try:\n                out = model(input_ids_, attention_mask_, position_ids_)\n            except TypeError:\n                out = model(input_ids_)\n        # Handle outputs that are tuple (logits, ...)\n        if isinstance(out, tuple) or isinstance(out, list):\n            logits = out[0]\n        else:\n            logits = out\n        return logits\n\n    # If padding removal is enabled and micro_batch contains unpadded inputs,\n    # try to use them. Otherwise, use full padded inputs.\n    use_pad_removal = getattr(self, \"use_padding_removal\", False)\n    if use_pad_removal and \"unpadded_input_ids\" in micro_batch:\n        # Unpadded path (if provided by caller)\n        unpadded_input_ids = micro_batch[\"unpadded_input_ids\"]\n        unpadded_attention_mask = micro_batch.get(\"unpadded_attention_mask\", None)\n        unpadded_pos_ids = micro_batch.get(\"unpadded_position_ids\", None)\n\n        logits_reward = _get_logits(self.reward_model, unpadded_input_ids, unpadded_attention_mask, unpadded_pos_ids)\n        logits_ref = _get_logits(self.ref_model, unpadded_input_ids, unpadded_attention_mask, unpadded_pos_ids)\n\n        # If the unpadded representation needs to be mapped back to padded space,\n        # assume caller provided a mapping (indices) named \"unpad_to_pad_index\".\n        # If not provided, broadcast fallback will be used (which may be wrong).\n        if \"unpad_to_pad_index\" in micro_batch:\n            idx = micro_batch[\"unpad_to_pad_index\"]  # expected shape (unpadded_tokens,)\n            # Map logits back: create zero logits and scatter\n            vocab_size = logits_reward.shape[-1]\n            logits_reward_full = torch.zeros((batch_size, seq_len, vocab_size), device=device, dtype=logits_reward.dtype)\n            logits_ref_full = torch.zeros_like(logits_reward_full)\n            # idx is expected to have linearized positions; scatter accordingly\n            logits_reward_full.view(-1, vocab_size).index_copy_(0, idx, logits_reward.view(-1, vocab_size))\n            logits_ref_full.view(-1, vocab_size).index_copy_(0, idx, logits_ref.view(-1, vocab_size))\n            logits_reward = logits_reward_full\n            logits_ref = logits_ref_full\n        else:\n            # Fallback: This is best-effort; use padded inputs instead\n            logits_reward = _get_logits(self.reward_model, input_ids, attention_mask, position_ids)\n            logits_ref = _get_logits(self.ref_model, input_ids, attention_mask, position_ids)\n    else:\n        # Standard padded forward\n        logits_reward = _get_logits(self.reward_model, input_ids, attention_mask, position_ids)\n        logits_ref = _get_logits(self.ref_model, input_ids, attention_mask, position_ids)\n\n    # Compute log probabilities and extract next-token log-probs.\n    # Typical LM: logits are for predicting token t given tokens up to t-1.\n    # So we take logits[:, :-1, :] and targets = input_ids[:, 1:].\n    # We'll produce q_values aligned with input_ids positions: set index 0 to 0.\n    # logp shape will be (batch, seq_len) with 0 at position 0.\n    # Note: if logits length differs from input_ids length, try to align by min length.\n    L_logits = logits_reward.shape[1]\n    L_targets = input_ids.shape[1]\n    L_common = min(L_logits - 1, L_targets - 1)  # number of valid next-token positions\n\n    # Initialize full tensors\n    q_values = torch.zeros((batch_size, seq_len), device=device, dtype=logits_reward.dtype)\n\n    if L_common > 0:\n        # Compute log softmax over vocab\n        logprob_reward = F.log_softmax(logits_reward, dim=-1)[:, : (1 + L_common), :]  # ensure enough length\n        logprob_ref = F.log_softmax(logits_ref, dim=-1)[:, : (1 + L_common), :]\n\n        # Targets: tokens at positions 1 .. 1+L_common\n        targets = input_ids[:, 1 : 1 + L_common]  # shape (B, L_common)\n\n        # Gather logprobs for targets using logits at positions 0..L_common-1\n        lp_r = torch.gather(logprob_reward[:, :L_common, :], dim=-1, index=targets.unsqueeze(-1)).squeeze(-1)\n        lp_ref = torch.gather(logprob_ref[:, :L_common, :], dim=-1, index=targets.unsqueeze(-1)).squeeze(-1)\n\n        # Place into q_values aligned to target positions (pos 1..1+L_common-1)\n        q_values[:, 1 : 1 + L_common] = lp_r - lp_ref\n\n    # Mask out positions that are padding (including position 0 if masked)\n    attn_bool = attention_mask.bool()\n    q_values = q_values * attn_bool.to(q_values.dtype)\n\n    # Zero out positions that are part of prompt (we only reward response tokens)\n    if prompt_length > 0:\n        q_values[:, :prompt_length] = 0.0\n\n    # Now derive token_level_scores based on process_reward_mode and reward_granularity\n    process_mode = getattr(self, \"process_reward_mode\", \"direct\")  # 'direct' or 'gae'\n    granularity = getattr(self, \"reward_granularity\", \"token\")  # 'token' or 'whole'\n\n    token_level_scores = torch.zeros_like(q_values)\n\n    # Helper to find last valid token index for each batch element\n    # last_valid_idx is index of last token where attention_mask==1\n    # If entire row is masked, set -1\n    attn_sum = attn_bool.long().sum(dim=1)\n    # last_valid_idx = attn_sum - 1\n    last_valid_idx = (attn_sum - 1).clamp(min=-1)\n\n    if process_mode == \"direct\":\n        if granularity == \"token\":\n            # Each response token gets its q_value\n            token_level_scores = q_values.clone()\n        else:\n            # Whole granularity: accumulate all q_values on last valid token within response\n            # For each sample, sum q_values over response tokens (prompt_length .. last_valid) and place at last_valid\n            for i in range(batch_size):\n                last_idx = int(last_valid_idx[i].item())\n                if last_idx >= prompt_length:\n                    total = q_values[i, prompt_length : last_idx + 1].sum()\n                    token_level_scores[i, last_idx] = total\n                else:\n                    # No response tokens for this sample -> leave zeros\n                    continue\n    elif process_mode == \"gae\":\n        # GAE-style exponentially-weighted future sum:\n        # A_t = sum_{k=0}^{T-t-1} (gamma * lam)^k * r_{t+k}\n        gamma = getattr(self, \"gamma\", 1.0)\n        lam = getattr(self, \"gae_lambda\", 1.0)\n        discount = float(gamma) * float(lam)\n        # Precompute powers if desired; do per-sample\n        for i in range(batch_size):\n            last_idx = int(last_valid_idx[i].item())\n            if last_idx < prompt_length:\n                continue  # no response tokens\n            # extract rewards vector for response positions\n            r = q_values[i, prompt_length : last_idx + 1]  # shape (T,)\n            T = r.shape[0]\n            if T == 0:\n                continue\n            if granularity == \"token\":\n                # compute A_t for each t in 0..T-1\n                # vectorized: for small T we can do simple loop; T is sequence length per example\n                # We'll compute from back: A_{T-1} = r_{T-1}; A_t = r_t + discount * A_{t+1}\n                A = torch.zeros_like(r)\n                acc_val = torch.tensor(0.0, device=device, dtype=r.dtype)\n                for t in range(T - 1, -1, -1):\n                    acc_val = r[t] + discount * acc_val\n                    A[t] = acc_val\n                token_level_scores[i, prompt_length : last_idx + 1] = A\n            else:\n                # whole granularity: compute total discounted accumulation and place at last_idx\n                acc_val = torch.tensor(0.0, device=device, dtype=r.dtype)\n                for t in range(T - 1, -1, -1):\n                    acc_val = r[t] + discount * acc_val\n                token_level_scores[i, last_idx] = acc_val\n    else:\n        # Unknown mode: fallback to direct token-level q_values\n        token_level_scores = q_values.clone()\n\n    # Ensure token_level_scores and q_values have padding positions zeroed\n    token_level_scores = token_level_scores * attn_bool.to(token_level_scores.dtype)\n\n    return token_level_scores, q_values"], "usage": {"prompt_tokens": 427, "completion_tokens": 3824, "total_tokens": 4251, "per_completion": [{"prompt_tokens": 427, "completion_tokens": 3824, "total_tokens": 4251}]}, "results": [false], "pass_ratios": [0.0]}
{"function_name": "compute_ce_dpo_loss_rm", "overview": "Computes cross-entropy style DPO loss", "function_signature": "def compute_ce_dpo_loss_rm(token_level_scores, acc, response_mask, beta):", "input_parameters": "- `token_level_scores`: Token-level rewards\n- `acc`: Binary accuracy labels\n- `response_mask`: Response mask\n- `beta`: Temperature parameter", "detailed_description": "Computes sequence-level scores, converts them to probabilities, and finally computes cross-entropy loss.", "output": "Scalar loss value", "func_implementation": "", "test_code_path": "code/test_code/test_ce_dpo_loss.py", "implementation_location": "code/recipe/prime/prime_core_algos.py:line 82-85", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_ce_dpo_loss_rm`.\n\n**Function Overview:**\nComputes cross-entropy style DPO loss\n\n**Function Signature:**\n```python\ndef compute_ce_dpo_loss_rm(token_level_scores, acc, response_mask, beta):\n```\n\n**Input Parameters:**\n- `token_level_scores`: Token-level rewards\n- `acc`: Binary accuracy labels\n- `response_mask`: Response mask\n- `beta`: Temperature parameter\n\n**Detailed Description:**\nComputes sequence-level scores, converts them to probabilities, and finally computes cross-entropy loss.\n\n**Expected Output:**\nScalar loss value\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def compute_ce_dpo_loss_rm(token_level_scores, acc, response_mask, beta):\n    \"\"\"\n    Compute a cross-entropy style DPO loss from token-level rewards.\n\n    Args:\n        token_level_scores: Tensor of shape (batch, seq_len) containing token-level rewards/scores.\n        acc: Tensor of shape (batch,) containing binary accuracy/preference labels (0 or 1).\n        response_mask: Tensor of shape (batch, seq_len) indicating which tokens belong to the response\n                       (can be bool or 0/1 floats).\n        beta: scalar temperature (float or 0-dim tensor) that scales sequence scores before sigmoid.\n\n    Returns:\n        A scalar tensor representing the mean cross-entropy loss across the batch.\n    \"\"\"\n    import torch\n    # Ensure mask is float for multiplication\n    mask = response_mask.float()\n\n    # Sequence-level score: sum of token-level scores over the response tokens\n    # (if you prefer average per-token score, divide by mask.sum(dim=1) after handling zeros)\n    seq_scores = (token_level_scores * mask).sum(dim=1)\n\n    # Scale by temperature to get logits for a binary preference model\n    logits = beta * seq_scores\n\n    # Convert logits to probabilities with sigmoid (numerically stable BCE will be computed)\n    probs = torch.sigmoid(logits)\n\n    # Ensure targets are float\n    targets = acc.float()\n\n    # Compute binary cross-entropy manually in a numerically stable way:\n    # Use small epsilon to avoid log(0) (sigmoid already avoids exact 0/1 in practice)\n    eps = 1e-12\n    probs = probs.clamp(eps, 1.0 - eps)\n    loss_per_example = -(targets * torch.log(probs) + (1.0 - targets) * torch.log(1.0 - probs))\n\n    # Return mean loss over batch\n    return loss_per_example.mean()"], "usage": {"prompt_tokens": 313, "completion_tokens": 1313, "total_tokens": 1626, "per_completion": [{"prompt_tokens": 313, "completion_tokens": 1313, "total_tokens": 1626}]}, "results": [false], "pass_ratios": [0.4444444444444444]}
{"function_name": "compute_detach_dpo_loss_rm", "overview": "Computes detached DPO loss, supporting BoN (Best-of-N) mode", "function_signature": "def compute_detach_dpo_loss_rm(token_level_scores, acc, Q_bc, acc_bc, response_mask, beta, bon_mode=\"none\"):", "input_parameters": "- Base: `token_level_scores`, `acc`, `response_mask`, `beta`\n- Contrast: `Q_bc` (batch Q-value matrix), `acc_bc` (batch accuracy matrix)\n- `bon_mode`: Weight mode {\"none\", \"bon_rm\", \"bon_acc\"}", "detailed_description": "Computes detached DPO loss based on contrastive samples, selecting samples with opposite accuracy labels within batch as contrastive baseline, and supports assigning higher training weights to high-quality samples through BoN weight mechanism.\nBoN weights include three modes: none mode, bon_rm mode, and bon_acc mode.", "output": "Weighted DPO loss value", "func_implementation": "", "test_code_path": "code/test_code/test_detach_dpo_loss.py", "implementation_location": "code/recipe/prime/prime_core_algos.py:line 88-116", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_detach_dpo_loss_rm`.\n\n**Function Overview:**\nComputes detached DPO loss, supporting BoN (Best-of-N) mode\n\n**Function Signature:**\n```python\ndef compute_detach_dpo_loss_rm(token_level_scores, acc, Q_bc, acc_bc, response_mask, beta, bon_mode=\"none\"):\n```\n\n**Input Parameters:**\n- Base: `token_level_scores`, `acc`, `response_mask`, `beta`\n- Contrast: `Q_bc` (batch Q-value matrix), `acc_bc` (batch accuracy matrix)\n- `bon_mode`: Weight mode {\"none\", \"bon_rm\", \"bon_acc\"}\n\n**Detailed Description:**\nComputes detached DPO loss based on contrastive samples, selecting samples with opposite accuracy labels within batch as contrastive baseline, and supports assigning higher training weights to high-quality samples through BoN weight mechanism.\nBoN weights include three modes: none mode, bon_rm mode, and bon_acc mode.\n\n**Expected Output:**\nWeighted DPO loss value\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport torch\nimport torch.nn.functional as F\n\ndef compute_detach_dpo_loss_rm(token_level_scores, acc, Q_bc, acc_bc, response_mask, beta, bon_mode=\"none\"):\n    \"\"\"\n    Compute detached DPO loss with optional Best-of-N (BoN) weighting.\n\n    Args:\n        token_level_scores: Tensor [B, L] per-token (log-)scores for each sample.\n        acc: Tensor [B] binary accuracy labels (0/1) for each sample.\n        Q_bc: Tensor [B, B] batch Q-value matrix used to pick contrastive baselines.\n        acc_bc: Tensor [B, B] batch accuracy matrix where acc_bc[i, j] is acc[j] (or equivalent).\n        response_mask: Tensor [B, L] mask (0/1 or bool) that selects response tokens to sum.\n        beta: scalar float temperature/coefficient for DPO.\n        bon_mode: one of {\"none\", \"bon_rm\", \"bon_acc\"} for BoN weighting.\n\n    Returns:\n        Scalar tensor: the weighted DPO loss (averaged by the sum of weights).\n    \"\"\"\n    # Ensure tensors are floats where needed\n    device = token_level_scores.device\n    B = token_level_scores.shape[0]\n\n    # Compute response-level scores by summing token-level scores over the response mask\n    # response_mask may be bool or float; cast to float\n    response_mask_f = response_mask.to(token_level_scores.dtype)\n    # Sum along token dimension (assume dim=1 is sequence length)\n    response_scores = (token_level_scores * response_mask_f).sum(dim=1)  # [B]\n\n    # Ensure acc is a 1-D tensor of ints/bool\n    acc_vec = acc.view(-1)\n    # Build opposite-accuracy mask per row: True where candidate j has different accuracy than sample i\n    # Use provided acc_bc directly (expected shape [B,B])\n    opposite_mask = (acc_bc != acc_vec.unsqueeze(1))  # [B, B], bool\n\n    # If no opposite exists for a row, we will ignore that row (weight=0)\n    valid_rows = opposite_mask.any(dim=1)  # [B], bool\n\n    # For selecting the best baseline per row, mask non-opposite entries with a very small value\n    # so they won't be selected by argmax. Use -inf-like value depending on dtype.\n    q_dtype = Q_bc.dtype\n    if q_dtype.is_floating_point:\n        neg_inf = torch.finfo(q_dtype).min / 2.0\n    else:\n        # fallback\n        neg_inf = -1e9\n    Q_masked = Q_bc.clone()\n    Q_masked[~opposite_mask] = neg_inf\n\n    # argmax per row gives selected baseline index j* (for invalid rows this is arbitrary)\n    selected_j = Q_masked.argmax(dim=1)  # [B] (long)\n\n    # Gather baseline response scores and detach them to prevent gradient flow into baselines\n    baseline_scores = response_scores[selected_j].detach()  # [B]\n\n    # Compute signed score difference such that it's always (s_preferred - s_unpreferred)\n    # If acc[i]==1 -> the sample i is preferred over opposite candidate; sign = +1\n    # If acc[i]==0 -> opposite candidate is preferred; sign = -1 so diff = s_j - s_i\n    sign = torch.where(acc_vec.to(response_scores.dtype) >= 0.5, 1.0, -1.0).to(response_scores.dtype)  # [B]\n    diff = sign * (response_scores - baseline_scores)  # [B]\n\n    # Compute DPO loss per sample: -log sigmoid(beta * diff)\n    logits = beta * diff\n    loss_per_sample = -F.logsigmoid(logits)  # [B]\n\n    # Zero out losses for invalid rows (no opposite baseline)\n    loss_per_sample = loss_per_sample * valid_rows.to(loss_per_sample.dtype)\n\n    # Compute BoN weights\n    weights = torch.ones_like(loss_per_sample, dtype=loss_per_sample.dtype, device=device)\n\n    if bon_mode == \"none\":\n        # uniform weights but zero out invalid rows\n        weights = weights * valid_rows.to(weights.dtype)\n\n    elif bon_mode == \"bon_rm\":\n        # Reward-model based weights:\n        # estimate a per-sample quality q_j by averaging Q over rows for each candidate j,\n        # then assign weight for row i as q_j_selected (the selected candidate's quality).\n        # This gives higher weight to rows whose selected baseline is of higher Q (better quality).\n        # Make the weights positive and normalize to mean 1 over valid rows.\n        # Compute candidate quality as mean over rows\n        q_per_candidate = Q_bc.mean(dim=0)  # [B]\n        raw_w = q_per_candidate[selected_j]  # [B]\n        # For invalid rows set 0\n        raw_w = raw_w * valid_rows.to(raw_w.dtype)\n        if raw_w.numel() == 0 or raw_w[valid_rows].numel() == 0:\n            # fallback to uniform weights if nothing valid\n            weights = valid_rows.to(weights.dtype)\n        else:\n            # shift to positive\n            min_val = raw_w[valid_rows].min()\n            shifted = raw_w - min_val + 1e-6\n            shifted = shifted * valid_rows.to(shifted.dtype)\n            # normalize to mean 1 among valid rows\n            mean_shifted = shifted[valid_rows].mean()\n            if mean_shifted == 0:\n                weights = valid_rows.to(weights.dtype)\n            else:\n                weights = shifted / mean_shifted\n\n    elif bon_mode == \"bon_acc\":\n        # Accuracy-based weights: give higher weight to high-quality (acc==1) samples.\n        # Use weight = 1 + acc (so acc==1 => 2, acc==0 => 1) then normalize to mean 1 among valid rows.\n        raw_w = (1.0 + acc_vec.to(weights.dtype)) * valid_rows.to(weights.dtype)\n        if raw_w[valid_rows].numel() == 0:\n            weights = valid_rows.to(weights.dtype)\n        else:\n            mean_raw = raw_w[valid_rows].mean()\n            if mean_raw == 0:\n                weights = valid_rows.to(weights.dtype)\n            else:\n                weights = raw_w / mean_raw\n    else:\n        raise ValueError(f\"Unknown bon_mode: {bon_mode}. Expected one of 'none', 'bon_rm', 'bon_acc'.\")\n\n    # Ensure weights are zero for invalid rows (safety)\n    weights = weights * valid_rows.to(weights.dtype)\n\n    # If no valid weights (sum==0), return zero loss\n    denom = weights.sum()\n    if denom == 0:\n        return torch.tensor(0.0, dtype=loss_per_sample.dtype, device=device)\n\n    # Weighted average loss\n    loss = (weights * loss_per_sample).sum() / denom\n\n    return loss\n```"], "usage": {"prompt_tokens": 401, "completion_tokens": 4642, "total_tokens": 5043, "per_completion": [{"prompt_tokens": 401, "completion_tokens": 4642, "total_tokens": 5043}]}, "results": [false], "pass_ratios": [0.0]}
{"function_name": "compute_dpo_accuracy", "overview": "Computes pairwise comparison DPO accuracy", "function_signature": "def compute_dpo_accuracy(token_level_scores, acc, response_mask, n_samples):", "input_parameters": "- `token_level_scores`: Token-level reward scores\n- `acc`: Accuracy labels\n- `response_mask`: Response mask\n- `n_samples`: Sample count per group", "detailed_description": "Uses pairwise comparison to evaluate reward model accuracy. Aggregates token-level scores to sequence-level scores, constructs sample pairs for comparison. Judges directional consistency by computing score differences and accuracy differences.\nUses weighted average to compute accuracy, with higher weights for sample pairs with larger accuracy differences. Returns 0.5 when all samples have the same accuracy.", "output": "Weighted pairwise comparison accuracy", "func_implementation": "", "test_code_path": "code/test_code/test_dpo_accuracy.py", "implementation_location": "code/recipe/prime/prime_core_algos.py:line 119-143", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_dpo_accuracy`.\n\n**Function Overview:**\nComputes pairwise comparison DPO accuracy\n\n**Function Signature:**\n```python\ndef compute_dpo_accuracy(token_level_scores, acc, response_mask, n_samples):\n```\n\n**Input Parameters:**\n- `token_level_scores`: Token-level reward scores\n- `acc`: Accuracy labels\n- `response_mask`: Response mask\n- `n_samples`: Sample count per group\n\n**Detailed Description:**\nUses pairwise comparison to evaluate reward model accuracy. Aggregates token-level scores to sequence-level scores, constructs sample pairs for comparison. Judges directional consistency by computing score differences and accuracy differences.\nUses weighted average to compute accuracy, with higher weights for sample pairs with larger accuracy differences. Returns 0.5 when all samples have the same accuracy.\n\n**Expected Output:**\nWeighted pairwise comparison accuracy\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["import torch\n\ndef compute_dpo_accuracy(token_level_scores, acc, response_mask, n_samples):\n    \"\"\"\n    Compute pairwise comparison DPO accuracy.\n\n    Parameters:\n    - token_level_scores: tensor-like, shape (batch_total, seq_len), token-level reward scores\n    - acc: tensor-like, shape (batch_total,), accuracy labels or scalar per sequence\n    - response_mask: tensor-like, shape (batch_total, seq_len), mask for response tokens (bool or 0/1)\n    - n_samples: int, number of samples per group\n\n    Returns:\n    - float: weighted pairwise comparison accuracy (0.5 if no informative pairs)\n    \"\"\"\n    # Convert inputs to torch tensors (preserve device if already torch)\n    token_level_scores = torch.as_tensor(token_level_scores)\n    acc = torch.as_tensor(acc)\n    response_mask = torch.as_tensor(response_mask)\n\n    # Basic shape checks and flattening\n    if token_level_scores.dim() != 2:\n        raise ValueError(\"token_level_scores must be 2D: (batch_total, seq_len)\")\n    batch_total, seq_len = token_level_scores.shape\n\n    if response_mask.shape != token_level_scores.shape:\n        raise ValueError(\"response_mask must have same shape as token_level_scores\")\n\n    if acc.dim() != 1 or acc.shape[0] != batch_total:\n        raise ValueError(\"acc must be 1D with length equal to batch_total\")\n\n    if n_samples <= 0:\n        raise ValueError(\"n_samples must be > 0\")\n\n    # Number of full groups\n    n_groups = batch_total // n_samples\n    if n_groups == 0:\n        # Not enough samples to form any group\n        return 0.5\n\n    # Possibly drop remainder samples if batch_total is not divisible by n_samples\n    effective_total = n_groups * n_samples\n    if effective_total != batch_total:\n        token_level_scores = token_level_scores[:effective_total]\n        response_mask = response_mask[:effective_total]\n        acc = acc[:effective_total]\n\n    device = token_level_scores.device\n\n    # Aggregate token-level scores to sequence-level by summing over response tokens\n    mask_float = response_mask.to(dtype=token_level_scores.dtype)\n    seq_scores = (token_level_scores * mask_float).sum(dim=1)  # shape (effective_total,)\n\n    # Reshape to (n_groups, n_samples)\n    seq_scores = seq_scores.view(n_groups, n_samples)\n    acc = acc.view(n_groups, n_samples).to(dtype=seq_scores.dtype)\n\n    # If n_samples == 1, no pairs can be formed\n    if n_samples == 1:\n        return 0.5\n\n    # Compute pairwise differences for each group: shape (n_groups, n_samples, n_samples)\n    s1 = seq_scores.unsqueeze(2)  # (G, n, 1)\n    s2 = seq_scores.unsqueeze(1)  # (G, 1, n)\n    score_diff = s1 - s2\n\n    a1 = acc.unsqueeze(2)\n    a2 = acc.unsqueeze(1)\n    acc_diff = a1 - a2\n\n    # Consider only unique unordered pairs i < j (upper triangular, excluding diagonal)\n    pair_mask = torch.triu(torch.ones((n_samples, n_samples), dtype=torch.bool, device=device), diagonal=1)\n    # Select pairs: result shape (n_groups, num_pairs)\n    score_diff_pairs = score_diff[:, pair_mask]\n    acc_diff_pairs = acc_diff[:, pair_mask]\n\n    # Weigh pairs by magnitude of accuracy difference\n    weights = torch.abs(acc_diff_pairs)\n\n    total_weight = weights.sum()\n    if total_weight == 0:\n        # No informative pairs (all acc equal within groups)\n        return 0.5\n\n    # A pair is correct if score_diff and acc_diff have the same sign (product > 0)\n    correct_mask = (score_diff_pairs * acc_diff_pairs) > 0\n    weighted_correct = (weights * correct_mask.to(dtype=weights.dtype)).sum()\n\n    accuracy = (weighted_correct / total_weight).item()\n    return accuracy"], "usage": {"prompt_tokens": 363, "completion_tokens": 2116, "total_tokens": 2479, "per_completion": [{"prompt_tokens": 363, "completion_tokens": 2116, "total_tokens": 2479}]}, "results": [false], "pass_ratios": [0.0]}
{"function_name": "compute_dpo_abs_accuracy", "overview": "Computes absolute accuracy metric", "function_signature": "def compute_dpo_abs_accuracy(token_level_scores, acc, response_mask, n_samples):", "input_parameters": "- `token_level_scores`: Token-level reward scores\n- `acc`: Accuracy labels\n- `response_mask`: Response mask", "detailed_description": "Converts aggregated token-level scores to predicted labels, compares with true labels to compute absolute accuracy.", "output": "Absolute classification accuracy", "func_implementation": "", "test_code_path": "code/test_code/test_dpo_abs_accuracy.py", "implementation_location": "code/recipe/prime/prime_core_algos.py:line 146-147", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_dpo_abs_accuracy`.\n\n**Function Overview:**\nComputes absolute accuracy metric\n\n**Function Signature:**\n```python\ndef compute_dpo_abs_accuracy(token_level_scores, acc, response_mask, n_samples):\n```\n\n**Input Parameters:**\n- `token_level_scores`: Token-level reward scores\n- `acc`: Accuracy labels\n- `response_mask`: Response mask\n\n**Detailed Description:**\nConverts aggregated token-level scores to predicted labels, compares with true labels to compute absolute accuracy.\n\n**Expected Output:**\nAbsolute classification accuracy\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def compute_dpo_abs_accuracy(token_level_scores, acc, response_mask, n_samples):\n    \"\"\"\n    Compute absolute classification accuracy from token-level scores.\n\n    The function aggregates token-level scores over the response tokens (using\n    response_mask) to obtain a scalar score per sample, groups samples into\n    groups of size `n_samples` (one group per query / prompt), picks the\n    sample with the highest aggregated score as the prediction, and compares\n    it with the true label(s) in `acc` to compute accuracy.\n\n    Args:\n        token_level_scores: Tensor/ndarray of token-level scores.\n            Supported shapes:\n              - (batch, n_samples, seq_len)\n              - (batch * n_samples, seq_len)\n        acc: True labels. Supported formats:\n              - shape (batch,) with integer indices in [0, n_samples-1]\n              - shape (batch, n_samples) with one-hot or preference scores\n        response_mask: Mask indicating response tokens. Shape must align with\n            token_level_scores token dimension:\n              - if token_level_scores is (batch, n_samples, seq_len): same shape\n              - if token_level_scores is (batch*n_samples, seq_len): same shape\n        n_samples: Number of samples per query (e.g., 2 for pairwise comparisons)\n\n    Returns:\n        accuracy: float in [0, 1] representing absolute classification accuracy\n    \"\"\"\n    # Determine whether inputs are torch tensors or numpy arrays\n    is_torch = False\n    try:\n        import torch\n        is_torch = isinstance(token_level_scores, torch.Tensor)\n    except Exception:\n        is_torch = False\n\n    # Helper functions to convert and do operations in either backend\n    if is_torch:\n        T = torch\n        tl_scores = token_level_scores\n        mask = response_mask\n        labels = acc\n        # Normalize shapes: ensure (batch, n_samples, seq_len)\n        if tl_scores.dim() == 2:\n            total_samples, seq_len = tl_scores.shape\n            if total_samples % n_samples != 0:\n                raise ValueError(\"token_level_scores first dim not divisible by n_samples\")\n            batch = total_samples // n_samples\n            tl_scores = tl_scores.view(batch, n_samples, seq_len)\n            mask = mask.view(batch, n_samples, seq_len)\n        elif tl_scores.dim() == 3:\n            batch, ns, seq_len = tl_scores.shape\n            if ns != n_samples:\n                raise ValueError(f\"token_level_scores has n_samples={ns} but argument n_samples={n_samples}\")\n        else:\n            raise ValueError(\"token_level_scores must be 2D or 3D tensor\")\n\n        # Masked mean aggregation per sample (avoid scale differences due to length)\n        mask_float = mask.to(dtype=tl_scores.dtype)\n        masked_sum = (tl_scores * mask_float).sum(dim=-1)  # (batch, n_samples)\n        denom = mask_float.sum(dim=-1).clamp(min=1.0)     # (batch, n_samples)\n        agg = masked_sum / denom                          # (batch, n_samples)\n\n        # Predicted index per group\n        preds = agg.argmax(dim=1)  # (batch,)\n\n        # Process labels: handle (batch,) integer labels or (batch, n_samples) one-hot/scores\n        if labels.dim() == 1:\n            true_idx = labels.long()\n        elif labels.dim() == 2 and labels.shape[1] == n_samples:\n            true_idx = labels.argmax(dim=1).long()\n        else:\n            raise ValueError(\"acc must be shape (batch,) or (batch, n_samples)\")\n\n        correct = (preds == true_idx).to(dtype=tl_scores.dtype)\n        accuracy = correct.mean().item()\n        return accuracy\n\n    else:\n        # Use numpy\n        import numpy as np\n        tl_scores = np.asarray(token_level_scores)\n        mask = np.asarray(response_mask)\n        labels = np.asarray(acc)\n\n        if tl_scores.ndim == 2:\n            total_samples, seq_len = tl_scores.shape\n            if total_samples % n_samples != 0:\n                raise ValueError(\"token_level_scores first dim not divisible by n_samples\")\n            batch = total_samples // n_samples\n            tl_scores = tl_scores.reshape(batch, n_samples, seq_len)\n            mask = mask.reshape(batch, n_samples, seq_len)\n        elif tl_scores.ndim == 3:\n            batch, ns, seq_len = tl_scores.shape\n            if ns != n_samples:\n                raise ValueError(f\"token_level_scores has n_samples={ns} but argument n_samples={n_samples}\")\n        else:\n            raise ValueError(\"token_level_scores must be 2D or 3D array\")\n\n        mask_float = mask.astype(tl_scores.dtype)\n        masked_sum = (tl_scores * mask_float).sum(axis=-1)  # (batch, n_samples)\n        denom = mask_float.sum(axis=-1)\n        denom = np.where(denom <= 0, 1.0, denom)            # avoid div by zero\n        agg = masked_sum / denom                            # (batch, n_samples)\n\n        preds = agg.argmax(axis=1)\n\n        if labels.ndim == 1:\n            true_idx = labels.astype(int)\n        elif labels.ndim == 2 and labels.shape[1] == n_samples:\n            true_idx = labels.argmax(axis=1).astype(int)\n        else:\n            raise ValueError(\"acc must be shape (batch,) or (batch, n_samples)\")\n\n        correct = (preds == true_idx).astype(float)\n        accuracy = float(correct.mean())\n        return accuracy"], "usage": {"prompt_tokens": 301, "completion_tokens": 2067, "total_tokens": 2368, "per_completion": [{"prompt_tokens": 301, "completion_tokens": 2067, "total_tokens": 2368}]}, "results": [false], "pass_ratios": [0.0]}
{"function_name": "compute_rloo_advantage_return", "overview": "Implements RLOO advantage estimation, supporting multiple reward source fusion", "function_signature": "def compute_rloo_advantage_return(data: verl.DataProto, response_mask: torch.Tensor, n_samples, config):", "input_parameters": "- `data`: Data protocol object containing DataProto.batch fields\n- `data.batch[\"rm_scores\"]`: Implicit reward scores\n- `data.batch[\"acc\"]`: Binary accuracy labels\n- `data.batch[\"prompts\"]`: Input prompt sequences\n- `data.batch[\"attention_mask\"]`: Attention mask\n- `response_mask`: Response mask indicating valid token positions\n- `n_samples`: Sample count per group, used for RLOO grouping computation\n- `config`: Reward weight configuration, hierarchical configuration object\n- `config.algorithm.reward_dpo_coef`: DPO reward weight coefficient, default 5.0\n- `config.algorithm.reward_gt_coef`: Accuracy reward weight coefficient, default 5.0", "detailed_description": "Function supports advantage computation with multi-source reward fusion. Reward sources include reward model scores and accuracy rewards.\nGroups multiple responses generated from the same prompt by n_samples intervals.\nUses RLOO to compute advantage function for each reward source and fuses multiple reward sources, computes cumulative return values after reward fusion and applies whitening.", "output": "- `advantages`: Whitened advantage function (torch.Tensor, shape: [batch_size, seq_len])\n- `returns`: Cumulative return values (torch.Tensor, shape: [batch_size, seq_len])", "func_implementation": "", "test_code_path": "code/test_code/test_rloo_advantage_return.py", "implementation_location": "code/recipe/prime/prime_core_algos.py:line 21-79", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_rloo_advantage_return`.\n\n**Function Overview:**\nImplements RLOO advantage estimation, supporting multiple reward source fusion\n\n**Function Signature:**\n```python\ndef compute_rloo_advantage_return(data: verl.DataProto, response_mask: torch.Tensor, n_samples, config):\n```\n\n**Input Parameters:**\n- `data`: Data protocol object containing DataProto.batch fields\n- `data.batch[\"rm_scores\"]`: Implicit reward scores\n- `data.batch[\"acc\"]`: Binary accuracy labels\n- `data.batch[\"prompts\"]`: Input prompt sequences\n- `data.batch[\"attention_mask\"]`: Attention mask\n- `response_mask`: Response mask indicating valid token positions\n- `n_samples`: Sample count per group, used for RLOO grouping computation\n- `config`: Reward weight configuration, hierarchical configuration object\n- `config.algorithm.reward_dpo_coef`: DPO reward weight coefficient, default 5.0\n- `config.algorithm.reward_gt_coef`: Accuracy reward weight coefficient, default 5.0\n\n**Detailed Description:**\nFunction supports advantage computation with multi-source reward fusion. Reward sources include reward model scores and accuracy rewards.\nGroups multiple responses generated from the same prompt by n_samples intervals.\nUses RLOO to compute advantage function for each reward source and fuses multiple reward sources, computes cumulative return values after reward fusion and applies whitening.\n\n**Expected Output:**\n- `advantages`: Whitened advantage function (torch.Tensor, shape: [batch_size, seq_len])\n- `returns`: Cumulative return values (torch.Tensor, shape: [batch_size, seq_len])\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport torch\n\ndef compute_rloo_advantage_return(data: \"verl.DataProto\", response_mask: torch.Tensor, n_samples=1, config=None):\n    \"\"\"\n    Compute RLOO (Remove-Leave-One-Out) advantage and returns with multi-source reward fusion.\n\n    Args:\n        data: verl.DataProto containing a .batch dict with keys:\n            - \"rm_scores\": reward model scalar scores, shape [batch_size]\n            - \"acc\": binary accuracy labels (0/1), shape [batch_size]\n            - (other fields ignored here)\n        response_mask: bool or 0/1 tensor indicating response token positions,\n                       shape [batch_size, seq_len]\n        n_samples: number of samples per prompt group used for RLOO grouping (int)\n        config: hierarchical config object with:\n            config.algorithm.reward_dpo_coef (float)\n            config.algorithm.reward_gt_coef (float)\n\n    Returns:\n        advantages: whitened advantage per-token tensor, shape [batch_size, seq_len]\n        returns: per-token cumulative return (fused reward) tensor, shape [batch_size, seq_len]\n    \"\"\"\n    # Extract tensors from data.batch\n    batch = data.batch\n    device = response_mask.device if isinstance(response_mask, torch.Tensor) else torch.device(\"cpu\")\n\n    # Get reward model scores and accuracy; ensure float tensors on correct device\n    rm_scores = torch.as_tensor(batch[\"rm_scores\"], dtype=torch.float32, device=device).reshape(-1)\n    acc = torch.as_tensor(batch[\"acc\"], dtype=torch.float32, device=device).reshape(-1)\n\n    bs = rm_scores.shape[0]\n    # Validate response_mask shape\n    if response_mask.dim() != 2 or response_mask.shape[0] != bs:\n        raise ValueError(f\"response_mask must be shape [batch_size, seq_len]; got {response_mask.shape}\")\n\n    seq_len = response_mask.shape[1]\n\n    # Get coefficients from config (with defaults if not present)\n    dpo_coef = getattr(getattr(config, \"algorithm\", object()), \"reward_dpo_coef\", 5.0)\n    gt_coef = getattr(getattr(config, \"algorithm\", object()), \"reward_gt_coef\", 5.0)\n\n    # Fuse reward sources into a single scalar reward per sample\n    # fused_reward = dpo_coef * rm_scores + gt_coef * acc\n    fused_reward = rm_scores * float(dpo_coef) + acc * float(gt_coef)  # shape [bs]\n\n    # Group into prompts: assume samples for the same prompt are contiguous and grouped by n_samples\n    if n_samples <= 0:\n        raise ValueError(\"n_samples must be >= 1\")\n    if bs % n_samples != 0:\n        raise ValueError(f\"batch size ({bs}) must be divisible by n_samples ({n_samples})\")\n\n    num_groups = bs // n_samples\n    # Reshape: [num_groups, n_samples]\n    fused_view = fused_reward.view(num_groups, n_samples)\n\n    # Compute leave-one-out baseline per sample:\n    # baseline_i = (sum_group - fused_i) / (n_samples - 1)\n    if n_samples == 1:\n        # No other samples to form a baseline; treat baseline as zero\n        baseline_view = torch.zeros_like(fused_view)\n    else:\n        sum_group = fused_view.sum(dim=1, keepdim=True)  # [num_groups, 1]\n        baseline_view = (sum_group - fused_view) / float(n_samples - 1)  # [num_groups, n_samples]\n\n    # Advantage per sample (scalar)\n    advantage_view = fused_view - baseline_view  # [num_groups, n_samples]\n    advantage_per_sample = advantage_view.view(-1)  # [bs]\n\n    # Build per-token returns and advantages by expanding scalars to response token positions\n    response_mask_float = response_mask.to(dtype=torch.float32)\n    # returns per token: fused reward scalar repeated for response tokens, zero elsewhere\n    returns = fused_reward.unsqueeze(1) * response_mask_float  # [bs, seq_len]\n\n    # Advantage per token: expand advantage scalar to response tokens\n    advantage_tokens = advantage_per_sample.unsqueeze(1) * response_mask_float  # [bs, seq_len]\n\n    # Whitening: compute mean/std across samples that have response tokens.\n    # We'll whiten the scalar advantages across samples that contain at least one response token,\n    # then re-expand to token positions. This keeps per-token structure intact and avoids\n    # mixing response and non-response positions in statistics.\n    has_response = (response_mask_float.sum(dim=1) > 0)  # [bs] bool\n    if has_response.any():\n        adv_vals = advantage_per_sample[has_response]  # scalars for samples that have responses\n        adv_mean = adv_vals.mean()\n        adv_std = adv_vals.std(unbiased=False)\n        eps = 1e-8\n        adv_std = adv_std if adv_std > 0 else torch.tensor(eps, device=device)\n        # whiten scalar advantages\n        whitened_adv_per_sample = (advantage_per_sample - adv_mean) / (adv_std + eps)\n        # For samples without response, keep zero advantage\n        whitened_adv_per_sample = torch.where(has_response, whitened_adv_per_sample, torch.zeros_like(whitened_adv_per_sample))\n    else:\n        # No response tokens anywhere: return zeros\n        whitened_adv_per_sample = torch.zeros_like(advantage_per_sample)\n\n    # Expand whitened advantages to per-token positions\n    advantages = whitened_adv_per_sample.unsqueeze(1) * response_mask_float  # [bs, seq_len]\n\n    return advantages, returns\n```"], "usage": {"prompt_tokens": 519, "completion_tokens": 2617, "total_tokens": 3136, "per_completion": [{"prompt_tokens": 519, "completion_tokens": 2617, "total_tokens": 3136}]}, "results": [false], "pass_ratios": [0.0]}
{"function_name": "RayPRIMETrainer.filter_and_downsample", "overview": "Implements intelligent sample filtering and priority downsampling for PRIME algorithm", "function_signature": "def filter_and_downsample(self, scores, batch: DataProto):", "input_parameters": "- `scores`: Verification score list, binary accuracy labels\n- `batch`: Data batch object\n- `config`: Filtering parameter configuration", "detailed_description": "Filters high-quality samples from oversampled candidate responses. Reconstructs verification scores into 2D matrix, performs dual filtering: accuracy filtering and length filtering.\nAccuracy filtering retains sample groups with average accuracy within configured range; length filtering removes sample groups exceeding maximum length threshold.\nSample groups meeting filtering conditions are retained with priority, ultimately retaining 1/oversample_factor proportion of samples. If all groups fail to meet filtering conditions, algorithm still retains corresponding proportion of samples to ensure training continues.", "output": "Filtered high-quality data batch", "func_implementation": "", "test_code_path": "code/test_code/test_filter_and_downsample.py", "implementation_location": "code/recipe/prime/prime_ray_trainer.py:line 540-572", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `RayPRIMETrainer.filter_and_downsample`.\n\n**Function Overview:**\nImplements intelligent sample filtering and priority downsampling for PRIME algorithm\n\n**Function Signature:**\n```python\ndef filter_and_downsample(self, scores, batch: DataProto):\n```\n\n**Input Parameters:**\n- `scores`: Verification score list, binary accuracy labels\n- `batch`: Data batch object\n- `config`: Filtering parameter configuration\n\n**Detailed Description:**\nFilters high-quality samples from oversampled candidate responses. Reconstructs verification scores into 2D matrix, performs dual filtering: accuracy filtering and length filtering.\nAccuracy filtering retains sample groups with average accuracy within configured range; length filtering removes sample groups exceeding maximum length threshold.\nSample groups meeting filtering conditions are retained with priority, ultimately retaining 1/oversample_factor proportion of samples. If all groups fail to meet filtering conditions, algorithm still retains corresponding proportion of samples to ensure training continues.\n\n**Expected Output:**\nFiltered high-quality data batch\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def filter_and_downsample(self, scores, batch: \"DataProto\"):\n    \"\"\"\n    Filter high-quality samples from oversampled candidate responses and downsample\n    to retain 1 / oversample_factor proportion (i.e., one candidate per original group).\n\n    Args:\n        scores: iterable of verification scores (binary accuracy labels) of length N.\n        batch: DataProto-like batch containing N candidate examples (N = num_groups * oversample_factor)\n\n    Returns:\n        Filtered batch containing one selected candidate per original group.\n    \"\"\"\n    import numpy as np\n\n    # Obtain configuration with fallbacks/ defaults\n    cfg = getattr(self, \"config\", None) or getattr(self, \"args\", None) or {}\n    oversample = getattr(self, \"oversample_factor\", None)\n    if oversample is None:\n        oversample = getattr(cfg, \"oversample_factor\", None) or cfg.get(\"oversample_factor\", None) if isinstance(cfg, dict) else None\n    if oversample is None:\n        # Default oversample factor if not provided\n        oversample = 4\n\n    min_acc = getattr(cfg, \"min_acc\", None) or (cfg.get(\"min_acc\") if isinstance(cfg, dict) else None)\n    max_acc = getattr(cfg, \"max_acc\", None) or (cfg.get(\"max_acc\") if isinstance(cfg, dict) else None)\n    max_len = getattr(cfg, \"max_length\", None) or (cfg.get(\"max_length\") if isinstance(cfg, dict) else None)\n\n    # sensible defaults\n    if min_acc is None:\n        min_acc = 0.0\n    if max_acc is None:\n        max_acc = 1.0\n    if max_len is None:\n        max_len = 2 ** 20  # effectively no length limit by default\n\n    # Convert scores to numpy array\n    scores_arr = np.asarray(scores).astype(float).reshape(-1)\n    total_candidates = scores_arr.shape[0]\n\n    # Try to ensure divisibility; if not divisible, trim tail\n    if total_candidates % oversample != 0:\n        # trim extra candidates at end while warning in comment\n        valid_count = (total_candidates // oversample) * oversample\n        if valid_count == 0:\n            raise ValueError(\"Not enough candidate samples to form any group with given oversample_factor.\")\n        scores_arr = scores_arr[:valid_count]\n        total_candidates = valid_count\n\n    num_groups = total_candidates // oversample\n    scores_mat = scores_arr.reshape(num_groups, oversample)\n\n    # Try to extract per-candidate lengths from batch in multiple ways\n    def try_get_attr(obj, name):\n        # support dict, attribute, or key access\n        try:\n            if isinstance(obj, dict):\n                return obj.get(name, None)\n            if hasattr(obj, name):\n                return getattr(obj, name)\n            if hasattr(obj, \"__getitem__\"):\n                return obj[name] if name in obj else None\n        except Exception:\n            return None\n        return None\n\n    lengths = None\n    # Common field names that might store response token ids or lengths\n    length_fields = [\"response_lengths\", \"resp_lens\", \"lengths\", \"seq_lens\"]\n    token_fields = [\"response_ids\", \"response_tokens\", \"labels\", \"input_ids\", \"decoder_input_ids\"]\n\n    for fld in length_fields:\n        val = try_get_attr(batch, fld)\n        if val is not None:\n            lengths = np.asarray(val).reshape(-1)\n            break\n\n    # If no direct lengths available try to infer from token arrays by counting non-pad tokens (pad id assumed 0)\n    if lengths is None:\n        tok_arr = None\n        for fld in token_fields:\n            val = try_get_attr(batch, fld)\n            if val is None:\n                continue\n            # If val is a numpy/torch array-like\n            try:\n                arr = np.asarray(val)\n                if arr.ndim == 2:\n                    tok_arr = arr\n                    break\n                # If flattened list of token sequences, try to treat as list-of-lists\n                if arr.ndim == 1 and isinstance(arr[0], (list, tuple, np.ndarray)):\n                    # convert to ragged lengths\n                    lengths = np.array([int(np.count_nonzero(np.asarray(x) != 0)) for x in arr])\n                    break\n            except Exception:\n                continue\n        if tok_arr is not None:\n            # assume pad token id = 0\n            lengths = np.count_nonzero(tok_arr != 0, axis=1)\n\n    # If lengths still None, fallback to zeros\n    if lengths is None:\n        lengths = np.zeros(total_candidates, dtype=int)\n    else:\n        lengths = np.asarray(lengths).reshape(-1)[:total_candidates]\n\n    lengths_mat = lengths.reshape(num_groups, oversample)\n\n    # Group-level statistics for filtering\n    avg_acc_per_group = scores_mat.mean(axis=1)\n    max_len_per_group = lengths_mat.max(axis=1)\n\n    pass_acc_group = (avg_acc_per_group >= min_acc) & (avg_acc_per_group <= max_acc)\n    pass_len_group = max_len_per_group <= max_len\n\n    # Selection: pick one candidate index per group\n    selected_global_indices = []\n    for g in range(num_groups):\n        group_scores = scores_mat[g]            # shape (oversample,)\n        group_lengths = lengths_mat[g]          # shape (oversample,)\n        base_idx = g * oversample\n\n        # Priority: if group passes both accuracy and length, prefer candidates meeting both criteria\n        if pass_acc_group[g] and pass_len_group[g]:\n            # candidates that are short enough (all are since pass_len_group) and highest score\n            best_score = group_scores.max()\n            cand_idxs = np.where(group_scores == best_score)[0]\n            if cand_idxs.size > 1:\n                # tie-breaker: choose shortest length among tied\n                tie_lengths = group_lengths[cand_idxs]\n                rel = np.argmin(tie_lengths)\n                chosen = int(cand_idxs[rel])\n            else:\n                chosen = int(cand_idxs[0])\n            selected_global_indices.append(base_idx + chosen)\n            continue\n\n        # If group passes only accuracy filtering\n        if pass_acc_group[g] and not pass_len_group[g]:\n            # prefer candidates under length threshold if any; otherwise pick highest score\n            ok_len_mask = group_lengths <= max_len\n            if ok_len_mask.any():\n                # among these, pick highest score then shortest length\n                candidates = np.where(ok_len_mask)[0]\n                sub_scores = group_scores[candidates]\n                best_score = sub_scores.max()\n                cand_sub = candidates[np.where(sub_scores == best_score)[0]]\n                if cand_sub.size > 1:\n                    rel = np.argmin(group_lengths[cand_sub])\n                    chosen = int(cand_sub[rel])\n                else:\n                    chosen = int(cand_sub[0])\n            else:\n                # fallback: pick highest score in group\n                best_score = group_scores.max()\n                cand_idxs = np.where(group_scores == best_score)[0]\n                chosen = int(cand_idxs[0])  # choose first if tie\n            selected_global_indices.append(base_idx + chosen)\n            continue\n\n        # If group fails accuracy but passes length\n        if (not pass_acc_group[g]) and pass_len_group[g]:\n            # prefer shortest among those passing length and highest score\n            best_score = group_scores.max()\n            cand_idxs = np.where(group_scores == best_score)[0]\n            # prefer those within length (all pass_len_group ensures max len ok, but some candidates could still be >max_len)\n            cand_idxs_in_len = [i for i in cand_idxs if group_lengths[i] <= max_len]\n            if len(cand_idxs_in_len) > 0:\n                if len(cand_idxs_in_len) > 1:\n                    tie_lengths = group_lengths[cand_idxs_in_len]\n                    rel = int(np.argmin(tie_lengths))\n                    chosen = int(cand_idxs_in_len[rel])\n                else:\n                    chosen = int(cand_idxs_in_len[0])\n            else:\n                chosen = int(cand_idxs[0])\n            selected_global_indices.append(base_idx + chosen)\n            continue\n\n        # Both filters fail: still must retain one candidate to keep training flowing\n        # Choose highest scoring candidate (prefer shorter length if tie)\n        best_score = group_scores.max()\n        cand_idxs = np.where(group_scores == best_score)[0]\n        if cand_idxs.size > 1:\n            tie_lengths = group_lengths[cand_idxs]\n            rel = np.argmin(tie_lengths)\n            chosen = int(cand_idxs[rel])\n        else:\n            chosen = int(cand_idxs[0])\n        selected_global_indices.append(base_idx + chosen)\n\n    selected_global_indices = np.array(selected_global_indices, dtype=int)\n\n    # Subset the batch according to selected_global_indices.\n    # Support multiple possible batch types: dict-like, object with __getitem__, or custom select()\n    def subset_batch(b, indices):\n        # If batch has a 'select' method (common in some DataProto implementations)\n        if hasattr(b, \"select\") and callable(getattr(b, \"select\")):\n            try:\n                return b.select(indices.tolist() if hasattr(indices, \"tolist\") else indices)\n            except Exception:\n                pass\n        # If batch supports indexing like b[indices]\n        try:\n            return b[indices]\n        except Exception:\n            pass\n        # If batch is a dict-like of arrays, subset each array along axis 0\n        if isinstance(b, dict):\n            out = {}\n            for k, v in b.items():\n                try:\n                    arr = np.asarray(v)\n                    # Only subset if first dim matches total_candidates\n                    if arr.shape and arr.shape[0] == total_candidates:\n                        out[k] = arr[indices]\n                    else:\n                        out[k] = v\n                except Exception:\n                    out[k] = v\n            return out\n        # As a last resort, attempt to shallow-copy attributes and subset array-like attributes\n        try:\n            new_b = b.__class__()\n            for attr in dir(b):\n                if attr.startswith(\"_\"):\n                    continue\n                try:\n                    val = getattr(b, attr)\n                except Exception:\n                    continue\n                try:\n                    arr = np.asarray(val)\n                    if arr.shape and arr.shape[0] == total_candidates:\n                        setattr(new_b, attr, arr[indices])\n                    else:\n                        setattr(new_b, attr, val)\n                except Exception:\n                    setattr(new_b, attr, val)\n            return new_b\n        except Exception:\n            # If everything fails, return original batch (best-effort fallback)\n            return b\n\n    filtered_batch = subset_batch(batch, selected_global_indices)\n    return filtered_batch"], "usage": {"prompt_tokens": 388, "completion_tokens": 4087, "total_tokens": 4475, "per_completion": [{"prompt_tokens": 388, "completion_tokens": 4087, "total_tokens": 4475}]}, "results": [false], "pass_ratios": [0.0]}
