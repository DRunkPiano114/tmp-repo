{"function_name": "compute_grpo_outcome_advantage_split", "overview": "Computes group GRPO (Group Relative Policy Optimization) advantage estimation, with the key innovation of using only on-policy samples to compute within-group baseline.", "function_signature": "def compute_grpo_outcome_advantage_split(token_level_rewards: torch.Tensor,\neos_mask: torch.Tensor,\nindex: torch.Tensor,\non_policy_mask: torch.Tensor,\nepsilon: float = 1e-6,\nuse_std: bool = True):", "input_parameters": "- `token_level_rewards`: Token-level reward tensor with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `eos_mask`: Valid token mask with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `index`: Sample grouping indices, n responses from the same prompt share the same index. **Note: This parameter should be a Python list or indexable sequence (e.g., `[0, 0, 1, 1]`), not `torch.Tensor`, as index values will be used as dictionary keys**\n- `on_policy_mask`: Boolean mask with shape `[batch_size]`, type `torch.Tensor`, marking which samples are on-policy generated\n- `epsilon`: Numerical stability constant, default 1e-6\n- `use_std`: Whether to use standard deviation for normalization, default True", "detailed_description": "This function needs to sum token-level reward tensors along the time dimension for each response sequence to obtain sequence-level total scores.\nThe main implementation performs grouped normalization (including both on-policy and off-policy) for multiple responses from the same prompt, computing relative advantage values. During normalization, only samples marked as on-policy are selected to compute the group's mean and standard deviation. If a group has only one on-policy sample, set mean to 0 and standard deviation to 1; if there are multiple samples, compute statistics normally. For cases where standard deviation is 0, replace it with 1 to avoid division by zero errors.\nIf standard deviation normalization is enabled, compute (score - mean) / (std + epsilon); otherwise only subtract the mean. Finally, expand sequence-level advantage values to token-level advantage tensors.", "output": "- `advantages`: Token-level advantage tensor with shape `[batch_size, response_length]`\n- `returns`: Token-level return tensor with shape `[batch_size, response_length]`, same as advantages", "func_implementation": "", "test_code_path": "code/test_code/test_compute_grpo_outcome_advantage_split.py", "implementation_location": "code/luffy/verl/verl/mix_src/mix_core_alg.py:line 12-67", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_grpo_outcome_advantage_split`.\n\n**Function Overview:**\nComputes group GRPO (Group Relative Policy Optimization) advantage estimation, with the key innovation of using only on-policy samples to compute within-group baseline.\n\n**Function Signature:**\n```python\ndef compute_grpo_outcome_advantage_split(token_level_rewards: torch.Tensor,\neos_mask: torch.Tensor,\nindex: torch.Tensor,\non_policy_mask: torch.Tensor,\nepsilon: float = 1e-6,\nuse_std: bool = True):\n```\n\n**Input Parameters:**\n- `token_level_rewards`: Token-level reward tensor with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `eos_mask`: Valid token mask with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `index`: Sample grouping indices, n responses from the same prompt share the same index. **Note: This parameter should be a Python list or indexable sequence (e.g., `[0, 0, 1, 1]`), not `torch.Tensor`, as index values will be used as dictionary keys**\n- `on_policy_mask`: Boolean mask with shape `[batch_size]`, type `torch.Tensor`, marking which samples are on-policy generated\n- `epsilon`: Numerical stability constant, default 1e-6\n- `use_std`: Whether to use standard deviation for normalization, default True\n\n**Detailed Description:**\nThis function needs to sum token-level reward tensors along the time dimension for each response sequence to obtain sequence-level total scores.\nThe main implementation performs grouped normalization (including both on-policy and off-policy) for multiple responses from the same prompt, computing relative advantage values. During normalization, only samples marked as on-policy are selected to compute the group's mean and standard deviation. If a group has only one on-policy sample, set mean to 0 and standard deviation to 1; if there are multiple samples, compute statistics normally. For cases where standard deviation is 0, replace it with 1 to avoid division by zero errors.\nIf standard deviation normalization is enabled, compute (score - mean) / (std + epsilon); otherwise only subtract the mean. Finally, expand sequence-level advantage values to token-level advantage tensors.\n\n**Expected Output:**\n- `advantages`: Token-level advantage tensor with shape `[batch_size, response_length]`\n- `returns`: Token-level return tensor with shape `[batch_size, response_length]`, same as advantages\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport torch\nfrom typing import Sequence\n\ndef compute_grpo_outcome_advantage_split(token_level_rewards: torch.Tensor,\n                                        eos_mask: torch.Tensor,\n                                        index: Sequence,\n                                        on_policy_mask: torch.Tensor,\n                                        epsilon: float = 1e-6,\n                                        use_std: bool = True):\n    \"\"\"\n    Compute group GRPO outcome advantage with within-group baseline computed\n    using only on-policy samples.\n\n    Args:\n        token_level_rewards: [batch_size, response_length] float tensor\n        eos_mask: [batch_size, response_length] mask (bool or 0/1) indicating valid tokens\n        index: sequence/list of group ids for each sample in the batch (length batch_size)\n               Note: index must be indexable/iterable and contain hashable keys (used as dict keys)\n        on_policy_mask: [batch_size] bool tensor marking which samples are on-policy\n        epsilon: small constant for numerical stability\n        use_std: whether to divide by std (normalize) or only subtract mean\n\n    Returns:\n        advantages: [batch_size, response_length] token-level advantage tensor\n        returns: [batch_size, response_length] same as advantages\n    \"\"\"\n    # Basic checks\n    if not isinstance(index, Sequence):\n        raise TypeError(\"index must be a Python sequence (e.g., list) of group identifiers.\")\n    batch_size, resp_len = token_level_rewards.shape\n    if len(index) != batch_size:\n        raise ValueError(\"Length of `index` must equal batch_size (first dim of token_level_rewards).\")\n    if on_policy_mask.shape[0] != batch_size:\n        raise ValueError(\"on_policy_mask must have length equal to batch_size.\")\n\n    device = token_level_rewards.device\n    dtype = token_level_rewards.dtype\n\n    # Ensure eos_mask is boolean mask\n    eos_bool = eos_mask.to(torch.bool)\n\n    # Compute sequence-level scores by summing valid token rewards\n    seq_scores = (token_level_rewards * eos_bool.to(dtype)).sum(dim=1)  # shape: [batch_size]\n\n    # Prepare output tensor for sequence-level advantages\n    seq_adv = torch.zeros_like(seq_scores, device=device, dtype=dtype)\n\n    # Convert on_policy_mask to python list of bools for easy indexing\n    on_policy_list = on_policy_mask.to(torch.bool).tolist()\n\n    # Identify unique groups in order of first occurrence\n    # Using dict.fromkeys preserves order and uniqueness\n    unique_groups = list(dict.fromkeys(index))\n\n    # For each group compute mean/std using only on-policy samples, then apply to all group members\n    for group in unique_groups:\n        # collect positions belonging to this group\n        positions = [i for i, g in enumerate(index) if g == group]\n        if len(positions) == 0:\n            continue  # should not happen but safe-guard\n\n        # on-policy positions within this group\n        on_positions = [p for p in positions if on_policy_list[p]]\n\n        # Decide mean and std according to rules:\n        # - if group has <= 1 on-policy sample: mean=0, std=1\n        # - else compute mean/std normally (std replaced by 1 if zero)\n        if len(on_positions) <= 1:\n            mean = torch.tensor(0.0, device=device, dtype=dtype)\n            std = torch.tensor(1.0, device=device, dtype=dtype)\n        else:\n            # compute mean and std on the selected on-policy sequence scores\n            vals = seq_scores[on_positions]\n            mean = vals.mean()\n            # use population std (unbiased=False) to avoid NaN for small samples; then guard zero\n            std = vals.std(unbiased=False)\n            # if std is zero (or numerically close), set to 1 to avoid division by zero\n            if (std.abs() <= 0).item():\n                std = torch.tensor(1.0, device=device, dtype=dtype)\n\n        # compute advantage for all samples in the group\n        if use_std:\n            # normalize with std (add epsilon for stability)\n            denom = std + epsilon\n            for p in positions:\n                seq_adv[p] = (seq_scores[p] - mean) / denom\n        else:\n            for p in positions:\n                seq_adv[p] = (seq_scores[p] - mean)\n\n    # Expand sequence-level advantages to token-level\n    adv_tokens = seq_adv.unsqueeze(1).expand(batch_size, resp_len).to(dtype)\n\n    # Zero-out invalid tokens (where eos_mask is False)\n    adv_tokens = adv_tokens * eos_bool.to(dtype)\n\n    # In this setting returns are the same as advantages\n    returns = adv_tokens.clone()\n\n    return adv_tokens, returns\n```"], "usage": {"prompt_tokens": 690, "completion_tokens": 2192, "total_tokens": 2882, "per_completion": [{"prompt_tokens": 690, "completion_tokens": 2192, "total_tokens": 2882}]}, "results": [false], "pass_ratios": [0.5]}
{"function_name": "compute_token_on_off_policy_loss", "overview": "Computes mixed policy loss, combining on-policy PPO clip loss and off-policy importance sampling loss, supporting multiple policy shaping methods to emphasize low-probability but critical actions.", "function_signature": "def compute_token_on_off_policy_loss(\nold_log_prob,\nlog_prob,\nadvantages,\neos_mask,\ncliprange,\nclip_upper_bound,\nprefix_mask,\noff_cliprange,\noff_normalize=False,\noff_abs_cliprange=None,\noff_max_clip=None,\noff_min_clip=None,\nall_max_clip=None,\noff_policy_reshape=\"no_reshape\",\noff_policy_reshape_weight=1.0,\noff_policy_reshape_pow_exp=0.5,\non_policy_reshape=\"no_reshape\",\non_policy_reshape_weight=1.0,\non_policy_reshape_pow_exp=0.5,\ntarget_probs=None,\nloss_remove_token_mean=False,\nloss_remove_clip=False,\n):", "input_parameters": "- `old_log_prob`: Old policy log probabilities with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `log_prob`: Current policy log probabilities with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `advantages`: Advantage estimates with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `eos_mask`: Valid token mask with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `cliprange`: PPO clip range, default 0.2\n- `clip_upper_bound`: PPO clip upper bound, default 100.0\n- `prefix_mask`: Mask marking off-policy tokens with shape `[batch_size, response_length]`, **type must be `torch.Tensor` with dtype `torch.bool`, as the code uses bitwise NOT operator `~prefix_mask` to obtain on-policy token mask**\n- `off_cliprange`: off-policy clip range (not used in current implementation)\n- `off_normalize`: Whether to normalize off-policy ratio, default False\n- `off_abs_cliprange`: off-policy absolute value clip range (not used in current implementation)\n- `off_max_clip`: off-policy ratio upper limit, None means no limit\n- `off_min_clip`: off-policy ratio lower limit, None means no limit\n- `all_max_clip`: Global probability upper limit, None means no limit\n- `off_policy_reshape`: off-policy policy shaping method name, default 'no_reshape'\n- `off_policy_reshape_weight`: Policy shaping weight parameter, default 1.0\n- `off_policy_reshape_pow_exp`: Power transformation exponent, default 0.5\n- `on_policy_reshape`: on-policy policy shaping method name, default 'no_reshape'\n- `on_policy_reshape_weight`: on-policy policy shaping weight, default 1.0\n- `on_policy_reshape_pow_exp`: on-policy power transformation exponent, default 0.5\n- `target_probs`: Optional target probabilities (for explicit behavior policy), default None\n- `loss_remove_token_mean`: Whether to remove token-level averaging, default False\n- `loss_remove_clip`: Whether to remove clip operation, default False", "detailed_description": "This function computes mixed policy gradient loss, with the core idea of simultaneously handling both on-policy and off-policy data. For on-policy data, it uses standard PPO algorithm to compute importance sampling ratio and perform clipping; for off-policy data, it directly uses current policy probabilities (or ratio relative to target probabilities) to compute loss, without relying on old policy.\nThe function distinguishes which tokens are off-policy and which are on-policy through prefix_mask, then combines the two losses with weighting. Additionally, the function should support multiple probability reshaping strategies and clipping mechanisms.", "output": "Returns dictionary containing the following key-value pairs:\n- `pg_loss`: Total policy gradient loss (scalar)\n- `off_pg_loss`: off-policy loss component (scalar)\n- `on_pg_loss`: on-policy loss component (scalar)\n- `off_pg_clipfrac`: off-policy clip fraction (scalar, currently 0 in implementation)\n- `on_pg_clipfrac`: on-policy clip fraction (scalar)\n- `ppo_kl`: PPO KL divergence (scalar)\n- `off_policy_prob`: Average probability of off-policy samples (scalar)\n- `on_policy_prob`: Average probability of on-policy samples (scalar)\n- `off_ratio_mean`: off-policy average ratio (scalar)\n- `off_ratio_max_clip_frac`: Fraction reaching upper clip limit (scalar)\n- `off_ratio_min_clip_frac`: Fraction reaching lower clip limit (scalar)", "func_implementation": "", "test_code_path": "code/test_code/test_compute_token_on_off_policy_loss.py", "implementation_location": "code/luffy/verl/verl/mix_src/mix_core_alg.py:line 69-251", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_token_on_off_policy_loss`.\n\n**Function Overview:**\nComputes mixed policy loss, combining on-policy PPO clip loss and off-policy importance sampling loss, supporting multiple policy shaping methods to emphasize low-probability but critical actions.\n\n**Function Signature:**\n```python\ndef compute_token_on_off_policy_loss(\nold_log_prob,\nlog_prob,\nadvantages,\neos_mask,\ncliprange,\nclip_upper_bound,\nprefix_mask,\noff_cliprange,\noff_normalize=False,\noff_abs_cliprange=None,\noff_max_clip=None,\noff_min_clip=None,\nall_max_clip=None,\noff_policy_reshape=\"no_reshape\",\noff_policy_reshape_weight=1.0,\noff_policy_reshape_pow_exp=0.5,\non_policy_reshape=\"no_reshape\",\non_policy_reshape_weight=1.0,\non_policy_reshape_pow_exp=0.5,\ntarget_probs=None,\nloss_remove_token_mean=False,\nloss_remove_clip=False,\n):\n```\n\n**Input Parameters:**\n- `old_log_prob`: Old policy log probabilities with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `log_prob`: Current policy log probabilities with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `advantages`: Advantage estimates with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `eos_mask`: Valid token mask with shape `[batch_size, response_length]`, type `torch.Tensor`\n- `cliprange`: PPO clip range, default 0.2\n- `clip_upper_bound`: PPO clip upper bound, default 100.0\n- `prefix_mask`: Mask marking off-policy tokens with shape `[batch_size, response_length]`, **type must be `torch.Tensor` with dtype `torch.bool`, as the code uses bitwise NOT operator `~prefix_mask` to obtain on-policy token mask**\n- `off_cliprange`: off-policy clip range (not used in current implementation)\n- `off_normalize`: Whether to normalize off-policy ratio, default False\n- `off_abs_cliprange`: off-policy absolute value clip range (not used in current implementation)\n- `off_max_clip`: off-policy ratio upper limit, None means no limit\n- `off_min_clip`: off-policy ratio lower limit, None means no limit\n- `all_max_clip`: Global probability upper limit, None means no limit\n- `off_policy_reshape`: off-policy policy shaping method name, default 'no_reshape'\n- `off_policy_reshape_weight`: Policy shaping weight parameter, default 1.0\n- `off_policy_reshape_pow_exp`: Power transformation exponent, default 0.5\n- `on_policy_reshape`: on-policy policy shaping method name, default 'no_reshape'\n- `on_policy_reshape_weight`: on-policy policy shaping weight, default 1.0\n- `on_policy_reshape_pow_exp`: on-policy power transformation exponent, default 0.5\n- `target_probs`: Optional target probabilities (for explicit behavior policy), default None\n- `loss_remove_token_mean`: Whether to remove token-level averaging, default False\n- `loss_remove_clip`: Whether to remove clip operation, default False\n\n**Detailed Description:**\nThis function computes mixed policy gradient loss, with the core idea of simultaneously handling both on-policy and off-policy data. For on-policy data, it uses standard PPO algorithm to compute importance sampling ratio and perform clipping; for off-policy data, it directly uses current policy probabilities (or ratio relative to target probabilities) to compute loss, without relying on old policy.\nThe function distinguishes which tokens are off-policy and which are on-policy through prefix_mask, then combines the two losses with weighting. Additionally, the function should support multiple probability reshaping strategies and clipping mechanisms.\n\n**Expected Output:**\nReturns dictionary containing the following key-value pairs:\n- `pg_loss`: Total policy gradient loss (scalar)\n- `off_pg_loss`: off-policy loss component (scalar)\n- `on_pg_loss`: on-policy loss component (scalar)\n- `off_pg_clipfrac`: off-policy clip fraction (scalar, currently 0 in implementation)\n- `on_pg_clipfrac`: on-policy clip fraction (scalar)\n- `ppo_kl`: PPO KL divergence (scalar)\n- `off_policy_prob`: Average probability of off-policy samples (scalar)\n- `on_policy_prob`: Average probability of on-policy samples (scalar)\n- `off_ratio_mean`: off-policy average ratio (scalar)\n- `off_ratio_max_clip_frac`: Fraction reaching upper clip limit (scalar)\n- `off_ratio_min_clip_frac`: Fraction reaching lower clip limit (scalar)\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport torch\n\ndef compute_token_on_off_policy_loss(\n    old_log_prob,\n    log_prob,\n    advantages,\n    eos_mask,\n    cliprange=0.2,\n    clip_upper_bound=100.0,\n    prefix_mask=None,\n    off_cliprange=None,\n    off_normalize=False,\n    off_abs_cliprange=None,\n    off_max_clip=None,\n    off_min_clip=None,\n    all_max_clip=None,\n    off_policy_reshape=\"no_reshape\",\n    off_policy_reshape_weight=1.0,\n    off_policy_reshape_pow_exp=0.5,\n    on_policy_reshape=\"no_reshape\",\n    on_policy_reshape_weight=1.0,\n    on_policy_reshape_pow_exp=0.5,\n    target_probs=None,\n    loss_remove_token_mean=False,\n    loss_remove_clip=False,\n):\n    \"\"\"\n    Compute mixed on-policy PPO and off-policy importance-sampling policy gradient loss per-token.\n\n    Notes about masks:\n      - eos_mask: valid tokens mask (should be bool or convertible to bool)\n      - prefix_mask: bool mask marking off-policy tokens (True -> off-policy). Must be dtype=torch.bool.\n                     On-policy mask will be ~prefix_mask.\n\n    Returns dict with keys:\n      - 'pg_loss', 'off_pg_loss', 'on_pg_loss', 'off_pg_clipfrac', 'on_pg_clipfrac',\n        'ppo_kl', 'off_policy_prob', 'on_policy_prob',\n        'off_ratio_mean', 'off_ratio_max_clip_frac', 'off_ratio_min_clip_frac'\n    \"\"\"\n\n    eps = 1e-12\n\n    # Ensure masks are boolean tensors on same device\n    device = log_prob.device\n    eos_mask = eos_mask.bool().to(device)\n    if prefix_mask is None:\n        # If no prefix mask provided, treat everything as on-policy\n        prefix_mask = torch.zeros_like(eos_mask, dtype=torch.bool, device=device)\n    else:\n        # Ensure dtype bool\n        if prefix_mask.dtype != torch.bool:\n            raise TypeError(\"prefix_mask must be a torch.Tensor with dtype torch.bool\")\n        prefix_mask = prefix_mask.to(device)\n\n    # on/off masks (only valid tokens)\n    off_mask = eos_mask & prefix_mask\n    on_mask = eos_mask & (~prefix_mask)\n\n    # Helper to safe mean over mask:\n    def masked_mean(x, mask):\n        # If mask has zero elements, return zero scalar on correct device/type\n        masked = x * mask.to(x.dtype)\n        total = mask.sum()\n        if total.item() == 0:\n            return (torch.tensor(0.0, device=device, dtype=x.dtype))\n        if not loss_remove_token_mean:\n            return masked.sum() / (total.to(x.dtype) + eps)\n        else:\n            # sequence-equal weighting: compute per-sequence mean (sum over tokens / num_tokens seq),\n            # then average over batch sequences (including sequences with zero tokens counted as zero)\n            per_seq_count = mask.sum(dim=1).clamp_min(1).to(x.dtype)  # avoid div by zero\n            per_seq_mean = (masked.sum(dim=1) / per_seq_count)\n            return per_seq_mean.mean()\n\n    # Convert log probs to probs\n    prob = torch.exp(log_prob)\n    old_prob = torch.exp(old_log_prob)\n\n    # On-policy (PPO style)\n    ratio_on = torch.exp(log_prob - old_log_prob)  # importance sampling ratio\n    # Optional on-policy probability reshaping (if desired) - for now allow few strategies\n    def compute_reshape_factor(p, method, weight, pow_exp, t_probs=None):\n        # p: current probabilities\n        if method is None or method == \"no_reshape\":\n            return torch.ones_like(p)\n        if method == \"inverse\":\n            # emphasize low-probability actions: 1 / p^pow_exp scaled by weight\n            return (1.0 / (p + eps)) ** (pow_exp) * weight\n        if method == \"pow\":\n            # use p^pow_exp (reduces low-prob emphasis if pow_exp>0)\n            return (p + eps) ** (pow_exp) * weight\n        if method == \"target_ratio\":\n            # if target probs available, weight by (target / p)^pow\n            if t_probs is None:\n                return torch.ones_like(p)\n            return ((t_probs + eps) / (p + eps)) ** (pow_exp) * weight\n        if method == \"neg_log\":\n            # (-log p)^pow\n            return (-(torch.log(p + eps))) ** (pow_exp) * weight\n        # default fallback\n        return torch.ones_like(p)\n\n    # Apply on-policy reshape factor to ratio if requested\n    on_reshape_factor = compute_reshape_factor(prob, on_policy_reshape, on_policy_reshape_weight, on_policy_reshape_pow_exp, target_probs)\n    ratio_on = ratio_on * on_reshape_factor\n\n    # PPO clipped surrogate:\n    unclipped_obj = ratio_on * advantages\n    clipped_ratio = torch.clamp(ratio_on, 1.0 - cliprange, 1.0 + cliprange)\n    clipped_ratio = torch.clamp(clipped_ratio, max=clip_upper_bound)  # also cap the upper bound\n    clipped_obj = clipped_ratio * advantages\n\n    if loss_remove_clip:\n        # don't perform clipping; use unclipped objective\n        chosen_obj = unclipped_obj\n    else:\n        # typical PPO logic: for positive advantages take min, for negative take max\n        chosen_obj = torch.where(advantages > 0, torch.min(unclipped_obj, clipped_obj), torch.max(unclipped_obj, clipped_obj))\n\n    # on-policy loss tokens (negative surrogate)\n    on_losses_tokens = -chosen_obj  # per-token\n    # Mask out off tokens not on-policy\n    on_losses_masked = on_losses_tokens * on_mask.to(on_losses_tokens.dtype)\n\n    # on clip fraction: fraction of on-policy tokens where ratio was clipped (|ratio - 1| > cliprange)\n    on_clip_violations = (torch.abs(ratio_on - 1.0) > cliprange).to(torch.float32) * on_mask.to(torch.float32)\n    on_pg_clipfrac = (on_clip_violations.sum() / (on_mask.sum().clamp_min(1.0))).item() if on_mask.sum().item() > 0 else 0.0\n\n    # PPO KL approx (mean old_log_prob - new_log_prob) over on-policy tokens\n    if on_mask.sum().item() > 0:\n        ppo_kl = ((old_log_prob - log_prob) * on_mask.to(old_log_prob.dtype)).sum() / on_mask.sum()\n        ppo_kl = ppo_kl.detach()\n    else:\n        ppo_kl = torch.tensor(0.0, device=device, dtype=log_prob.dtype)\n\n    # Off-policy handling\n    # According to description: use current policy probabilities (or ratio relative to target_probs)\n    if target_probs is None:\n        off_ratio = prob.clone()\n    else:\n        off_ratio = prob / (target_probs + eps)\n\n    # optional normalization across off-policy tokens\n    if off_normalize:\n        # normalize by mean over off-policy tokens (avoid dividing by zero)\n        mean_off = masked_mean(off_ratio, off_mask)\n        if mean_off.item() != 0:\n            off_ratio = off_ratio / (mean_off + eps)\n\n    # Apply off-policy reshaping strategies\n    off_reshape_factor = compute_reshape_factor(prob, off_policy_reshape, off_policy_reshape_weight, off_policy_reshape_pow_exp, target_probs)\n    off_ratio = off_ratio * off_reshape_factor\n\n    # Global clipping rules for off_ratio\n    if off_max_clip is not None:\n        off_ratio = torch.clamp(off_ratio, max=off_max_clip)\n    if off_min_clip is not None:\n        off_ratio = torch.clamp(off_ratio, min=off_min_clip)\n    if all_max_clip is not None:\n        off_ratio = torch.clamp(off_ratio, max=all_max_clip)\n\n    # Off-policy loss tokens\n    off_losses_tokens = -(off_ratio * advantages)\n    off_losses_masked = off_losses_tokens * off_mask.to(off_losses_tokens.dtype)\n\n    # Off clip stats (fraction reaching upper/lower clips)\n    def clip_frac_hits(ratio, mask, min_clip, max_clip):\n        total = mask.sum().item()\n        if total == 0:\n            return 0.0, 0.0\n        max_frac = 0.0\n        min_frac = 0.0\n        if max_clip is not None:\n            max_hits = ((ratio >= max_clip - 1e-12) & mask).to(torch.float32).sum().item()\n            max_frac = max_hits / total\n        if min_clip is not None:\n            min_hits = ((ratio <= min_clip + 1e-12) & mask).to(torch.float32).sum().item()\n            min_frac = min_hits / total\n        return max_frac, min_frac\n\n    off_ratio_mean_val = masked_mean(off_ratio, off_mask).item() if off_mask.sum().item() > 0 else 0.0\n    off_ratio_max_clip_frac, off_ratio_min_clip_frac = clip_frac_hits(off_ratio, off_mask, off_min_clip, off_max_clip)\n\n    # Means for probabilities\n    off_policy_prob = masked_mean(prob, off_mask).item() if off_mask.sum().item() > 0 else 0.0\n    on_policy_prob = masked_mean(prob, on_mask).item() if on_mask.sum().item() > 0 else 0.0\n\n    # Convert token-level masked losses to scalars\n    on_pg_loss = masked_mean(on_losses_tokens, on_mask)\n    off_pg_loss = masked_mean(off_losses_tokens, off_mask)\n\n    # Off clip fraction currently not used in some implementations -> set to 0 if none applied; else  proportion of off tokens clipped by all_max/off_max/off_min\n    # Use fraction of tokens that were clamped by any of the supplied constraints\n    if off_mask.sum().item() == 0:\n        off_pg_clipfrac = 0.0\n    else:\n        clamp_hits = torch.zeros_like(off_mask, dtype=torch.float32)\n        if off_max_clip is not None:\n            clamp_hits = clamp_hits + ((off_ratio >= off_max_clip - 1e-12) & off_mask).to(torch.float32)\n        if off_min_clip is not None:\n            clamp_hits = clamp_hits + ((off_ratio <= off_min_clip + 1e-12) & off_mask).to(torch.float32)\n        if all_max_clip is not None:\n            clamp_hits = clamp_hits + ((off_ratio >= all_max_clip - 1e-12) & off_mask).to(torch.float32)\n        off_pg_clipfrac = (clamp_hits.sum() / off_mask.sum().clamp_min(1.0)).item()\n\n    # Total policy gradient loss\n    pg_loss = (on_pg_loss + off_pg_loss)\n\n    # Ensure all returns are python scalars or tensors consistently detached\n    if isinstance(pg_loss, torch.Tensor):\n        pg_loss = pg_loss.detach()\n    if isinstance(on_pg_loss, torch.Tensor):\n        on_pg_loss = on_pg_loss.detach()\n    if isinstance(off_pg_loss, torch.Tensor):\n        off_pg_loss = off_pg_loss.detach()\n\n    return {\n        \"pg_loss\": pg_loss,\n        \"off_pg_loss\": off_pg_loss,\n        \"on_pg_loss\": on_pg_loss,\n        \"off_pg_clipfrac\": float(off_pg_clipfrac),\n        \"on_pg_clipfrac\": float(on_pg_clipfrac),\n        \"ppo_kl\": float(ppo_kl.detach().item()) if isinstance(ppo_kl, torch.Tensor) else float(ppo_kl),\n        \"off_policy_prob\": float(off_policy_prob),\n        \"on_policy_prob\": float(on_policy_prob),\n        \"off_ratio_mean\": float(off_ratio_mean_val),\n        \"off_ratio_max_clip_frac\": float(off_ratio_max_clip_frac),\n        \"off_ratio_min_clip_frac\": float(off_ratio_min_clip_frac),\n    }\n```"], "usage": {"prompt_tokens": 1130, "completion_tokens": 4827, "total_tokens": 5957, "per_completion": [{"prompt_tokens": 1130, "completion_tokens": 4827, "total_tokens": 5957}]}, "results": [false], "pass_ratios": [0.21428571428571427]}
{"function_name": "compute_sft_pure_loss", "overview": "Computes pure SFT (Supervised Fine-Tuning) loss, i.e., negative log likelihood loss, used for supervised learning on off-policy samples in multi-task learning.", "function_signature": "", "input_parameters": "- `log_prob`: Model-predicted log probabilities with shape `[batch_size, response_length]`\n- `eos_mask`: Valid token mask with shape `[batch_size, response_length]`", "detailed_description": "Computes negative log probability for each token as loss value. Uses eos_mask to mask the loss, computing loss only at valid token positions. Averages the masked loss to obtain batch-level SFT loss.", "output": "- `sft_loss`: Scalar representing average SFT loss", "func_implementation": "", "test_code_path": "code/test_code/test_compute_sft_pure_loss.py", "implementation_location": "code/luffy/verl/verl/mix_src/mix_core_alg.py:line 7-10", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_sft_pure_loss`.\n\n**Function Overview:**\nComputes pure SFT (Supervised Fine-Tuning) loss, i.e., negative log likelihood loss, used for supervised learning on off-policy samples in multi-task learning.\n\n**Function Signature:**\n```python\n\n```\n\n**Input Parameters:**\n- `log_prob`: Model-predicted log probabilities with shape `[batch_size, response_length]`\n- `eos_mask`: Valid token mask with shape `[batch_size, response_length]`\n\n**Detailed Description:**\nComputes negative log probability for each token as loss value. Uses eos_mask to mask the loss, computing loss only at valid token positions. Averages the masked loss to obtain batch-level SFT loss.\n\n**Expected Output:**\n- `sft_loss`: Scalar representing average SFT loss\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def compute_sft_pure_loss(log_prob, eos_mask):\n    \"\"\"\n    Compute pure SFT (Supervised Fine-Tuning) loss as the negative log-likelihood\n    averaged over valid token positions.\n\n    Args:\n        log_prob (torch.Tensor): Log probabilities predicted by the model,\n            shape [batch_size, response_length].\n        eos_mask (torch.Tensor): Mask indicating valid tokens (1 for valid, 0 for invalid),\n            shape [batch_size, response_length]. Can be bool or numeric.\n\n    Returns:\n        torch.Tensor: Scalar tensor containing the average SFT loss.\n    \"\"\"\n    import torch\n\n    # Basic shape check\n    if log_prob.shape != eos_mask.shape:\n        raise ValueError(f\"log_prob and eos_mask must have the same shape, \"\n                         f\"got {log_prob.shape} and {eos_mask.shape}\")\n\n    # Ensure mask is float and on same device/dtype as log_prob for computations\n    mask = eos_mask.to(dtype=log_prob.dtype, device=log_prob.device)\n\n    # Per-token negative log-likelihood\n    per_token_loss = -log_prob\n\n    # Apply mask: only keep losses at valid token positions\n    masked_loss = per_token_loss * mask\n\n    # Sum masked loss and normalize by number of valid tokens\n    valid_count = mask.sum()\n\n    # If there are no valid tokens, return zero loss on the correct device/dtype\n    if valid_count.item() == 0:\n        return torch.tensor(0.0, device=log_prob.device, dtype=log_prob.dtype)\n\n    sft_loss = masked_loss.sum() / valid_count\n    return sft_loss"], "usage": {"prompt_tokens": 358, "completion_tokens": 855, "total_tokens": 1213, "per_completion": [{"prompt_tokens": 358, "completion_tokens": 855, "total_tokens": 1213}]}, "results": [false], "pass_ratios": [0.9375]}
