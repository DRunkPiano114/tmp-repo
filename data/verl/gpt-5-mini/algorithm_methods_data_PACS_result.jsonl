{"function_name": "compute_reward", "overview": "Computes \"reward\" for policy improvement, measuring log probability change of current policy relative to reference policy.", "function_signature": "def compute_reward(\nold_log_prob,\nlog_prob,\nresponse_mask,\nbeta=1.0,\nreward_method=\"1\",\n):", "input_parameters": "- `old_log_prob`: Reference policy token-level log probabilities, shape `[batch_size, response_length]`\n- `log_prob`: Current policy token-level log probabilities, shape `[batch_size, response_length]`\n- `response_mask`: Response portion mask, shape `[batch_size, response_length]`\n- `beta`: Reward scaling factor, controls aggressiveness of policy updates (default 1.0)\n- `reward_method`: Reward computation method, '1' or '2'", "detailed_description": "Function computes reward values in reinforcement learning based on specified method.\nMethod 1 computes sum of new-old log probability differences, measuring policy improvement magnitude.\nMethod 2 computes normalized sum of current log probabilities.\nIf an invalid method number is specified, function raises an exception.", "output": "- `reward`: Policy improvement reward, shape `[batch_size, 1]`", "func_implementation": "", "test_code_path": "code/test_code/test_compute_reward.py", "implementation_location": "code/src/pacs/pacs_core_algos.py:line 7-21", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_reward`.\n\n**Function Overview:**\nComputes \"reward\" for policy improvement, measuring log probability change of current policy relative to reference policy.\n\n**Function Signature:**\n```python\ndef compute_reward(\nold_log_prob,\nlog_prob,\nresponse_mask,\nbeta=1.0,\nreward_method=\"1\",\n):\n```\n\n**Input Parameters:**\n- `old_log_prob`: Reference policy token-level log probabilities, shape `[batch_size, response_length]`\n- `log_prob`: Current policy token-level log probabilities, shape `[batch_size, response_length]`\n- `response_mask`: Response portion mask, shape `[batch_size, response_length]`\n- `beta`: Reward scaling factor, controls aggressiveness of policy updates (default 1.0)\n- `reward_method`: Reward computation method, '1' or '2'\n\n**Detailed Description:**\nFunction computes reward values in reinforcement learning based on specified method.\nMethod 1 computes sum of new-old log probability differences, measuring policy improvement magnitude.\nMethod 2 computes normalized sum of current log probabilities.\nIf an invalid method number is specified, function raises an exception.\n\n**Expected Output:**\n- `reward`: Policy improvement reward, shape `[batch_size, 1]`\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["import torch\nimport numpy as np\n\ndef compute_reward(\n    old_log_prob,\n    log_prob,\n    response_mask,\n    beta=1.0,\n    reward_method=\"1\",\n):\n    \"\"\"\n    Compute reward for policy improvement.\n\n    Args:\n        old_log_prob: reference policy token-level log probs, shape [B, L]\n        log_prob: current policy token-level log probs, shape [B, L]\n        response_mask: response mask (0/1 or bool), shape [B, L]\n        beta: scaling factor\n        reward_method: '1' or '2' (or int 1/2)\n            - '1': reward = beta * sum_over_tokens( (log_prob - old_log_prob) * mask )\n            - '2': reward = beta * ( sum_over_tokens( log_prob * mask ) / sum(mask) )\n\n    Returns:\n        reward: tensor/ndarray of shape [B, 1]\n    \"\"\"\n    # Accept both string and int for reward_method\n    if isinstance(reward_method, str):\n        method = reward_method.strip()\n    else:\n        method = str(reward_method)\n\n    # Determine backend (torch or numpy)\n    if isinstance(log_prob, torch.Tensor) or isinstance(old_log_prob, torch.Tensor) or isinstance(response_mask, torch.Tensor):\n        # Use torch. Convert inputs to tensors if necessary.\n        old_log_prob = torch.as_tensor(old_log_prob) if not isinstance(old_log_prob, torch.Tensor) else old_log_prob\n        log_prob = torch.as_tensor(log_prob) if not isinstance(log_prob, torch.Tensor) else log_prob\n        response_mask = torch.as_tensor(response_mask) if not isinstance(response_mask, torch.Tensor) else response_mask\n\n        # Ensure same device and dtype for mask operations\n        mask = response_mask.to(dtype=log_prob.dtype, device=log_prob.device)\n\n        if method == \"1\":\n            # Sum of (new - old) logprobs over masked positions\n            diff = (log_prob - old_log_prob) * mask\n            reward = beta * diff.sum(dim=1, keepdim=True)\n        elif method == \"2\":\n            # Normalized sum of current logprobs over masked positions\n            masked_sum = (log_prob * mask).sum(dim=1, keepdim=True)\n            denom = mask.sum(dim=1, keepdim=True).clamp_min(1e-8)\n            reward = beta * (masked_sum / denom)\n        else:\n            raise ValueError(f\"Unsupported reward_method: {reward_method}. Expected '1' or '2'.\")\n        return reward\n\n    elif isinstance(log_prob, np.ndarray) or isinstance(old_log_prob, np.ndarray) or isinstance(response_mask, np.ndarray):\n        # Use numpy. Convert inputs to arrays if necessary.\n        old_log_prob = np.asarray(old_log_prob)\n        log_prob = np.asarray(log_prob)\n        response_mask = np.asarray(response_mask)\n\n        mask = response_mask.astype(log_prob.dtype)\n\n        if method == \"1\":\n            diff = (log_prob - old_log_prob) * mask\n            reward = beta * diff.sum(axis=1, keepdims=True)\n        elif method == \"2\":\n            masked_sum = (log_prob * mask).sum(axis=1, keepdims=True)\n            denom = mask.sum(axis=1, keepdims=True)\n            # avoid div by zero\n            denom = np.where(denom == 0, 1.0, denom)\n            reward = beta * (masked_sum / denom)\n        else:\n            raise ValueError(f\"Unsupported reward_method: {reward_method}. Expected '1' or '2'.\")\n        return reward\n\n    else:\n        raise TypeError(\"Inputs must be torch.Tensor or numpy.ndarray (at least one).\")"], "usage": {"prompt_tokens": 440, "completion_tokens": 1303, "total_tokens": 1743, "per_completion": [{"prompt_tokens": 440, "completion_tokens": 1303, "total_tokens": 1743}]}, "results": [false], "pass_ratios": [0.0]}
{"function_name": "compute_weight", "overview": "Computes sample weights to handle class imbalance problem (uneven positive/negative sample ratio).", "function_signature": "def compute_weight(score, rollout_n, mode=\"question\"):", "input_parameters": "- `score`: Verifiable rewards (labels), shape `[batch_size, 1]`\n- `rollout_n`: Number of responses generated per prompt\n- `mode`: Weight computation mode, 'question'/'only_positive'/'only_negative'", "detailed_description": "Function assigns training weights to samples based on different modes. In \"question\" mode, function groups samples by rollout_n size, using class-balanced weights for groups with label imbalance to make positive and negative samples contribute equally to training. In \"only_positive\" mode, only positive samples (label 1) are assigned weight 1.0, negative samples get weight 0. In \"only_negative\" mode, the opposite applies: only negative samples (label 0) are assigned weight 1.0. If an invalid mode is specified, function raises an exception.", "output": "- `weight`: Sample weights, shape `[batch_size, 1]`", "func_implementation": "", "test_code_path": "code/test_code/test_compute_weight.py", "implementation_location": "code/src/pacs/pacs_core_algos.py:line 59-86", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_weight`.\n\n**Function Overview:**\nComputes sample weights to handle class imbalance problem (uneven positive/negative sample ratio).\n\n**Function Signature:**\n```python\ndef compute_weight(score, rollout_n, mode=\"question\"):\n```\n\n**Input Parameters:**\n- `score`: Verifiable rewards (labels), shape `[batch_size, 1]`\n- `rollout_n`: Number of responses generated per prompt\n- `mode`: Weight computation mode, 'question'/'only_positive'/'only_negative'\n\n**Detailed Description:**\nFunction assigns training weights to samples based on different modes. In \"question\" mode, function groups samples by rollout_n size, using class-balanced weights for groups with label imbalance to make positive and negative samples contribute equally to training. In \"only_positive\" mode, only positive samples (label 1) are assigned weight 1.0, negative samples get weight 0. In \"only_negative\" mode, the opposite applies: only negative samples (label 0) are assigned weight 1.0. If an invalid mode is specified, function raises an exception.\n\n**Expected Output:**\n- `weight`: Sample weights, shape `[batch_size, 1]`\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def compute_weight(score, rollout_n, mode=\"question\"):\n    \"\"\"\n    Compute per-sample weights to handle class imbalance.\n\n    Args:\n        score: array-like or torch.Tensor of shape [batch_size, 1] or [batch_size], containing 0/1 labels.\n        rollout_n: int, number of responses generated per prompt (group size).\n        mode: one of \"question\", \"only_positive\", \"only_negative\".\n\n    Returns:\n        weight: same type as input (torch.Tensor or numpy.ndarray), shape [batch_size, 1], float weights.\n    \"\"\"\n    # Lazy import torch if available\n    try:\n        import torch\n        _has_torch = True\n    except Exception:\n        torch = None\n        _has_torch = False\n\n    # Validate mode\n    if mode not in (\"question\", \"only_positive\", \"only_negative\"):\n        raise ValueError(f\"Invalid mode '{mode}'. Must be one of 'question', 'only_positive', 'only_negative'.\")\n\n    # Normalize input to tensor/array and shape [batch_size]\n    is_torch_tensor = _has_torch and isinstance(score, torch.Tensor)\n    if is_torch_tensor:\n        s = score.detach()  # do not modify grad info\n        if s.dim() == 2 and s.size(1) == 1:\n            s = s.squeeze(1)\n        s = s.float()\n        batch_size = s.numel()\n    else:\n        import numpy as _np\n        s = _np.asarray(score)\n        if s.ndim == 2 and s.shape[1] == 1:\n            s = s.reshape(-1)\n        s = s.astype(_np.float32)\n        batch_size = s.shape[0]\n\n    # Validate rollout_n\n    if not (isinstance(rollout_n, int) and rollout_n >= 1):\n        raise ValueError(\"rollout_n must be an integer >= 1\")\n\n    if mode == \"only_positive\":\n        if is_torch_tensor:\n            w = (s == 1).float().unsqueeze(1)\n        else:\n            import numpy as _np\n            w = (_np.equal(s, 1).astype(_np.float32)).reshape(-1, 1)\n        return w\n\n    if mode == \"only_negative\":\n        if is_torch_tensor:\n            w = (s == 0).float().unsqueeze(1)\n        else:\n            import numpy as _np\n            w = (_np.equal(s, 0).astype(_np.float32)).reshape(-1, 1)\n        return w\n\n    # mode == \"question\"\n    if batch_size % rollout_n != 0:\n        raise ValueError(f\"batch_size ({batch_size}) must be divisible by rollout_n ({rollout_n}) for 'question' mode\")\n\n    num_groups = batch_size // rollout_n\n\n    if is_torch_tensor:\n        s_grouped = s.view(num_groups, rollout_n)  # shape [num_groups, rollout_n]\n        # count positives per group\n        pos_counts = s_grouped.sum(dim=1)  # shape [num_groups]\n        neg_counts = rollout_n - pos_counts\n\n        # Prepare weight tensor\n        weights_grouped = torch.ones_like(s_grouped, dtype=torch.float32)\n\n        for i in range(num_groups):\n            pos = pos_counts[i].item()\n            neg = neg_counts[i].item()\n            if pos > 0 and neg > 0:\n                w_pos = rollout_n / (2.0 * pos)\n                w_neg = rollout_n / (2.0 * neg)\n                # assign\n                group = s_grouped[i]\n                weights_grouped[i] = torch.where(group == 1, torch.tensor(w_pos, dtype=torch.float32), torch.tensor(w_neg, dtype=torch.float32))\n            else:\n                # If only one class present in group, keep weight = 1.0 for all members\n                weights_grouped[i] = torch.ones(rollout_n, dtype=torch.float32)\n\n        weights = weights_grouped.view(-1, 1)\n        return weights\n\n    else:\n        import numpy as _np\n        s_grouped = s.reshape(num_groups, rollout_n)  # shape [num_groups, rollout_n]\n        pos_counts = s_grouped.sum(axis=1)  # shape [num_groups]\n        neg_counts = rollout_n - pos_counts\n\n        weights_grouped = _np.ones_like(s_grouped, dtype=_np.float32)\n\n        for i in range(num_groups):\n            pos = float(pos_counts[i])\n            neg = float(neg_counts[i])\n            if pos > 0 and neg > 0:\n                w_pos = rollout_n / (2.0 * pos)\n                w_neg = rollout_n / (2.0 * neg)\n                group = s_grouped[i]\n                weights_grouped[i] = _np.where(group == 1, w_pos, w_neg)\n            else:\n                weights_grouped[i] = _np.ones(rollout_n, dtype=_np.float32)\n\n        weights = weights_grouped.reshape(-1, 1)\n        return weights"], "usage": {"prompt_tokens": 436, "completion_tokens": 1949, "total_tokens": 2385, "per_completion": [{"prompt_tokens": 436, "completion_tokens": 1949, "total_tokens": 2385}]}, "results": [false], "pass_ratios": [0.0]}
{"function_name": "compute_pacs_loss", "overview": "Computes PACS's core loss function, using advantage function as logit and verifiable rewards as labels with binary cross-entropy loss.", "function_signature": "def compute_pacs_loss(\nprompts,\nold_log_prob,\nlog_prob,\ntoken_level_scores,\nresponse_mask,\nrollout_n,\nalgorithm_config=None,\n):", "input_parameters": "- `prompts`: Input prompts, shape `[batch_size, prompt_length]`\n- `old_log_prob`: Reference policy log probabilities, shape `[batch_size, response_length]`\n- `log_prob`: Current policy log probabilities, shape `[batch_size, response_length]`\n- `token_level_scores`: Verifiable rewards (labels), shape `[batch_size, response_length]`\n- `response_mask`: Response mask, shape `[batch_size, response_length]`\n- `rollout_n`: Number of responses generated per prompt\n- `algorithm_config`: Algorithm configuration object containing `beta`, `reward_method`, `adv_estimator`, `use_weight`, `weight_mode`, etc.", "detailed_description": "Function needs to verify that every rollout_n samples correspond to the same prompt, then compute reward values and advantage values (based on specified advantage estimator). Finally, function sums token-level scores and uses binary cross-entropy loss with advantage values as supervision signal and scores as predicted values to compute loss. If sample weighting is enabled, balanced weights are computed based on score labels to handle sample imbalance.", "output": "- `pacs_loss`: PACS loss value, scalar", "func_implementation": "", "test_code_path": "code/test_code/test_compute_pacs_loss.py", "implementation_location": "code/src/pacs/pacs_core_algos.py:line 89-138", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_pacs_loss`.\n\n**Function Overview:**\nComputes PACS's core loss function, using advantage function as logit and verifiable rewards as labels with binary cross-entropy loss.\n\n**Function Signature:**\n```python\ndef compute_pacs_loss(\nprompts,\nold_log_prob,\nlog_prob,\ntoken_level_scores,\nresponse_mask,\nrollout_n,\nalgorithm_config=None,\n):\n```\n\n**Input Parameters:**\n- `prompts`: Input prompts, shape `[batch_size, prompt_length]`\n- `old_log_prob`: Reference policy log probabilities, shape `[batch_size, response_length]`\n- `log_prob`: Current policy log probabilities, shape `[batch_size, response_length]`\n- `token_level_scores`: Verifiable rewards (labels), shape `[batch_size, response_length]`\n- `response_mask`: Response mask, shape `[batch_size, response_length]`\n- `rollout_n`: Number of responses generated per prompt\n- `algorithm_config`: Algorithm configuration object containing `beta`, `reward_method`, `adv_estimator`, `use_weight`, `weight_mode`, etc.\n\n**Detailed Description:**\nFunction needs to verify that every rollout_n samples correspond to the same prompt, then compute reward values and advantage values (based on specified advantage estimator). Finally, function sums token-level scores and uses binary cross-entropy loss with advantage values as supervision signal and scores as predicted values to compute loss. If sample weighting is enabled, balanced weights are computed based on score labels to handle sample imbalance.\n\n**Expected Output:**\n- `pacs_loss`: PACS loss value, scalar\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["import torch\nimport torch.nn.functional as F\n\ndef compute_pacs_loss(\n    prompts,\n    old_log_prob,\n    log_prob,\n    token_level_scores,\n    response_mask,\n    rollout_n,\n    algorithm_config=None,\n):\n    \"\"\"\n    Compute PACS loss.\n\n    - Verifies that every group of `rollout_n` samples share the same prompt.\n    - Aggregates token-level verifiable rewards into per-response labels (mean or sum).\n    - Computes an advantage per response according to algorithm_config.adv_estimator.\n    - Uses binary cross-entropy with logits: logits = advantage, targets = aggregated scores.\n    - Optionally applies sample weighting to balance positive/negative examples.\n\n    Parameters\n    - prompts: LongTensor [batch_size, prompt_length]\n    - old_log_prob: FloatTensor [batch_size, response_length]\n    - log_prob: FloatTensor [batch_size, response_length]\n    - token_level_scores: FloatTensor [batch_size, response_length] (per-token verifiable reward/labels)\n    - response_mask: FloatTensor or BoolTensor [batch_size, response_length] (1 for valid tokens)\n    - rollout_n: int, number of responses per prompt\n    - algorithm_config: object with optional attributes:\n        - beta: float scaling factor for final loss (default 1.0)\n        - reward_method: 'mean' or 'sum' (how to aggregate token-level scores) (default 'mean')\n        - adv_estimator: one of:\n            'logprob'               -> advantage = sum((log_prob - old_log_prob) * mask)\n            'logprob_minus_group'   -> above, minus per-prompt-group mean (centering)\n            'reward'                -> advantage = aggregated reward\n            'reward_minus_group'    -> advantage = reward - per-prompt-group mean(reward)\n          (default: 'logprob_minus_group')\n        - use_weight: bool, whether to compute sample balancing weights (default False)\n        - weight_mode: currently only 'balanced' supported (default 'balanced')\n    Returns:\n    - pacs_loss: scalar tensor\n    \"\"\"\n\n    # Defaults for algorithm_config\n    class _Cfg:\n        beta = 1.0\n        reward_method = \"mean\"\n        adv_estimator = \"logprob_minus_group\"\n        use_weight = False\n        weight_mode = \"balanced\"\n\n    if algorithm_config is None:\n        cfg = _Cfg()\n    else:\n        # Create cfg object with defaults for missing attributes\n        cfg = _Cfg()\n        for k in [\"beta\", \"reward_method\", \"adv_estimator\", \"use_weight\", \"weight_mode\"]:\n            if hasattr(algorithm_config, k):\n                setattr(cfg, k, getattr(algorithm_config, k))\n\n    # Basic checks & device/dtype setup\n    if rollout_n <= 0:\n        raise ValueError(\"rollout_n must be > 0\")\n    if prompts is None:\n        raise ValueError(\"prompts must be provided\")\n\n    # Convert tensors to proper dtype/device\n    device = prompts.device\n    old_log_prob = old_log_prob.to(device)\n    log_prob = log_prob.to(device)\n    token_level_scores = token_level_scores.to(device)\n    response_mask = response_mask.to(device).float()\n\n    batch_size = prompts.shape[0]\n    if batch_size % rollout_n != 0:\n        raise ValueError(f\"batch_size ({batch_size}) must be divisible by rollout_n ({rollout_n})\")\n\n    # Verify groupings: each contiguous block of rollout_n rows correspond to same prompt\n    # (i.e., token ids equal)\n    num_groups = batch_size // rollout_n\n    for g in range(num_groups):\n        start = g * rollout_n\n        block = prompts[start : start + rollout_n]  # shape [rollout_n, prompt_length]\n        # Check all rows equal to the first row\n        first = block[0]\n        if not torch.all(block == first):\n            raise ValueError(f\"Prompts in group {g} (rows {start}-{start+rollout_n-1}) are not identical\")\n\n    # Aggregate token-level scores into per-response scalar labels\n    # reward_method: 'mean' (default) or 'sum'\n    if cfg.reward_method == \"sum\":\n        # Sum only over valid tokens\n        scores = (token_level_scores * response_mask).sum(dim=1)  # shape [batch]\n        # For numerical stability, optionally normalize? We'll leave raw sums.\n        # But BCE expects targets in [0,1]; best to convert to mean in typical setups.\n        # If user explicitly asked for sum, we trust they want raw sums (but clamp later).\n        # To be safe, we'll also clamp to [0,1] by dividing by max tokens (so it becomes fraction).\n        max_tokens = response_mask.sum(dim=1).clamp(min=1.0)\n        # Convert summed score to fraction of tokens that were positive (so in [0,1])\n        labels = (token_level_scores * response_mask).sum(dim=1) / max_tokens\n    else:\n        # default to mean over valid tokens -> label in [0,1]\n        denom = response_mask.sum(dim=1).clamp(min=1.0)\n        labels = (token_level_scores * response_mask).sum(dim=1) / denom  # shape [batch]\n\n    # Compute advantages according to estimator\n    # adv_estimator options: 'logprob', 'logprob_minus_group', 'reward', 'reward_minus_group'\n    # Compute token-wise logprob diffs\n    logprob_diff = ((log_prob - old_log_prob) * response_mask).sum(dim=1)  # [batch]\n\n    if cfg.adv_estimator == \"logprob\":\n        advantages = logprob_diff\n    elif cfg.adv_estimator == \"logprob_minus_group\":\n        # subtract group mean within each prompt group\n        advantages = logprob_diff.clone()\n        for g in range(num_groups):\n            start = g * rollout_n\n            end = start + rollout_n\n            group_mean = advantages[start:end].mean()\n            advantages[start:end] = advantages[start:end] - group_mean\n    elif cfg.adv_estimator == \"reward\":\n        advantages = labels.clone()\n    elif cfg.adv_estimator == \"reward_minus_group\":\n        advantages = labels.clone()\n        for g in range(num_groups):\n            start = g * rollout_n\n            end = start + rollout_n\n            gm = advantages[start:end].mean()\n            advantages[start:end] = advantages[start:end] - gm\n    else:\n        # fallback default: logprob minus group mean\n        advantages = logprob_diff.clone()\n        for g in range(num_groups):\n            start = g * rollout_n\n            end = start + rollout_n\n            group_mean = advantages[start:end].mean()\n            advantages[start:end] = advantages[start:end] - group_mean\n\n    # Prepare labels for BCE: should be in [0,1]. Ensure labels are in [0,1].\n    # It is expected token_level_scores are already in [0,1]. But clamp to [0,1] defensively.\n    targets = labels.clamp(min=0.0, max=1.0)\n\n    # Compute weights if requested\n    weight_tensor = None\n    if cfg.use_weight:\n        # Derive binary class membership for balancing. Consider positive if target > 0.5\n        binary_targets = (targets > 0.5).long()\n        n_total = binary_targets.numel()\n        n_pos = int(binary_targets.sum().item())\n        n_neg = n_total - n_pos\n\n        if n_pos == 0 or n_neg == 0:\n            # No balancing possible; fallback to uniform weights\n            weight_tensor = torch.ones_like(targets)\n        else:\n            # Balanced weights: weight = N / (2 * count[class])\n            weight_pos = n_total / (2.0 * n_pos)\n            weight_neg = n_total / (2.0 * n_neg)\n            weight_tensor = torch.where(binary_targets.bool(), torch.tensor(weight_pos, device=device), torch.tensor(weight_neg, device=device)).float()\n\n    # Compute BCE with logits: logits = advantages, targets = targets\n    # Use per-sample weight if provided; reduction mean\n    bce_loss = F.binary_cross_entropy_with_logits(advantages, targets, weight=weight_tensor, reduction=\"mean\")\n\n    # Apply beta scaling if provided\n    pacs_loss = cfg.beta * bce_loss\n\n    return pacs_loss"], "usage": {"prompt_tokens": 515, "completion_tokens": 3784, "total_tokens": 4299, "per_completion": [{"prompt_tokens": 515, "completion_tokens": 3784, "total_tokens": 4299}]}, "results": [false], "pass_ratios": [0.0]}
