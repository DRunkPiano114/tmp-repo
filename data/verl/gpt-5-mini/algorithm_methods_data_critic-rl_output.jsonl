{"function_name": "RewardModelForCritic.build_revision_messages", "overview": "Builds prompt messages for generating revised code. Combines original problem, initial solution, and critique into chat template format.", "function_signature": "def build_revision_messages(self, init_messages, critique):", "input_parameters": "- `init_messages`: Initial dialogue history list containing problem (user role) and initial solution (assistant role)\n- `critique`: Model-generated critique text string", "detailed_description": "Function receives initial dialogue history and critique text, validates that initial dialogue history is list type and last message role is assistant, calls related functions for content cleaning (keeping only code portion) and updating, then converts processed plain text critique with no leading/trailing whitespace and revision instruction \"Please finalize your answer accordingly using the same format.\" into new message with user role appended to end of dialogue.", "output": "- Complete dialogue message list (List[Dict]) containing original problem, cleaned initial code, and revision instruction", "func_implementation": "", "test_code_path": "code/test_code/test_build_revision_messages.py", "implementation_location": "code/ctrl/rl/critic_rm.py:line 136-150", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `RewardModelForCritic.build_revision_messages`.\n\n**Function Overview:**\nBuilds prompt messages for generating revised code. Combines original problem, initial solution, and critique into chat template format.\n\n**Function Signature:**\n```python\ndef build_revision_messages(self, init_messages, critique):\n```\n\n**Input Parameters:**\n- `init_messages`: Initial dialogue history list containing problem (user role) and initial solution (assistant role)\n- `critique`: Model-generated critique text string\n\n**Detailed Description:**\nFunction receives initial dialogue history and critique text, validates that initial dialogue history is list type and last message role is assistant, calls related functions for content cleaning (keeping only code portion) and updating, then converts processed plain text critique with no leading/trailing whitespace and revision instruction \"Please finalize your answer accordingly using the same format.\" into new message with user role appended to end of dialogue.\n\n**Expected Output:**\n- Complete dialogue message list (List[Dict]) containing original problem, cleaned initial code, and revision instruction\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def build_revision_messages(self, init_messages, critique):\n    \"\"\"\n    Build a dialogue message list for generating a revised solution.\n\n    Steps:\n    - Validate init_messages is a list and that the last message is from the assistant.\n    - Extract/keep only the code portion from the assistant's last message (if code fences are present).\n    - Append a new user message that contains the cleaned critique (stripped) plus a fixed\n      revision instruction: \"Please finalize your answer accordingly using the same format.\"\n    - Return the new list of messages (each as dicts with 'role' and 'content').\n\n    Parameters\n    - init_messages: list of dicts, each with at least 'role' and 'content'\n    - critique: str, model-generated critique text\n\n    Returns\n    - List[Dict]: new dialogue messages containing original problem(s), cleaned assistant code,\n      and the revision instruction message from user role.\n    \"\"\"\n    import re\n    import copy\n\n    # Basic validation\n    if not isinstance(init_messages, list):\n        raise TypeError(\"init_messages must be a list of message dicts.\")\n    if len(init_messages) == 0:\n        raise ValueError(\"init_messages must not be empty.\")\n    last_msg = init_messages[-1]\n    if not isinstance(last_msg, dict) or last_msg.get(\"role\") != \"assistant\":\n        raise ValueError(\"The last message in init_messages must be an assistant message.\")\n\n    # Work on a deep copy to avoid mutating the caller's data\n    messages = copy.deepcopy(init_messages)\n\n    # Helper to extract code from content using Markdown code fences.\n    # If multiple code blocks exist, join them with two newlines.\n    def _extract_code_from_content(content: str) -> str:\n        if not isinstance(content, str):\n            return \"\"\n        # Find all triple-backtick blocks, capturing inner content (non-greedy)\n        code_blocks = re.findall(r\"```(?:[\\w+-]*)\\n(.*?)```\", content, flags=re.S)\n        if code_blocks:\n            # Strip each block and join\n            cleaned_blocks = [block.strip() for block in code_blocks if block.strip() != \"\"]\n            return \"\\n\\n\".join(cleaned_blocks).strip()\n        # Fallback: if no code fences, attempt to heuristically find the largest indented/code-like section.\n        # Simple fallback: return the whole content stripped.\n        return content.strip()\n\n    # Replace assistant last message content with only the code portion\n    assistant_content = messages[-1].get(\"content\", \"\")\n    cleaned_code = _extract_code_from_content(assistant_content)\n    # If extraction yields empty but original content had text, keep original (strip)\n    if cleaned_code == \"\" and isinstance(assistant_content, str):\n        cleaned_code = assistant_content.strip()\n\n    messages[-1][\"content\"] = cleaned_code\n\n    # Prepare the critique + revision instruction as a new user message\n    critique_text = (critique or \"\").strip()\n    revision_instruction = \"Please finalize your answer accordingly using the same format.\"\n    if critique_text:\n        final_user_content = f\"{critique_text}\\n\\n{revision_instruction}\"\n    else:\n        final_user_content = revision_instruction\n\n    messages.append({\"role\": \"user\", \"content\": final_user_content})\n\n    return messages"], "usage": {"prompt_tokens": 398, "completion_tokens": 1411, "total_tokens": 1809, "per_completion": [{"prompt_tokens": 398, "completion_tokens": 1411, "total_tokens": 1809}]}}
{"function_name": "RewardModelForCritic.get_reward_all", "overview": "Concurrently executes sandbox tests for multiple critique-revision pairs, computing verifiable rewards. Improves execution efficiency through asynchronous I/O mechanism, supporting large-scale parallel evaluation.", "function_signature": "async def get_reward_all(\nself, critiques: List[str], revisions: List[str], samples: List[Any]\n) -> List[float]:", "input_parameters": "- `critiques`: Critique text list, each element corresponding to a generated critique\n- `revisions`: Revised code list, one-to-one correspondence with critiques\n- `samples`: Test sample list containing test cases and problem metadata", "detailed_description": "Function calls underlying reward computation method for each triplet (sample, critique, revision), evaluating improvement quality of revised version relative to original sample, returning numerical reward scores. All computation tasks execute asynchronously and concurrently, maximizing processing throughput while ensuring resource control, with default maximum concurrency set to MAX_REQUESTS (256), requiring progress bar (\"Generating rewards\") for real-time completion feedback.", "output": "- Float list, each element representing reward value for corresponding critique-revision pair, range [0.0, 1.0]", "func_implementation": "", "test_code_path": "code/test_code/test_get_reward_all.py", "implementation_location": "code/ctrl/rl/critic_rm.py:line 198-209", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `RewardModelForCritic.get_reward_all`.\n\n**Function Overview:**\nConcurrently executes sandbox tests for multiple critique-revision pairs, computing verifiable rewards. Improves execution efficiency through asynchronous I/O mechanism, supporting large-scale parallel evaluation.\n\n**Function Signature:**\n```python\nasync def get_reward_all(\nself, critiques: List[str], revisions: List[str], samples: List[Any]\n) -> List[float]:\n```\n\n**Input Parameters:**\n- `critiques`: Critique text list, each element corresponding to a generated critique\n- `revisions`: Revised code list, one-to-one correspondence with critiques\n- `samples`: Test sample list containing test cases and problem metadata\n\n**Detailed Description:**\nFunction calls underlying reward computation method for each triplet (sample, critique, revision), evaluating improvement quality of revised version relative to original sample, returning numerical reward scores. All computation tasks execute asynchronously and concurrently, maximizing processing throughput while ensuring resource control, with default maximum concurrency set to MAX_REQUESTS (256), requiring progress bar (\"Generating rewards\") for real-time completion feedback.\n\n**Expected Output:**\n- Float list, each element representing reward value for corresponding critique-revision pair, range [0.0, 1.0]\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["async def get_reward_all(\n    self, critiques: List[str], revisions: List[str], samples: List[Any]\n) -> List[float]:\n    \"\"\"\n    Concurrently compute rewards for lists of (sample, critique, revision) triplets.\n\n    Uses at most MAX_REQUESTS concurrent tasks (default 256). Shows a progress bar\n    labelled \"Generating rewards\". Each underlying single-item computation is\n    expected to be provided by self.get_reward(sample, critique, revision) which\n    can be either a coroutine or a synchronous function. Results are returned in\n    the same order as the inputs and are clamped to [0.0, 1.0].\n\n    Returns:\n        List[float]: reward values in range [0.0, 1.0] for each triplet.\n    \"\"\"\n    import asyncio\n    import inspect\n    import functools\n    from typing import Tuple\n    try:\n        from tqdm import tqdm\n    except Exception:\n        # Fallback simple progress indicator if tqdm is not available\n        tqdm = None\n\n    # Determine concurrency limit, allow class override via attribute if present\n    max_concurrency = getattr(self, \"MAX_REQUESTS\", 256) or 256\n    semaphore = asyncio.Semaphore(max_concurrency)\n\n    n = len(critiques)\n    if not (len(revisions) == n == len(samples)):\n        raise ValueError(\"critiques, revisions and samples must have the same length\")\n\n    # Prepare result container\n    results = [0.0] * n\n\n    # Helper to call underlying single-item reward function consistently\n    is_corofunc = asyncio.iscoroutinefunction(getattr(self, \"get_reward\", None))\n\n    async def _worker(idx: int, sample: Any, critique: str, revision: str) -> Tuple[int, float]:\n        await semaphore.acquire()\n        try:\n            # Call the underlying implementation appropriately (await if coroutine)\n            try:\n                if is_corofunc:\n                    val = await self.get_reward(sample, critique, revision)\n                else:\n                    loop = asyncio.get_running_loop()\n                    func = functools.partial(self.get_reward, sample, critique, revision)\n                    val = await loop.run_in_executor(None, func)\n                # Ensure numeric and clamp to [0.0, 1.0]\n                val = float(val)\n                if val != val:  # NaN guard\n                    val = 0.0\n                val = max(0.0, min(1.0, val))\n            except Exception:\n                # If any single computation fails, default to 0.0 (safe fallback)\n                val = 0.0\n        finally:\n            semaphore.release()\n        return idx, val\n\n    # Create all tasks\n    tasks = [asyncio.create_task(_worker(i, samples[i], critiques[i], revisions[i])) for i in range(n)]\n\n    # Progress bar setup\n    if tqdm is not None:\n        pbar = tqdm(total=n, desc=\"Generating rewards\")\n    else:\n        pbar = None\n        # simple counter for fallback\n        completed_count = 0\n\n    try:\n        for fut in asyncio.as_completed(tasks):\n            idx, val = await fut\n            results[idx] = float(val)\n            if pbar is not None:\n                pbar.update(1)\n            else:\n                completed_count += 1\n    finally:\n        if pbar is not None:\n            pbar.close()\n\n    return results"], "usage": {"prompt_tokens": 443, "completion_tokens": 2052, "total_tokens": 2495, "per_completion": [{"prompt_tokens": 443, "completion_tokens": 2052, "total_tokens": 2495}]}}
{"function_name": "sanitize", "overview": "Extracts pure code blocks from mixed text containing code and explanations, removes explanatory text, preserves markdown format code block structure.", "function_signature": "def sanitize(text):", "input_parameters": "- `text`: Mixed text string containing code blocks", "detailed_description": "Function uses regular expressions to identify and extract markdown format code blocks, matching code blocks wrapped in triple backticks through regex, recognizing cases with or without python identifier. Matching process is case-insensitive. After successful match, extracts content of first code block and reformats it to standard markdown format ````python\\n{code}\\n``` for return. If no code block found in text or code block is empty, directly returns original text.", "output": "- Formatted code block string (````python\\n{code}\\n```) or original text (when no code block)\nIn `build_revision_messages` function, used to clean model-generated initial solution, remove explanatory text, keep only code portion, ensuring conciseness of revision prompt.", "func_implementation": "", "test_code_path": "code/test_code/test_sanitize_desanitize.py", "implementation_location": "code/ctrl/rl/critic_rm.py:line 76-84", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `sanitize`.\n\n**Function Overview:**\nExtracts pure code blocks from mixed text containing code and explanations, removes explanatory text, preserves markdown format code block structure.\n\n**Function Signature:**\n```python\ndef sanitize(text):\n```\n\n**Input Parameters:**\n- `text`: Mixed text string containing code blocks\n\n**Detailed Description:**\nFunction uses regular expressions to identify and extract markdown format code blocks, matching code blocks wrapped in triple backticks through regex, recognizing cases with or without python identifier. Matching process is case-insensitive. After successful match, extracts content of first code block and reformats it to standard markdown format ````python\\n{code}\\n``` for return. If no code block found in text or code block is empty, directly returns original text.\n\n**Expected Output:**\n- Formatted code block string (````python\\n{code}\\n```) or original text (when no code block)\nIn `build_revision_messages` function, used to clean model-generated initial solution, remove explanatory text, keep only code portion, ensuring conciseness of revision prompt.\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def sanitize(text):\n    \"\"\"\n    Extract the first markdown code block (```...```) from the given text and\n    return it reformatted as a python markdown code block:\n        ```python\n        {code}\n        ```\n    If no code block is found or the code block is empty, return the original text.\n\n    The search is case-insensitive and supports optional language identifier\n    after the opening backticks.\n    \"\"\"\n    import re\n\n    if not isinstance(text, str):\n        # If input is not a string, keep behavior simple and return it unchanged\n        return text\n\n    # Match opening ``` optionally followed by a language identifier (non-newline),\n    # optional newline, then capture code content non-greedily until the next ```\n    pattern = re.compile(r\"```(?:\\s*([^\\n`]*))?\\n?(.*?)```\", re.DOTALL | re.IGNORECASE)\n    m = pattern.search(text)\n\n    if not m:\n        # No markdown code block found\n        return text\n\n    code_content = m.group(2) or \"\"\n    # Remove leading/trailing newline characters for a clean block but keep other whitespace\n    code_content = code_content.strip(\"\\n\")\n\n    if code_content == \"\":\n        # Empty code block -> return original text\n        return text\n\n    # Return standardized python code block\n    return f\"```python\\n{code_content}\\n```\""], "usage": {"prompt_tokens": 409, "completion_tokens": 923, "total_tokens": 1332, "per_completion": [{"prompt_tokens": 409, "completion_tokens": 923, "total_tokens": 1332}]}}
{"function_name": "desanitize", "overview": "Removes markdown code block markers (triple backticks) from code text, extracts pure code string.", "function_signature": "def desanitize(text):", "input_parameters": "- `text`: Text with or without markdown code block markers", "detailed_description": "Function uses regular expression to match code blocks wrapped in triple backticks, recognizing cases with or without python identifier. After successful match, extracts content inside code block, returns after removing leading/trailing whitespace. If no code block format found in text, directly returns original text.", "output": "- Pure code string with markdown markers removed\nIn `get_sandbox_response` function, used to extract pure code from revision text, remove markdown format markers, prepare code for sandbox execution.", "func_implementation": "", "test_code_path": "code/test_code/test_sanitize_desanitize.py", "implementation_location": "code/ctrl/rl/critic_rm.py:line 66-73", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `desanitize`.\n\n**Function Overview:**\nRemoves markdown code block markers (triple backticks) from code text, extracts pure code string.\n\n**Function Signature:**\n```python\ndef desanitize(text):\n```\n\n**Input Parameters:**\n- `text`: Text with or without markdown code block markers\n\n**Detailed Description:**\nFunction uses regular expression to match code blocks wrapped in triple backticks, recognizing cases with or without python identifier. After successful match, extracts content inside code block, returns after removing leading/trailing whitespace. If no code block format found in text, directly returns original text.\n\n**Expected Output:**\n- Pure code string with markdown markers removed\nIn `get_sandbox_response` function, used to extract pure code from revision text, remove markdown format markers, prepare code for sandbox execution.\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["import re\n\ndef desanitize(text):\n    \"\"\"\n    Remove Markdown code block markers (triple backticks) from the given text.\n    If a code block is found, return the content inside the first triple-backtick\n    block (with optional language identifier) with leading/trailing whitespace removed.\n    If no code block is present, return the original text unchanged.\n\n    Examples handled:\n      ```python\n      code here\n      ```\n      ```code()```\n      ```print(\"inline\")```\n    \"\"\"\n    if not isinstance(text, str):\n        # If input is not a string, return it unchanged (caller responsibility).\n        return text\n\n    # Match triple backticks, optionally followed by a language identifier (anything\n    # except a backtick or newline), optional newline, then capture anything\n    # non-greedily until the next triple backticks.\n    pattern = r'```(?:[^\\n`]*)\\n?(?P<code>.*?)```'\n    m = re.search(pattern, text, re.DOTALL)\n    if m:\n        return m.group('code').strip()\n    # No code block found; return original text unchanged.\n    return text"], "usage": {"prompt_tokens": 356, "completion_tokens": 1043, "total_tokens": 1399, "per_completion": [{"prompt_tokens": 356, "completion_tokens": 1043, "total_tokens": 1399}]}}
{"function_name": "normalize_code", "overview": "Normalizes Python code through Abstract Syntax Tree (AST) level transformation, unifies variable naming, improves cache hit rate.", "function_signature": "def normalize_code(code_str):", "input_parameters": "- `code_str`: Python code string to be normalized", "detailed_description": "Function uniformly renames variables in Python code to standard form (v_0, v_1, ...) through AST parsing, enabling functionally identical but differently named code to be recognized as equivalent. If code has syntax errors, function returns original code without transformation.", "output": "- Normalized code string, or original code (when parsing fails)\nDuring training, model may generate multiple semantically identical but differently named code versions. For example, `x = 1; return x` and `y = 1; return y` are functionally completely equivalent. Through normalization, both are converted to `v_0 = 1; return v_0`, using same cache key, avoiding repeated sandbox execution, significantly improving training efficiency.", "func_implementation": "", "test_code_path": "code/test_code/test_normalize_code.py", "implementation_location": "code/ctrl/rl/critic_rm.py:line 35-63", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `normalize_code`.\n\n**Function Overview:**\nNormalizes Python code through Abstract Syntax Tree (AST) level transformation, unifies variable naming, improves cache hit rate.\n\n**Function Signature:**\n```python\ndef normalize_code(code_str):\n```\n\n**Input Parameters:**\n- `code_str`: Python code string to be normalized\n\n**Detailed Description:**\nFunction uniformly renames variables in Python code to standard form (v_0, v_1, ...) through AST parsing, enabling functionally identical but differently named code to be recognized as equivalent. If code has syntax errors, function returns original code without transformation.\n\n**Expected Output:**\n- Normalized code string, or original code (when parsing fails)\nDuring training, model may generate multiple semantically identical but differently named code versions. For example, `x = 1; return x` and `y = 1; return y` are functionally completely equivalent. Through normalization, both are converted to `v_0 = 1; return v_0`, using same cache key, avoiding repeated sandbox execution, significantly improving training efficiency.\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport ast\n\ndef normalize_code(code_str):\n    \"\"\"\n    Normalize Python code by uniformly renaming variable bindings to v_0, v_1, ...\n    If parsing fails (syntax error), returns original code_str.\n\n    Rules / heuristics:\n    - Rename variable bindings such as:\n        * assignment targets (x = ...)\n        * annotated assignment targets (x: int = ...)\n        * augmented assignments (x += 1)\n        * for/async for targets\n        * with-as optional_vars\n        * comprehension targets\n        * function arguments\n        * function names (they are bindings in the outer scope)\n        * class names (they are bindings in the outer scope)\n        * exception handler names\n    - Do NOT rename import aliases (to avoid breaking module bindings).\n    - Do NOT rename special names: 'self', 'cls', names starting and ending with '__', or single '_' .\n    - When a binding is created we assign a new canonical name globally (v_0, v_1, ...),\n      and replace all Name occurrences that refer to that binding with the canonical name.\n    - If any AST parse/unparse fails, original code is returned.\n    \"\"\"\n\n    try:\n        tree = ast.parse(code_str)\n    except SyntaxError:\n        # If parsing fails, return original code unmodified\n        return code_str\n\n    class NormalizeTransformer(ast.NodeTransformer):\n        def __init__(self):\n            super().__init__()\n            self.counter = 0\n            # global map original_name -> canonical_name \"v_N\"\n            self.name_map = {}\n            # stack of scopes; each scope keeps a set of protected names (from global/nonlocal)\n            self.scope_stack = []\n            # helper set of names we should not rename\n            self.skip_names = {\"self\", \"cls\", \"_\"}\n        \n        def push_scope(self):\n            self.scope_stack.append({\"protected\": set()})\n        \n        def pop_scope(self):\n            self.scope_stack.pop()\n        \n        def current_scope_protected(self):\n            if not self.scope_stack:\n                return set()\n            return self.scope_stack[-1][\"protected\"]\n        \n        def make_canonical(self, orig_name):\n            # do not canonicalize certain names\n            if not orig_name:\n                return None\n            if orig_name in self.skip_names:\n                return None\n            if orig_name.startswith(\"__\") and orig_name.endswith(\"__\"):\n                return None\n            # If already mapped, return existing\n            if orig_name in self.name_map:\n                return self.name_map[orig_name]\n            # Otherwise assign a new canonical name\n            new_name = f\"v_{self.counter}\"\n            self.counter += 1\n            self.name_map[orig_name] = new_name\n            return new_name\n\n        # Helpers to collect binding names in complex targets (tuples, lists, etc.)\n        def collect_and_map_targets(self, node):\n            \"\"\"\n            Recursively walk assignment/target nodes to find Name nodes that are bindings (Store).\n            For each such name create a canonical mapping (unless protected/skip).\n            \"\"\"\n            if node is None:\n                return\n            if isinstance(node, ast.Name):\n                # Only consider Store/Del contexts as bindings\n                # (AnnAssign sometimes has target with ast.Name)\n                # We'll map regardless of ctx here, but ensure it's a binding context\n                ctx = getattr(node, \"ctx\", None)\n                # treat as binding if ctx is Store or Del or Param or None (for AnnAssign)\n                if isinstance(ctx, (ast.Store, ast.Del)) or isinstance(ctx, ast.Param) or ctx is None:\n                    orig = node.id\n                    # Do not map if this name is protected in current scope\n                    if orig in self.current_scope_protected():\n                        return\n                    self.make_canonical(orig)\n                return\n            # Tuple/List for unpacking\n            if isinstance(node, (ast.Tuple, ast.List)):\n                for elt in node.elts:\n                    self.collect_and_map_targets(elt)\n                return\n            # Attribute is not a simple name binding; skip it\n            if isinstance(node, ast.Attribute):\n                return\n            # Subscript target (e.g., a[i] = ...) is not a binding for a variable name\n            # so skip\n            if isinstance(node, ast.Subscript):\n                return\n            # For other node types, recursively visit their fields\n            for field, value in ast.iter_fields(node):\n                if isinstance(value, list):\n                    for item in value:\n                        if isinstance(item, ast.AST):\n                            self.collect_and_map_targets(item)\n                elif isinstance(value, ast.AST):\n                    self.collect_and_map_targets(value)\n\n        # Scope management\n        def visit_Module(self, node):\n            self.push_scope()\n            # do not protect any names at module level by default\n            self.generic_visit(node)\n            self.pop_scope()\n            return node\n\n        def visit_FunctionDef(self, node):\n            # The function name is a binding in the outer scope\n            # Map function name (unless special)\n            canonical = self.make_canonical(node.name)\n            if canonical:\n                node.name = canonical\n            # Enter function scope\n            self.push_scope()\n            # Process decorators (they are expressions)\n            node.decorator_list = [self.visit(d) for d in node.decorator_list]\n            # Process args: these are bindings in function scope\n            self._process_arguments(node.args)\n            # Handle possible 'global'/'nonlocal' statements within function body will be visited later\n            # Now visit body\n            node.body = [self.visit(n) for n in node.body]\n            # Process returns, type_comment etc via generic_visit for remaining fields\n            node.returns = self.visit(node.returns) if node.returns else None\n            self.pop_scope()\n            return node\n\n        def visit_AsyncFunctionDef(self, node):\n            # same as FunctionDef\n            canonical = self.make_canonical(node.name)\n            if canonical:\n                node.name = canonical\n            self.push_scope()\n            node.decorator_list = [self.visit(d) for d in node.decorator_list]\n            self._process_arguments(node.args)\n            node.body = [self.visit(n) for n in node.body]\n            node.returns = self.visit(node.returns) if node.returns else None\n            self.pop_scope()\n            return node\n\n        def visit_ClassDef(self, node):\n            # class name is a binding in the outer scope; map it\n            canonical = self.make_canonical(node.name)\n            if canonical:\n                node.name = canonical\n            # Enter class scope (though class body has its own locals)\n            self.push_scope()\n            node.bases = [self.visit(b) for b in node.bases]\n            node.keywords = [self.visit(k) for k in node.keywords]\n            node.decorator_list = [self.visit(d) for d in node.decorator_list]\n            node.body = [self.visit(n) for n in node.body]\n            self.pop_scope()\n            return node\n\n        def _process_arguments(self, args_node):\n            # args_node is ast.arguments\n            # Process all arg.arg names (positional, posonly, kwonly, vararg, kwarg)\n            for arg_list in (getattr(args_node, \"posonlyargs\", []),\n                             getattr(args_node, \"args\", []),\n                             getattr(args_node, \"kwonlyargs\", [])):\n                for a in arg_list:\n                    if isinstance(a, ast.arg):\n                        canonical = self.make_canonical(a.arg)\n                        if canonical:\n                            a.arg = canonical\n            # vararg and kwarg\n            if getattr(args_node, \"vararg\", None):\n                a = args_node.vararg\n                canonical = self.make_canonical(a.arg)\n                if canonical:\n                    a.arg = canonical\n            if getattr(args_node, \"kwarg\", None):\n                a = args_node.kwarg\n                canonical = self.make_canonical(a.arg)\n                if canonical:\n                    a.arg = canonical\n            # Visit annotations (types) if present\n            for field in (\"annotations\",):\n                pass\n            # No need to generic_visit here; the transformer will later visit the body where these args are used.\n\n        def visit_Global(self, node):\n            # mark these names as protected in current scope\n            for name in node.names:\n                if self.scope_stack:\n                    self.scope_stack[-1][\"protected\"].add(name)\n            return node\n\n        def visit_Nonlocal(self, node):\n            # mark these names as protected in current (inner) scope\n            for name in node.names:\n                if self.scope_stack:\n                    self.scope_stack[-1][\"protected\"].add(name)\n            return node\n\n        def visit_Assign(self, node):\n            # Collect bindings in targets and map them\n            for t in node.targets:\n                self.collect_and_map_targets(t)\n            # Skip mapping for imports: handled elsewhere\n            # Now visit value and other fields\n            node.value = self.visit(node.value) if node.value else None\n            node.targets = [self.visit(t) for t in node.targets]\n            return node\n\n        def visit_AnnAssign(self, node):\n            # annotated assignment: target is a binding if simple or Name\n            self.collect_and_map_targets(node.target)\n            node.target = self.visit(node.target)\n            node.annotation = self.visit(node.annotation) if node.annotation else None\n            node.value = self.visit(node.value) if node.value else None\n            return node\n\n        def visit_AugAssign(self, node):\n            self.collect_and_map_targets(node.target)\n            node.target = self.visit(node.target)\n            node.value = self.visit(node.value) if node.value else None\n            return node\n\n        def visit_For(self, node):\n            self.collect_and_map_targets(node.target)\n            node.target = self.visit(node.target)\n            node.iter = self.visit(node.iter)\n            node.body = [self.visit(n) for n in node.body]\n            node.orelse = [self.visit(n) for n in node.orelse]\n            return node\n\n        def visit_AsyncFor(self, node):\n            self.collect_and_map_targets(node.target)\n            node.target = self.visit(node.target)\n            node.iter = self.visit(node.iter)\n            node.body = [self.visit(n) for n in node.body]\n            node.orelse = [self.visit(n) for n in node.orelse]\n            return node\n\n        def visit_With(self, node):\n            # with items may have optional_vars which are bindings\n            for item in node.items:\n                if item.optional_vars:\n                    self.collect_and_map_targets(item.optional_vars)\n            # visit items and body\n            node.items = [self.visit(it) for it in node.items]\n            node.body = [self.visit(n) for n in node.body]\n            return node\n\n        def visit_comprehension(self, node):\n            # comprehension target is binding\n            self.collect_and_map_targets(node.target)\n            node.target = self.visit(node.target)\n            node.iter = self.visit(node.iter)\n            node.ifs = [self.visit(i) for i in node.ifs]\n            return node\n\n        def visit_ExceptHandler(self, node):\n            # ExceptHandler.name can be string or AST node depending on Python version\n            if hasattr(node, \"name\"):\n                # Python where name is a string\n                if isinstance(node.name, str):\n                    if node.name not in self.current_scope_protected():\n                        canonical = self.make_canonical(node.name)\n                        if canonical:\n                            node.name = canonical\n                elif isinstance(node.name, ast.AST):\n                    # AST node (older/newer variations); collect and map if Name\n                    self.collect_and_map_targets(node.name)\n                    node.name = self.visit(node.name)\n            node.type = self.visit(node.type) if node.type else None\n            node.body = [self.visit(n) for n in node.body]\n            return node\n\n        def visit_Import(self, node):\n            # Do not rename import alias bindings to avoid breaking external references\n            # Just visit the aliases' names as expressions (they are strings), skip mapping\n            return node  # leave as-is\n\n        def visit_ImportFrom(self, node):\n            # Same as Import: skip renaming import alias bindings\n            return node\n\n        def visit_Name(self, node):\n            # Replace name occurrences if we have a mapping\n            orig = node.id\n            # If this Name is part of an attribute or something else, it's still ast.Name;\n            # we'll replace only if we have a canonical mapping for orig\n            if orig in self.name_map:\n                node.id = self.name_map[orig]\n            return node\n\n        def visit_arg(self, node):\n            # arg nodes for function parameters: their .arg string might already have been mapped\n            # when processing arguments; but in case visit_arg is called independently, ensure mapping exists\n            if node.arg:\n                # If already mapped, replace; else, if it's a parameter (binding), create mapping\n                if node.arg in self.name_map:\n                    node.arg = self.name_map[node.arg]\n                else:\n                    canonical = self.make_canonical(node.arg)\n                    if canonical:\n                        node.arg = canonical\n            # visit annotation if present\n            node.annotation = self.visit(node.annotation) if getattr(node, \"annotation\", None) else None\n            return node\n\n        def generic_visit(self, node):\n            # For any other nodes, default traversal\n            return super().generic_visit(node)\n\n    transformer = NormalizeTransformer()\n    try:\n        new_tree = transformer.visit(tree)\n        ast.fix_missing_locations(new_tree)\n        # Convert AST back to code string. Use ast.unparse if available.\n        try:\n            normalized_code = ast.unparse(new_tree)\n        except Exception:\n            # As a fallback, try compile+inspect.getsource? But simplest: return original if unparse fails\n            return code_str\n        return normalized_code\n    except Exception:\n        # If transformation/unparse fails for any reason, return original code\n        return code_str\n```"], "usage": {"prompt_tokens": 412, "completion_tokens": 6002, "total_tokens": 6414, "per_completion": [{"prompt_tokens": 412, "completion_tokens": 6002, "total_tokens": 6414}]}}
