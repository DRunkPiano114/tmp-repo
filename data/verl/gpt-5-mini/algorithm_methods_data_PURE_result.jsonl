{"function_name": "ProcessRewardModelWorker._forward_micro_batch", "overview": "Computes token-level scores from the Process Reward Model (PRM) and performs approximate weighting for min-form credit assignment based on configuration.", "function_signature": "def _forward_micro_batch(self, micro_batch):", "input_parameters": "- `micro_batch`: Micro-batch data containing `input_ids`, `attention_mask`, `position_ids`, `responses`, `reward_mask`, etc.\n- `response_length`: Response sequence length (inferred from batch tensors)\n- `temperature`: Temperature for approximate min-form (when `reward_model.credit_assignment` is a float)", "detailed_description": "Performs forward pass on the Process Reward Model (PRM), computing token-level scores via softmax to obtain the positive-negative class probability difference. Uses masking to retain only valid scores from the response portion.\nIf depadding or sequence parallelism optimization is enabled, padding must be removed before computation and restored afterward.\nCredit assignment weighting executes different strategies based on configuration mode: min-form, strict min-form, or gamma-decay. Lower-scoring tokens should receive higher weights.", "output": "- `rm_score`: Token-level process reward scores (shape `[batch_size, response_length]`)", "func_implementation": "", "test_code_path": "code/test_code/test_forward_micro_batch.py", "implementation_location": "code/verl/workers/fsdp_workers.py:line 1462-1539", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `ProcessRewardModelWorker._forward_micro_batch`.\n\n**Function Overview:**\nComputes token-level scores from the Process Reward Model (PRM) and performs approximate weighting for min-form credit assignment based on configuration.\n\n**Function Signature:**\n```python\ndef _forward_micro_batch(self, micro_batch):\n```\n\n**Input Parameters:**\n- `micro_batch`: Micro-batch data containing `input_ids`, `attention_mask`, `position_ids`, `responses`, `reward_mask`, etc.\n- `response_length`: Response sequence length (inferred from batch tensors)\n- `temperature`: Temperature for approximate min-form (when `reward_model.credit_assignment` is a float)\n\n**Detailed Description:**\nPerforms forward pass on the Process Reward Model (PRM), computing token-level scores via softmax to obtain the positive-negative class probability difference. Uses masking to retain only valid scores from the response portion.\nIf depadding or sequence parallelism optimization is enabled, padding must be removed before computation and restored afterward.\nCredit assignment weighting executes different strategies based on configuration mode: min-form, strict min-form, or gamma-decay. Lower-scoring tokens should receive higher weights.\n\n**Expected Output:**\n- `rm_score`: Token-level process reward scores (shape `[batch_size, response_length]`)\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def _forward_micro_batch(self, micro_batch):\n    \"\"\"\n    Forward pass on the Process Reward Model (PRM) for a micro-batch.\n    Returns token-level process reward scores for the response portion\n    with approximate credit-assignment weighting applied.\n\n    Expected micro_batch keys (some optional):\n      - input_ids: [batch, seq_len]\n      - attention_mask: [batch, seq_len]\n      - position_ids: [batch, seq_len] (optional)\n      - responses: [batch, response_length] (optional)\n      - response_ids / response: alternative keys for responses (optional)\n      - reward_mask / response_mask / loss_mask: boolean mask with 1s for response tokens (optional)\n\n    Output:\n      - rm_score: torch.Tensor of shape [batch, response_length]\n    \"\"\"\n    import torch\n    import math\n\n    # --- helpers to read config/params from self robustly ---\n    cfg = getattr(self, \"config\", None)\n    # reward model config might live in different attributes; attempt common paths\n    rm_cfg = None\n    if cfg is not None and hasattr(cfg, \"reward_model\"):\n        rm_cfg = cfg.reward_model\n    else:\n        # fallbacks\n        rm_cfg = getattr(self, \"reward_model_cfg\", None) or getattr(self, \"reward_cfg\", None)\n\n    # credit assignment spec: can be float (temperature), string, or object with fields\n    credit_assignment = None\n    if rm_cfg is not None:\n        # try attribute names commonly used\n        credit_assignment = getattr(rm_cfg, \"credit_assignment\", None)\n    # also allow direct attribute on self\n    if credit_assignment is None:\n        credit_assignment = getattr(self, \"credit_assignment\", None)\n\n    # possible gamma parameter for gamma-decay\n    gamma_param = None\n    if rm_cfg is not None:\n        gamma_param = getattr(rm_cfg, \"gamma\", None)\n    if gamma_param is None:\n        gamma_param = getattr(self, \"gamma\", None)\n\n    # --- extract tensors from micro_batch ---\n    input_ids = micro_batch.get(\"input_ids\", None)\n    attention_mask = micro_batch.get(\"attention_mask\", None)\n    position_ids = micro_batch.get(\"position_ids\", None)\n\n    # response identification\n    responses = micro_batch.get(\"responses\", None)\n    if responses is None:\n        # common alternate names\n        responses = micro_batch.get(\"response_ids\", None) or micro_batch.get(\"response\", None)\n\n    # reward / response mask\n    reward_mask = micro_batch.get(\"reward_mask\", None)\n    if reward_mask is None:\n        reward_mask = micro_batch.get(\"response_mask\", None)\n    if reward_mask is None:\n        reward_mask = micro_batch.get(\"loss_mask\", None)\n\n    device = None\n    if input_ids is not None:\n        device = input_ids.device\n    elif attention_mask is not None:\n        device = attention_mask.device\n\n    # if we can't find any of these, raise\n    if input_ids is None:\n        raise ValueError(\"micro_batch must contain 'input_ids'\")\n\n    batch_size, seq_len = input_ids.shape\n\n    # infer response_length\n    if responses is not None:\n        response_length = responses.shape[1]\n    elif reward_mask is not None:\n        # assume mask is [batch, seq_len] where 1 indicates response tokens; but might vary per example.\n        # we assume consistent response_length across batch => take max\n        with torch.no_grad():\n            response_lengths = reward_mask.to(torch.long).sum(dim=1)\n            response_length = int(response_lengths.max().item())\n            if response_length == 0:\n                # no response tokens; return zeros\n                return torch.zeros((batch_size, 0), device=device, dtype=torch.float32)\n    else:\n        # fallback: try to infer response_length from position of some marker; default to 0\n        raise ValueError(\"micro_batch must contain 'responses' or 'reward_mask' to determine response_length\")\n\n    # --- forward PRM: handle possible depadding/sequence-parallel shortcuts ---\n    # If model provides a specialized depadding forward, prefer it.\n    model = self.model\n\n    model_inputs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n    if position_ids is not None:\n        model_inputs[\"position_ids\"] = position_ids\n\n    # prefer model.forward_depadding if available and self indicates depadding is desired\n    logits = None\n    if getattr(self, \"depadding\", False) and hasattr(model, \"forward_depadding\"):\n        # forward_depadding should take same inputs and return logits aligned to original tokens\n        logits = model.forward_depadding(**model_inputs)\n    else:\n        # normal forward\n        out = model(**model_inputs)\n        # model may return logits directly or a dict with 'logits'\n        if isinstance(out, dict):\n            logits = out.get(\"logits\", None)\n            if logits is None:\n                # try common alt names\n                logits = out.get(\"scores\", None)\n        else:\n            logits = out\n\n    if logits is None:\n        raise RuntimeError(\"Couldn't obtain logits from reward model forward pass\")\n\n    # Expected logits shape: [batch, seq_len, num_classes] OR [batch*seq_len, num_classes]\n    if logits.ndim == 2:\n        # flattened tokens; reshape back if possible\n        # assume flattening over batch*seq_len\n        num_classes = logits.shape[-1]\n        try:\n            logits = logits.view(batch_size, seq_len, num_classes)\n        except Exception:\n            # last resort: attempt to infer sequence dimension by dividing\n            raise RuntimeError(\"Unexpected flattened logits shape; cannot reshape to [batch, seq_len, classes]\")\n\n    if logits.ndim != 3:\n        raise RuntimeError(f\"Unexpected logits shape: {logits.shape}; expected [batch, seq_len, num_classes]\")\n\n    # convert logits -> probabilities over classes\n    probs = torch.softmax(logits, dim=-1)  # [batch, seq_len, num_classes]\n\n    # For binary classification (pos vs neg), we compute p_pos - p_neg. If >2 classes,\n    # try to interpret class 1 as positive and class 0 negative; otherwise fallback\n    num_classes = probs.shape[-1]\n    if num_classes >= 2:\n        # p_pos is last class (commonly) or class index 1\n        p_pos = probs[..., 1]\n        p_neg = probs[..., 0]\n        token_scores = p_pos - p_neg  # [batch, seq_len]\n    else:\n        # single-class case: treat raw logits as score\n        token_scores = logits.squeeze(-1)\n\n    # Ensure token_scores shape\n    token_scores = token_scores.view(batch_size, seq_len)\n\n    # --- isolate response tokens ---\n    if responses is not None:\n        # assume responses are the last response_length tokens in the sequence\n        # (common for generation where input consists of prompt + response)\n        if response_length > seq_len:\n            raise ValueError(\"response_length inferred from responses is larger than input sequence length\")\n        rm_scores_resp = token_scores[:, -response_length:].contiguous()  # [batch, response_length]\n        response_mask = torch.ones_like(rm_scores_resp, dtype=torch.bool, device=device)\n    else:\n        # use reward_mask to gather response tokens; reward_mask may differ per example length.\n        rw_mask = reward_mask.to(torch.bool)\n        # For ease, gather the rightmost response_length ones for each example into fixed-length tensor\n        rm_scores_resp = torch.zeros((batch_size, response_length), device=device, dtype=token_scores.dtype)\n        response_mask = torch.zeros_like(rm_scores_resp, dtype=torch.bool, device=device)\n        for i in range(batch_size):\n            idxs = torch.nonzero(rw_mask[i], as_tuple=False).squeeze(-1)\n            if idxs.numel() == 0:\n                continue\n            # take at most response_length tokens, preserve order\n            take = idxs[-response_length:]\n            # if fewer than response_length, right-align them\n            start = response_length - take.numel()\n            rm_scores_resp[i, start:] = token_scores[i, take]\n            response_mask[i, start:] = True\n\n    # Now rm_scores_resp: [batch, response_length]. response_mask indicates valid tokens in each row.\n\n    # --- credit assignment weighting ---\n    # Lower-scoring tokens should get higher weight.\n    # We will produce weights per-token (normalized per-example to sum to 1 across valid response tokens)\n    eps = 1e-12\n    weights = torch.zeros_like(rm_scores_resp, device=device, dtype=rm_scores_resp.dtype)\n\n    # convenience: masked scores where invalid positions are set to +inf so they won't get min or large weight\n    masked_scores = rm_scores_resp.clone()\n    masked_scores[~response_mask] = float(\"inf\")\n\n    # Case 1: credit_assignment is a float -> approximate min-form via softmin with temperature\n    if isinstance(credit_assignment, float):\n        temperature = float(credit_assignment)\n        if temperature <= 0:\n            # temperature <= 0 treated as strict min-form\n            credit_assignment = \"strict_min\"\n        else:\n            # softmin: weights proportional to exp(-score / T) over valid tokens\n            # To avoid overflow, subtract min before exp\n            valid_scores = masked_scores.clone()\n            valid_scores[~response_mask] = float(\"inf\")  # ensure masked positions not included\n            # For numerical stability, set large positive for invalid, then compute per-row\n            # compute per-row min over valid positions\n            min_val, _ = torch.min(valid_scores, dim=1, keepdim=True)  # [batch,1]\n            # replace inf rows (no valid tokens) with zeros afterwards\n            stable = torch.exp(-(valid_scores - min_val) / (temperature + eps))\n            stable[~response_mask] = 0.0\n            denom = stable.sum(dim=1, keepdim=True) + eps\n            weights = stable / denom\n\n    # Case 2: strict min-form: only the minimal tokens get weight (distributed among ties)\n    if isinstance(credit_assignment, str) and credit_assignment.lower() in (\"strict_min\", \"strict-min\", \"strictmin\"):\n        # For each row, find minimal score among valid tokens; give equal weight to ties\n        # If there are no valid tokens, weights remain zero\n        min_vals, _ = torch.min(masked_scores, dim=1, keepdim=True)  # inf if none valid\n        is_min = (masked_scores == min_vals) & response_mask\n        counts = is_min.sum(dim=1, keepdim=True).clamp(min=1)\n        weights = is_min.to(weights.dtype) / counts\n\n    # Case 3: min-form (string) -> use default temperature 1.0 softmin\n    if isinstance(credit_assignment, str) and credit_assignment.lower() in (\"min_form\", \"min-form\", \"minform\"):\n        temperature = 1.0\n        valid_scores = masked_scores.clone()\n        min_val, _ = torch.min(valid_scores, dim=1, keepdim=True)\n        stable = torch.exp(-(valid_scores - min_val) / (temperature + eps))\n        stable[~response_mask] = 0.0\n        denom = stable.sum(dim=1, keepdim=True) + eps\n        weights = stable / denom\n\n    # Case 4: gamma-decay: weights proportional to gamma^(rank-1) with rank ascending by score (lowest score rank=1)\n    if isinstance(credit_assignment, str) and credit_assignment.lower() in (\"gamma\", \"gamma_decay\", \"gamma-decay\"):\n        gamma = 0.9 if gamma_param is None else float(gamma_param)\n        # sort indices by ascending score\n        sorted_scores, sorted_idx = torch.sort(masked_scores, dim=1, stable=True)\n        batch_idx = torch.arange(batch_size, device=device).unsqueeze(1).expand_as(sorted_idx)\n        # ranks: positions in sorted order; compute weight per sorted position\n        ranks = torch.arange(1, response_length + 1, device=device, dtype=rm_scores_resp.dtype).unsqueeze(0)\n        weight_sorted = (gamma ** (ranks - 1))\n        # zero out positions that were originally masked (sorted_scores==inf)\n        valid_sorted = (sorted_scores != float(\"inf\")).to(rm_scores_resp.dtype)\n        weight_sorted = weight_sorted * valid_sorted\n        denom = weight_sorted.sum(dim=1, keepdim=True) + eps\n        weight_sorted = weight_sorted / denom  # normalized\n        # scatter back to original token order\n        weights = torch.zeros_like(weight_sorted)\n        weights[batch_idx, sorted_idx] = weight_sorted\n\n    # If weights are still all zeros (no credit_assignment match), fallback to uniform over valid tokens\n    if (weights.sum(dim=1) == 0).any():\n        # build uniform weights where response_mask is true\n        counts = response_mask.to(weights.dtype).sum(dim=1, keepdim=True).clamp(min=1)\n        uniform = response_mask.to(weights.dtype) / counts\n        # fill only rows where weights sum==0\n        zero_rows = (weights.sum(dim=1, keepdim=True) == 0)\n        weights = torch.where(zero_rows, uniform, weights)\n\n    # Final rm_score: apply weights so that tokens with lower scores get higher weight overall.\n    # Some users might expect raw scores; but description asked to \"perform approximate weighting\" so apply it.\n    rm_score = rm_scores_resp * weights\n\n    # Optionally mask out invalid positions to zero (should already be zero)\n    rm_score[~response_mask] = 0.0\n\n    return rm_score"], "usage": {"prompt_tokens": 450, "completion_tokens": 4676, "total_tokens": 5126, "per_completion": [{"prompt_tokens": 450, "completion_tokens": 4676, "total_tokens": 5126}]}, "results": [false], "pass_ratios": [0.0]}
{"function_name": "ProcessRewardModelWorker.compute_rm_score", "overview": "Batch-constructs PRM inputs (splitting by steps and appending newlines after each step), performs inference with (optional) variable-length sharding/backfilling, and aggregates to obtain batch-level token-level `rm_scores`.", "function_signature": "def compute_rm_score(self, data:DataProto):", "input_parameters": "- `data`: `DataProto` containing `responses`, `reward_mask`, etc.;", "detailed_description": "Splits response sequences by steps to construct input format for the process reward model.\nIn dynamic batch mode, reorganizes data into multiple mini-batches based on total token count limit and records original order; in fixed batch mode, evenly splits by sample count.\nCalls forward function for each mini-batch to compute token-level scores, concatenating results from all batches. In dynamic batch mode, restores results to original order based on recorded indices.", "output": "- `DataProto`: `batch['rm_scores']` with shape `[batch_size, response_length]`", "func_implementation": "", "test_code_path": "code/test_code/test_compute_rm_score.py", "implementation_location": "code/verl/workers/fsdp_workers.py:line 1541-1594", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `ProcessRewardModelWorker.compute_rm_score`.\n\n**Function Overview:**\nBatch-constructs PRM inputs (splitting by steps and appending newlines after each step), performs inference with (optional) variable-length sharding/backfilling, and aggregates to obtain batch-level token-level `rm_scores`.\n\n**Function Signature:**\n```python\ndef compute_rm_score(self, data:DataProto):\n```\n\n**Input Parameters:**\n- `data`: `DataProto` containing `responses`, `reward_mask`, etc.;\n\n**Detailed Description:**\nSplits response sequences by steps to construct input format for the process reward model.\nIn dynamic batch mode, reorganizes data into multiple mini-batches based on total token count limit and records original order; in fixed batch mode, evenly splits by sample count.\nCalls forward function for each mini-batch to compute token-level scores, concatenating results from all batches. In dynamic batch mode, restores results to original order based on recorded indices.\n\n**Expected Output:**\n- `DataProto`: `batch['rm_scores']` with shape `[batch_size, response_length]`\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def compute_rm_score(self, data: \"DataProto\"):\n    \"\"\"\n    Compute token-level reward model scores for a batch of responses.\n\n    Behavior:\n    - Attempts to split each response into \"steps\" using `reward_mask` if the\n      responses are token sequences (numeric arrays / tensors / lists of ints).\n      After each step we append a newline (if possible) to construct inputs\n      expected by the process reward model.\n    - Supports two batching modes:\n        * Dynamic batching by token count: accumulate mini-batches so the\n          total number of tokens in a mini-batch does not exceed\n          `self.max_tokens_per_batch` (if present, otherwise default).\n        * Fixed batching by sample count: evenly split by `self.fixed_batch_size`\n          (if present), or else send the whole batch at once.\n    - Calls a forward function to obtain token-level scores for each mini-batch.\n      The forward function is looked up in order: `self.forward_rm`,\n      `self.forward`, `self.model.forward`, or `self.model` if callable.\n      If none are available, returns zeros as scores.\n    - Concatenates mini-batch results and restores original order in dynamic\n      batching mode.\n    - Stores the final token-level scores into data['rm_scores'] with the same\n      first two dimensions as responses: [batch_size, response_length].\n\n    Notes:\n    - This implementation is defensive and accepts several response formats:\n      numpy arrays, torch tensors, lists of lists (token ids), or list of\n      strings. If step-level splitting is not possible due to format,\n      the entire response is used as one input to the RM.\n    \"\"\"\n\n    import numpy as _np\n    try:\n        import torch as _torch\n    except Exception:\n        _torch = None\n\n    # Helper to set result into DataProto\n    def _set_result(data_obj, key, value):\n        try:\n            data_obj[key] = value\n        except Exception:\n            setattr(data_obj, key, value)\n\n    # Extract responses and reward_mask\n    responses = getattr(data, \"responses\", None)\n    reward_mask = getattr(data, \"reward_mask\", None)\n\n    # If responses are missing, return early with no-op\n    if responses is None:\n        _set_result(data, \"rm_scores\", None)\n        return data\n\n    # Normalize responses into a list for uniform processing\n    # Supported formats:\n    # - numpy array / torch tensor of shape [B, L] (token ids)\n    # - list of lists (token ids)\n    # - list of strings (raw text)\n    is_tensor = False\n    is_numpy = False\n    is_list_of_lists = False\n    is_list_of_strings = False\n\n    # Detect types\n    if _torch is not None and _torch.is_tensor(responses):\n        is_tensor = True\n        resp_arr = responses\n        B = resp_arr.shape[0]\n    elif isinstance(responses, _np.ndarray):\n        is_numpy = True\n        resp_arr = responses\n        B = resp_arr.shape[0]\n    elif isinstance(responses, list):\n        B = len(responses)\n        if B == 0:\n            _set_result(data, \"rm_scores\", _np.zeros((0, 0), dtype=_np.float32))\n            return data\n        first = responses[0]\n        if isinstance(first, str):\n            is_list_of_strings = True\n        elif isinstance(first, (list, tuple)) or (hasattr(first, \"__iter__\") and not isinstance(first, (str, bytes))):\n            is_list_of_lists = True\n        else:\n            # fallback: single-dim list of tokens -> treat as 1 sample\n            if all(isinstance(x, (int,)) for x in responses):\n                # Make it a batch of 1\n                is_list_of_lists = True\n                responses = [responses]\n                B = 1\n            else:\n                # Unknown format: treat as strings\n                is_list_of_strings = True\n    else:\n        # Unknown type: try to coerce to string and treat as single sample\n        responses = [str(responses)]\n        is_list_of_strings = True\n        B = 1\n\n    # Determine response length (for output shape). If token ids 2D, use that.\n    if is_tensor or is_numpy:\n        L = int(resp_arr.shape[1])\n    elif is_list_of_lists:\n        # pad to max len\n        L = max((len(x) for x in responses), default=0)\n    elif is_list_of_strings:\n        # For strings we cannot decide token-level length; try to use reward_mask length if available\n        if reward_mask is not None:\n            try:\n                L = int(_np.asarray(reward_mask).shape[1])\n            except Exception:\n                # fallback: use length of string (characters) -- but that's unlikely desired\n                L = max((len(s) for s in responses), default=0)\n        else:\n            L = max((len(s) for s in responses), default=0)\n    else:\n        L = 0\n\n    # Decide batching strategy\n    dynamic_batching = getattr(self, \"dynamic_batching\", False)\n    max_tokens_per_batch = getattr(self, \"max_tokens_per_batch\", 4096)\n    fixed_batch_size = getattr(self, \"fixed_batch_size\", None)\n\n    # Build input units: For each sample, we want one string or token list that will be\n    # fed to the RM forward function. If possible, split by steps (using reward_mask)\n    # appending newlines after each step. If not possible, feed the entire response.\n    per_sample_inputs = []  # each element: either list[int] token ids or str (text)\n    per_sample_orig_idx = list(range(B))\n\n    # helper to append newline to token list or string\n    def _append_newline_to_tokens(tok_list):\n        # Try to append a newline token id using tokenizer if available.\n        if hasattr(self, \"tokenizer\") and callable(getattr(self.tokenizer, \"encode\", None)):\n            nl_tok = self.tokenizer.encode(\"\\n\")\n            # tokenizer.encode may return a list; we want token ids\n            if isinstance(nl_tok, list):\n                tok_list = tok_list + nl_tok\n            else:\n                tok_list = tok_list + [nl_tok]\n            return tok_list\n        else:\n            # no tokenizer: just append a sentinel token id (e.g., newline ASCII 10)\n            tok_list = list(tok_list) + [10]\n            return tok_list\n\n    def _append_newline_to_text(text):\n        return text + \"\\n\"\n\n    # If we have token-level sequences and a reward_mask of same shape, we can split into steps.\n    try:\n        if (is_tensor or is_numpy or is_list_of_lists) and (reward_mask is not None):\n            # Convert reward_mask to numpy array for indexing\n            mask_arr = _np.asarray(reward_mask)\n            # If mask is 1D per sample or 2D\n            if mask_arr.ndim == 1 and mask_arr.shape[0] == B:\n                # Interpreted as \"number of steps\" per sample - cannot split; fallback\n                mask_arr = None\n            elif mask_arr.ndim == 2 and mask_arr.shape[0] == B:\n                # mask_arr shape [B, L_mask] expected to match response token length.\n                pass\n            else:\n                # incompatible mask; ignore splitting\n                mask_arr = None\n        else:\n            mask_arr = None\n    except Exception:\n        mask_arr = None\n\n    # Fill per_sample_inputs\n    if mask_arr is not None:\n        # We assume mask_arr contains step-boundary indicators per token (True at end-of-step)\n        for i in range(B):\n            # Obtain token sequence for this sample\n            if is_tensor:\n                seq = resp_arr[i].cpu().tolist() if hasattr(resp_arr[i], \"cpu\") else resp_arr[i].tolist()\n            elif is_numpy:\n                seq = resp_arr[i].tolist()\n            else:  # list_of_lists\n                seq = list(responses[i])\n\n            # If mask length differs from seq length, trim/pad mask to seq length\n            m = mask_arr[i]\n            if len(m) != len(seq):\n                m = m[: len(seq)]\n                if len(m) < len(seq):\n                    # pad with False\n                    m = _np.concatenate([m, _np.zeros(len(seq) - len(m), dtype=bool)])\n\n            steps = []\n            cur = []\n            for tok, end_flag in zip(seq, m):\n                cur.append(tok)\n                if bool(end_flag):\n                    # end of step: append newline and flush\n                    cur_with_nl = _append_newline_to_tokens(cur)\n                    steps.extend(cur_with_nl)\n                    cur = []\n            # Any leftover tokens (no end flag) -> include as is\n            if len(cur) > 0:\n                # treat leftover as final step (append newline as well)\n                cur_with_nl = _append_newline_to_tokens(cur)\n                steps.extend(cur_with_nl)\n            per_sample_inputs.append(steps)\n    else:\n        # No valid mask usable for splitting; construct inputs conservatively.\n        if is_list_of_strings:\n            # Use full strings as inputs (optionally append newline if wanted)\n            for s in responses:\n                per_sample_inputs.append(_append_newline_to_text(s))\n        elif is_tensor or is_numpy or is_list_of_lists:\n            for i in range(B):\n                if is_tensor:\n                    seq = resp_arr[i].cpu().tolist() if hasattr(resp_arr[i], \"cpu\") else resp_arr[i].tolist()\n                elif is_numpy:\n                    seq = resp_arr[i].tolist()\n                else:\n                    seq = list(responses[i])\n                # append newline token/sentinel\n                seq_with_nl = _append_newline_to_tokens(seq)\n                per_sample_inputs.append(seq_with_nl)\n        else:\n            # fallback: string coercion\n            for i in range(B):\n                s = str(responses[i])\n                per_sample_inputs.append(_append_newline_to_text(s))\n\n    # Prepare mini-batches\n    # For dynamic batching, accumulate until token count exceeds limit\n    indices = list(range(B))\n    minibatches = []  # list of (indices_in_batch, inputs_in_batch)\n    if dynamic_batching:\n        cur_idxs = []\n        cur_tok_count = 0\n        for idx in indices:\n            inp = per_sample_inputs[idx]\n            # token count: length of token list if tokenized, else estimate via len(string)\n            tok_count = len(inp) if not isinstance(inp, str) else len(inp.split())\n            if len(cur_idxs) > 0 and (cur_tok_count + tok_count > max_tokens_per_batch):\n                minibatches.append((cur_idxs, [per_sample_inputs[i] for i in cur_idxs]))\n                cur_idxs = [idx]\n                cur_tok_count = tok_count\n            else:\n                cur_idxs.append(idx)\n                cur_tok_count += tok_count\n        if len(cur_idxs) > 0:\n            minibatches.append((cur_idxs, [per_sample_inputs[i] for i in cur_idxs]))\n    else:\n        if fixed_batch_size is not None and fixed_batch_size > 0:\n            # split by sample count\n            for start in range(0, B, fixed_batch_size):\n                idxs = list(range(start, min(start + fixed_batch_size, B)))\n                minibatches.append((idxs, [per_sample_inputs[i] for i in idxs]))\n        else:\n            # All samples in one batch\n            minibatches.append((indices, per_sample_inputs))\n\n    # Find a forward function\n    forward_fn = None\n    for name in (\"forward_rm\", \"forward\", \"model_forward\", \"predict\"):\n        if hasattr(self, name) and callable(getattr(self, name)):\n            forward_fn = getattr(self, name)\n            break\n    if forward_fn is None:\n        # try self.model\n        model_obj = getattr(self, \"model\", None)\n        if model_obj is not None and callable(model_obj):\n            forward_fn = model_obj\n        elif model_obj is not None and hasattr(model_obj, \"forward\") and callable(getattr(model_obj, \"forward\")):\n            forward_fn = model_obj.forward\n\n    # Function to call forward and normalize output to numpy array [batch, L_i]\n    def _call_forward(batch_inputs):\n        # If no forward, return zeros per input length\n        if forward_fn is None:\n            outs = []\n            for inp in batch_inputs:\n                if isinstance(inp, str):\n                    length = len(inp.split())\n                else:\n                    length = len(inp)\n                outs.append(_np.zeros((length,), dtype=_np.float32))\n            return outs\n\n        # Try calling forward directly; it may accept token ids or strings depending on model\n        try:\n            raw_out = forward_fn(batch_inputs)\n            # raw_out may be:\n            # - list/tuple of sequences, one per input\n            # - numpy array shape [B, L]\n            # - torch tensor shape [B, L]\n            if raw_out is None:\n                # Fallback zeros\n                outs = []\n                for inp in batch_inputs:\n                    length = len(inp) if not isinstance(inp, str) else len(inp.split())\n                    outs.append(_np.zeros((length,), dtype=_np.float32))\n                return outs\n\n            if _torch is not None and _torch.is_tensor(raw_out):\n                arr = raw_out.detach().cpu().numpy()\n                # arr shape [B, L]\n                return [arr[i] for i in range(arr.shape[0])]\n\n            if isinstance(raw_out, _np.ndarray):\n                return [raw_out[i] for i in range(raw_out.shape[0])]\n\n            # If list-like, return as-is but ensure numpy arrays\n            if isinstance(raw_out, (list, tuple)):\n                outs = []\n                for item in raw_out:\n                    if item is None:\n                        outs.append(_np.zeros((0,), dtype=_np.float32))\n                    elif _torch is not None and _torch.is_tensor(item):\n                        outs.append(item.detach().cpu().numpy())\n                    elif isinstance(item, _np.ndarray):\n                        outs.append(item)\n                    elif isinstance(item, (list, tuple)):\n                        outs.append(_np.asarray(item, dtype=_np.float32))\n                    else:\n                        # unsupported type: try to coerce scalar -> zeros\n                        try:\n                            outs.append(_np.asarray(item, dtype=_np.float32))\n                        except Exception:\n                            outs.append(_np.zeros((0,), dtype=_np.float32))\n                return outs\n\n            # Unknown raw_out type: fallback zeros\n            outs = []\n            for inp in batch_inputs:\n                length = len(inp) if not isinstance(inp, str) else len(inp.split())\n                outs.append(_np.zeros((length,), dtype=_np.float32))\n            return outs\n        except Exception:\n            # If forward call fails, return zeros\n            outs = []\n            for inp in batch_inputs:\n                length = len(inp) if not isinstance(inp, str) else len(inp.split())\n                outs.append(_np.zeros((length,), dtype=_np.float32))\n            return outs\n\n    # Run forward on each minibatch and gather outputs in order\n    # We'll create an output list of length B where each entry is a 1D numpy array of token scores\n    outputs_by_index = [None] * B\n    for idxs, batch_inputs in minibatches:\n        batch_outputs = _call_forward(batch_inputs)\n        # Ensure batch_outputs length matches batch_inputs\n        if len(batch_outputs) != len(idxs):\n            # try to broadcast if raw output was [B, L] flattened earlier\n            # fallback: align by min length\n            minlen = min(len(batch_outputs), len(idxs))\n            for j in range(minlen):\n                outputs_by_index[idxs[j]] = _np.asarray(batch_outputs[j], dtype=_np.float32)\n            for j in range(minlen, len(idxs)):\n                # fill missing with zeros\n                inp = batch_inputs[j]\n                length = len(inp) if not isinstance(inp, str) else len(inp.split())\n                outputs_by_index[idxs[j]] = _np.zeros((length,), dtype=_np.float32)\n        else:\n            for j, out in enumerate(batch_outputs):\n                outputs_by_index[idxs[j]] = _np.asarray(out, dtype=_np.float32)\n\n    # Build final rm_scores array with shape [B, L] (response_length)\n    # For each sample, we may have token-level scores corresponding to the constructed\n    # per_sample_inputs which may be longer than original L (because we appended newline tokens).\n    # We'll truncate/pad to original L.\n    rm_scores = _np.zeros((B, L), dtype=_np.float32)\n    for i in range(B):\n        out = outputs_by_index[i]\n        if out is None:\n            continue\n        # out length may differ: align by truncating or padding with zeros\n        take = min(len(out), L)\n        if take > 0:\n            rm_scores[i, :take] = out[:take]\n        # if out longer than L, ignore the rest; if shorter, remaining positions remain 0\n\n    # Store into data\n    _set_result(data, \"rm_scores\", rm_scores)\n\n    return data"], "usage": {"prompt_tokens": 416, "completion_tokens": 5264, "total_tokens": 5680, "per_completion": [{"prompt_tokens": 416, "completion_tokens": 5264, "total_tokens": 5680}]}, "results": [true], "pass_ratios": [0.7142857142857143]}
{"function_name": "compute_return", "overview": "Computes token-level return sequences based on `method`", "function_signature": "def compute_return(token_level_rewards, eos_mask, method='sum', gamma=1.0):", "input_parameters": "- `token_level_rewards`: `[batch_size, response_length]`\n- `eos_mask`: `[batch_size, response_length]`\n- `method`: `'sum'|'min'`\n- `gamma`: Discount factor (applicable in `'sum'` mode)", "detailed_description": "Computes token-level return sequences based on method, using masking to ensure only valid positions are processed.\n\"method\" includes \"sum\" and \"min\".", "output": "- `returns`: `[batch_size, response_length]`", "func_implementation": "", "test_code_path": "code/test_code/test_compute_return.py", "implementation_location": "code/verl/trainer/ppo/core_algos.py:line 70-88", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_return`.\n\n**Function Overview:**\nComputes token-level return sequences based on `method`\n\n**Function Signature:**\n```python\ndef compute_return(token_level_rewards, eos_mask, method='sum', gamma=1.0):\n```\n\n**Input Parameters:**\n- `token_level_rewards`: `[batch_size, response_length]`\n- `eos_mask`: `[batch_size, response_length]`\n- `method`: `'sum'|'min'`\n- `gamma`: Discount factor (applicable in `'sum'` mode)\n\n**Detailed Description:**\nComputes token-level return sequences based on method, using masking to ensure only valid positions are processed.\n\"method\" includes \"sum\" and \"min\".\n\n**Expected Output:**\n- `returns`: `[batch_size, response_length]`\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport torch\n\ndef compute_return(token_level_rewards, eos_mask, method='sum', gamma=1.0):\n    \"\"\"\n    Compute token-level returns for sequences.\n\n    Args:\n        token_level_rewards (torch.Tensor): shape [B, L], per-token rewards.\n        eos_mask (torch.Tensor): shape [B, L], mask indicating valid positions (truthy for valid tokens).\n        method (str): 'sum' or 'min'.\n            - 'sum': discounted sum of future rewards (R_t = r_t + gamma * r_{t+1} + ...),\n                     accumulation stops where eos_mask is False.\n            - 'min': running minimum of future rewards (R_t = min_{k>=t and valid} r_k),\n                     reset where eos_mask is False.\n        gamma (float): discount factor used for 'sum' method.\n\n    Returns:\n        torch.Tensor: returns of shape [B, L], same dtype/device as token_level_rewards.\n    \"\"\"\n    if method not in ('sum', 'min'):\n        raise ValueError(f\"Unsupported method: {method}. Choose 'sum' or 'min'.\")\n\n    rewards = token_level_rewards\n    mask = eos_mask.bool()\n\n    if rewards.dim() != 2 or mask.dim() != 2:\n        raise ValueError(\"token_level_rewards and eos_mask must be 2D tensors [batch, length].\")\n    if rewards.shape != mask.shape:\n        raise ValueError(\"token_level_rewards and eos_mask must have the same shape.\")\n\n    B, L = rewards.shape\n    returns = torch.zeros_like(rewards)\n\n    if method == 'sum':\n        # Reverse discounted cumulative sum with reset where mask is False\n        # running_return is scalar per sequence\n        running_return = torch.zeros(B, dtype=rewards.dtype, device=rewards.device)\n        for t in range(L - 1, -1, -1):\n            # If position not valid, reset running_return to 0 and output 0\n            # Otherwise accumulate: r_t + gamma * running_return\n            valid = mask[:, t]\n            running_return = running_return * gamma\n            running_return = torch.where(valid, running_return + rewards[:, t], torch.zeros_like(running_return))\n            returns[:, t] = running_return\n\n    else:  # method == 'min'\n        # Reverse running minimum with reset where mask is False\n        inf = torch.tensor(float('inf'), dtype=rewards.dtype, device=rewards.device)\n        running_min = torch.full((B,), float('inf'), dtype=rewards.dtype, device=rewards.device)\n        for t in range(L - 1, -1, -1):\n            valid = mask[:, t]\n            running_min = torch.where(valid, torch.min(running_min, rewards[:, t]), inf)\n            # For invalid positions, we want return 0 (not +inf)\n            returns[:, t] = torch.where(valid, running_min, torch.zeros_like(running_min))\n\n    return returns\n```"], "usage": {"prompt_tokens": 354, "completion_tokens": 1488, "total_tokens": 1842, "per_completion": [{"prompt_tokens": 354, "completion_tokens": 1488, "total_tokens": 1842}]}, "results": [false], "pass_ratios": [0.9]}
