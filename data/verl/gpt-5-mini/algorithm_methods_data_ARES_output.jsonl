{"function_name": "compute_score", "overview": "Computes ARES hierarchical entropy rewards, dynamically adjusting reward signals based on sample difficulty, accuracy, and High Window Entropy (HWE) token count.", "function_signature": "def compute_score(reward_inputs: List[Dict[str, Any]], format_weight: float = 0.1, alpha_length_bonus: float = 0.5, alpha_entropy: float = 0.5) -> List[Dict[str, float]]:", "input_parameters": "- `reward_inputs`: List of reward input dictionaries, each containing:\n- `response`: Model-generated response text\n- `ground_truth`: Ground truth answer\n- `difficulty`: Sample difficulty label (\"easy\" / \"medium\" / \"hard\")\n- `high_entropy_token_num`: Number of HWE tokens in response\n- `target_high_entropy_token_num`: Target HWE token count (difficulty-related)\n- `alpha_entropy`: Entropy reward coefficient (difficulty-related)\n- `accuracy`: Accuracy label (optional, if not provided, extracted from response and evaluated)\n- `format_weight`: Format reward weight, default 0.1\n- `alpha_length_bonus`: Length reward coefficient (legacy parameter, actual usage is alpha_entropy)\n- `alpha_entropy`: Default entropy reward coefficient, default 0.5 (used if sample doesn't specify)", "detailed_description": "This function implements ARES's core reward shaping mechanism, adaptively adjusting reasoning resource allocation based on task difficulty and model performance. The function computes format reward, accuracy reward, and entropy reward. For easy questions, when the answer is correct, it penalizes overly long reasoning chains, while when the answer is incorrect, it provides moderate positive rewards to encourage the model to explore more possibilities. Medium-difficulty questions require reasoning length closer to the target value; both excessive and insufficient reasoning are penalized when the answer is correct, but longer reasoning is still encouraged when the answer is incorrect. Hard questions adopt the opposite strategy: when the answer is correct, reasoning chains close to or exceeding the target are rewarded, and penalties are only applied when reasoning is significantly insufficient; when the answer is incorrect, longer reasoning is also encouraged to find the correct path.\nThe entire reward mechanism uses Huber loss function to smoothly handle penalty terms, Sigmoid function to achieve reward saturation effects, and sets different tolerance boundaries and reward caps for different difficulties.\nDifficulty-related hyperparameters that may be used:\n- `MARGIN_FRAC`: Tolerance band width (easy: 15%, medium: 25%, hard: 35%)\n- `KAPPA`: Huber function inflection point (easy: 2.0, medium: 3.0, hard: 4.0)\n- `TEMP`: Sigmoid temperature parameter (easy: 2.0, medium: 2.5, hard: 3.0)\n- `CAP_SCALE`: Entropy reward cap coefficient (easy: 1.0, medium: 1.0, hard: 1.2)", "output": "- `scores`: List of score dictionaries, each containing:\n- `overall`: Total score = accuracy + entropy_score\n- `accuracy`: Accuracy score (0.0 or 1.0)\n- `format`: Format score (0.0 or 1.0)\n- `high_entropy_token_num_score`: Entropy reward score", "func_implementation": "", "test_code_path": "code/test_code/test_compute_score.py", "implementation_location": "code/examples/reward_function/aepo.py:line 58-133", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_score`.\n\n**Function Overview:**\nComputes ARES hierarchical entropy rewards, dynamically adjusting reward signals based on sample difficulty, accuracy, and High Window Entropy (HWE) token count.\n\n**Function Signature:**\n```python\ndef compute_score(reward_inputs: List[Dict[str, Any]], format_weight: float = 0.1, alpha_length_bonus: float = 0.5, alpha_entropy: float = 0.5) -> List[Dict[str, float]]:\n```\n\n**Input Parameters:**\n- `reward_inputs`: List of reward input dictionaries, each containing:\n- `response`: Model-generated response text\n- `ground_truth`: Ground truth answer\n- `difficulty`: Sample difficulty label (\"easy\" / \"medium\" / \"hard\")\n- `high_entropy_token_num`: Number of HWE tokens in response\n- `target_high_entropy_token_num`: Target HWE token count (difficulty-related)\n- `alpha_entropy`: Entropy reward coefficient (difficulty-related)\n- `accuracy`: Accuracy label (optional, if not provided, extracted from response and evaluated)\n- `format_weight`: Format reward weight, default 0.1\n- `alpha_length_bonus`: Length reward coefficient (legacy parameter, actual usage is alpha_entropy)\n- `alpha_entropy`: Default entropy reward coefficient, default 0.5 (used if sample doesn't specify)\n\n**Detailed Description:**\nThis function implements ARES's core reward shaping mechanism, adaptively adjusting reasoning resource allocation based on task difficulty and model performance. The function computes format reward, accuracy reward, and entropy reward. For easy questions, when the answer is correct, it penalizes overly long reasoning chains, while when the answer is incorrect, it provides moderate positive rewards to encourage the model to explore more possibilities. Medium-difficulty questions require reasoning length closer to the target value; both excessive and insufficient reasoning are penalized when the answer is correct, but longer reasoning is still encouraged when the answer is incorrect. Hard questions adopt the opposite strategy: when the answer is correct, reasoning chains close to or exceeding the target are rewarded, and penalties are only applied when reasoning is significantly insufficient; when the answer is incorrect, longer reasoning is also encouraged to find the correct path.\nThe entire reward mechanism uses Huber loss function to smoothly handle penalty terms, Sigmoid function to achieve reward saturation effects, and sets different tolerance boundaries and reward caps for different difficulties.\nDifficulty-related hyperparameters that may be used:\n- `MARGIN_FRAC`: Tolerance band width (easy: 15%, medium: 25%, hard: 35%)\n- `KAPPA`: Huber function inflection point (easy: 2.0, medium: 3.0, hard: 4.0)\n- `TEMP`: Sigmoid temperature parameter (easy: 2.0, medium: 2.5, hard: 3.0)\n- `CAP_SCALE`: Entropy reward cap coefficient (easy: 1.0, medium: 1.0, hard: 1.2)\n\n**Expected Output:**\n- `scores`: List of score dictionaries, each containing:\n- `overall`: Total score = accuracy + entropy_score\n- `accuracy`: Accuracy score (0.0 or 1.0)\n- `format`: Format score (0.0 or 1.0)\n- `high_entropy_token_num_score`: Entropy reward score\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["from typing import List, Dict, Any\nimport math\n\ndef compute_score(reward_inputs: List[Dict[str, Any]],\n                  format_weight: float = 0.1,\n                  alpha_length_bonus: float = 0.5,\n                  alpha_entropy: float = 0.5) -> List[Dict[str, float]]:\n    \"\"\"\n    Compute ARES hierarchical entropy rewards for a batch of samples.\n\n    Returns a list of dicts containing:\n      - overall: accuracy + entropy_score\n      - accuracy: 0.0 or 1.0\n      - format: 0.0 or 1.0\n      - high_entropy_token_num_score: entropy reward (can be negative/positive)\n\n    Notes:\n      - Per-sample alpha_entropy (if present) overrides the function alpha_entropy.\n      - format_weight and alpha_length_bonus are accepted for API compatibility.\n        The actual entropy scaling uses alpha_entropy (per-sample or default).\n    \"\"\"\n\n    # Difficulty hyperparameters\n    DIFF_PARAMS = {\n        'easy':   {'MARGIN_FRAC': 0.15, 'KAPPA': 2.0, 'TEMP': 2.0,  'CAP_SCALE': 1.0},\n        'medium': {'MARGIN_FRAC': 0.25, 'KAPPA': 3.0, 'TEMP': 2.5,  'CAP_SCALE': 1.0},\n        'hard':   {'MARGIN_FRAC': 0.35, 'KAPPA': 4.0, 'TEMP': 3.0,  'CAP_SCALE': 1.2},\n    }\n\n    def _normalize_text(s: str) -> str:\n        # Basic normalization for simple accuracy check: lowercase, strip whitespace\n        if s is None:\n            return \"\"\n        return ' '.join(s.lower().strip().split())\n\n    def _simple_accuracy_from_text(response: str, ground_truth: str) -> float:\n        # If exact normalized match or ground truth appears in response -> correct\n        r = _normalize_text(response)\n        g = _normalize_text(ground_truth)\n        if not g:\n            return 0.0\n        if r == g:\n            return 1.0\n        if g in r:\n            return 1.0\n        return 0.0\n\n    def _format_present(response: str) -> float:\n        # Heuristic: presence of typical answer-format markers\n        if not response:\n            return 0.0\n        rl = response.lower()\n        markers = ['final answer', 'answer:', 'answer -', 'final:']\n        for m in markers:\n            if m in rl:\n                return 1.0\n        return 0.0\n\n    def _huber(x: float, kappa: float) -> float:\n        # Huber loss for non-negative x\n        if x <= kappa:\n            return 0.5 * x * x\n        else:\n            return kappa * (x - 0.5 * kappa)\n\n    def _sigmoid(x: float) -> float:\n        # Numerically stable sigmoid\n        # For large negative x, returns near 0; for large positive x, near 1\n        # but we expect x typically in modest ranges\n        try:\n            return 1.0 / (1.0 + math.exp(-x))\n        except OverflowError:\n            return 0.0 if x < 0 else 1.0\n\n    results: List[Dict[str, float]] = []\n\n    for idx, sample in enumerate(reward_inputs):\n        # Extract fields with safe defaults\n        response = sample.get('response', '') or ''\n        ground_truth = sample.get('ground_truth', '') or ''\n        difficulty = (sample.get('difficulty') or 'medium').lower()\n        difficulty = difficulty if difficulty in DIFF_PARAMS else 'medium'\n        observed = float(sample.get('high_entropy_token_num', 0) or 0)\n        target = float(sample.get('target_high_entropy_token_num', 0) or 0)\n        # If target is zero or not provided, avoid zero margin by setting minimum target 1 (tolerance computed relatively)\n        if target <= 0:\n            target = max(1.0, observed)\n\n        # alpha per sample overrides default alpha_entropy param\n        alpha = float(sample.get('alpha_entropy', alpha_entropy) or alpha_entropy)\n\n        # Accuracy: if provided use it (accept numeric/boolean), else compute from text\n        if 'accuracy' in sample and sample['accuracy'] is not None:\n            acc_val = sample['accuracy']\n            # Coerce to 0.0/1.0\n            try:\n                accuracy = 1.0 if float(acc_val) >= 0.5 else 0.0\n            except Exception:\n                accuracy = 1.0 if bool(acc_val) else 0.0\n        else:\n            accuracy = _simple_accuracy_from_text(response, ground_truth)\n\n        # Format present (0/1)\n        fmt = _format_present(response)\n\n        # Difficulty-specific params\n        params = DIFF_PARAMS[difficulty]\n        MARGIN_FRAC = params['MARGIN_FRAC']\n        KAPPA = params['KAPPA']\n        TEMP = params['TEMP']\n        CAP_SCALE = params['CAP_SCALE']\n\n        # Compute margin and signed difference\n        margin = MARGIN_FRAC * target\n        diff = observed - target  # positive means more HWE tokens than target\n\n        # penalty magnitude beyond tolerance (only beyond margin matters)\n        exceed = max(0.0, abs(diff) - margin)\n\n        # Huber penalty on exceed\n        hub = _huber(exceed, KAPPA)\n\n        # Base cap for entropy reward magnitude\n        base_cap = alpha * CAP_SCALE\n\n        # Initialize raw entropy reward\n        entropy_reward = 0.0\n\n        # Strategy per difficulty & correctness\n        if difficulty == 'easy':\n            if accuracy >= 1.0:\n                # Penalize overly long reasoning chains; small shortfall not penalized.\n                if diff > margin:\n                    # penalty increases with huber(exceed); use sigmoid to saturate\n                    s = _sigmoid(hub / max(1e-6, TEMP))\n                    entropy_reward = - base_cap * s\n                else:\n                    # no reward/penalty (encourage brevity)\n                    entropy_reward = 0.0\n            else:\n                # Incorrect: give moderate positive reward to encourage exploration (longer chains)\n                if diff > 0:\n                    s = _sigmoid((diff - margin) / max(1e-6, TEMP))\n                    # moderate positive up to half cap\n                    entropy_reward = 0.5 * base_cap * s\n                else:\n                    # Slight encouragement even if shorter, but small magnitude\n                    s = _sigmoid((margin + diff) / (TEMP + 1.0))\n                    entropy_reward = 0.2 * base_cap * s\n\n        elif difficulty == 'medium':\n            if accuracy >= 1.0:\n                # Penalize both excessive and insufficient when correct; reward being in-band\n                if abs(diff) > margin:\n                    s = _sigmoid(hub / max(1e-6, TEMP))\n                    entropy_reward = - base_cap * s\n                else:\n                    # Within tolerance: modest positive reward for \"just right\"\n                    entropy_reward = 0.5 * base_cap\n            else:\n                # Incorrect: encourage longer reasoning\n                if diff > 0:\n                    s = _sigmoid((diff - margin) / max(1e-6, TEMP))\n                    entropy_reward = base_cap * s\n                else:\n                    # small encouragement for trying more (but small)\n                    s = _sigmoid((margin + diff) / (TEMP + 1.0))\n                    entropy_reward = 0.1 * base_cap * s\n\n        else:  # hard\n            if accuracy >= 1.0:\n                # Reward chains close to or exceeding target; penalize only when significantly insufficient\n                if diff < -margin:\n                    # insufficient: penalize\n                    s = _sigmoid(hub / max(1e-6, TEMP))\n                    entropy_reward = - base_cap * s\n                else:\n                    # If at/above target, reward (saturating)\n                    s = _sigmoid(max(0.0, diff) / max(1e-6, TEMP))\n                    entropy_reward = base_cap * s\n            else:\n                # Incorrect: encourage longer reasoning (stronger on hard)\n                if diff > 0:\n                    s = _sigmoid((diff - margin) / max(1e-6, TEMP))\n                    # on hard tasks encourage more strongly (full cap)\n                    entropy_reward = base_cap * s\n                else:\n                    s = _sigmoid((margin + diff) / (TEMP + 1.0))\n                    entropy_reward = 0.15 * base_cap * s\n\n        # Clip entropy_reward to be within [-base_cap, base_cap] for stability\n        entropy_reward = max(-base_cap, min(base_cap, entropy_reward))\n\n        # Optionally combine format reward (we provide format field but do not add to 'overall')\n        # Keeping format binary as requested; format_weight could be applied elsewhere by caller.\n\n        # Define overall as accuracy + entropy_score per instructions\n        overall = float(accuracy) + float(entropy_reward)\n\n        # Put into result structure\n        res = {\n            'overall': float(overall),\n            'accuracy': float(accuracy),\n            'format': float(fmt),\n            'high_entropy_token_num_score': float(entropy_reward)\n        }\n        results.append(res)\n\n    return results"], "usage": {"prompt_tokens": 884, "completion_tokens": 4423, "total_tokens": 5307, "per_completion": [{"prompt_tokens": 884, "completion_tokens": 4423, "total_tokens": 5307}]}}
{"function_name": "RayPPOTrainer.update_difficulty_and_skip_gid_set", "overview": "Updates sample difficulty dictionary and adaptive entropy coefficients based on current batch rollout results, implementing online difficulty bucketing and resource budget adjustment.", "function_signature": "def update_difficulty_and_skip_gid_set(self, batch: DataProto) -> None:", "input_parameters": "- `batch`: DataProto object containing:\n- `global_id`: Sample global ID (for cross-batch tracking)\n- `accuracy`: Accuracy list (accuracy of each response)\n- `entropies`: Average entropy list (average token entropy of each response)\n- `high_entropy_token_num`: HWE token count list (HWE token count of each response)", "detailed_description": "This function is responsible for dynamically updating question difficulty classification and adaptively adjusting entropy penalty coefficients during training. The function needs to aggregate accuracy, entropy, and high-entropy token counts for each question by global identifier. Questions are categorized into easy, medium, and hard levels based on accuracy. Based on high-entropy token counts, the function needs to compute target reasoning length for each difficulty level and adjust entropy penalty coefficients based on deviation between actual generated length and target value. When adjusting entropy penalty coefficients, gradient ascent can be applied. For easy and medium questions, if reasoning is too long, increase penalty strength; for hard questions, increase encouragement when reasoning is insufficient.\nDifficulty assignment based on accuracy thresholds:\n- `acc_rate ≥ 2/3` → \"easy\"\n- `1/3 ≤ acc_rate < 2/3` → \"medium\"\n- `acc_rate < 1/3` → \"hard\"\n(Optional) The function adds samples that are \"all correct with extremely low average entropy\" to `self.skip_gid_set`, and subsequent training will skip these overly simple samples, focusing on challenging data.", "output": "- No direct return value, but updates the following class attributes:\n- `self.difficulty_dict_train`: Sample difficulty dictionary `{global_id: difficulty}`\n- `self.target_high_entropy_token_num_dict`: Target HWE token count `{difficulty: target_num}`\n- `self.alpha_entropy_dict`: Entropy coefficient dictionary `{difficulty: alpha}`\n- `self.skip_gid_set`: Skip sample set (optional)", "func_implementation": "", "test_code_path": "code/test_code/test_update_difficulty.py", "implementation_location": "code/verl/trainer/ray_trainer.py:line 775-825", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `RayPPOTrainer.update_difficulty_and_skip_gid_set`.\n\n**Function Overview:**\nUpdates sample difficulty dictionary and adaptive entropy coefficients based on current batch rollout results, implementing online difficulty bucketing and resource budget adjustment.\n\n**Function Signature:**\n```python\ndef update_difficulty_and_skip_gid_set(self, batch: DataProto) -> None:\n```\n\n**Input Parameters:**\n- `batch`: DataProto object containing:\n- `global_id`: Sample global ID (for cross-batch tracking)\n- `accuracy`: Accuracy list (accuracy of each response)\n- `entropies`: Average entropy list (average token entropy of each response)\n- `high_entropy_token_num`: HWE token count list (HWE token count of each response)\n\n**Detailed Description:**\nThis function is responsible for dynamically updating question difficulty classification and adaptively adjusting entropy penalty coefficients during training. The function needs to aggregate accuracy, entropy, and high-entropy token counts for each question by global identifier. Questions are categorized into easy, medium, and hard levels based on accuracy. Based on high-entropy token counts, the function needs to compute target reasoning length for each difficulty level and adjust entropy penalty coefficients based on deviation between actual generated length and target value. When adjusting entropy penalty coefficients, gradient ascent can be applied. For easy and medium questions, if reasoning is too long, increase penalty strength; for hard questions, increase encouragement when reasoning is insufficient.\nDifficulty assignment based on accuracy thresholds:\n- `acc_rate ≥ 2/3` → \"easy\"\n- `1/3 ≤ acc_rate < 2/3` → \"medium\"\n- `acc_rate < 1/3` → \"hard\"\n(Optional) The function adds samples that are \"all correct with extremely low average entropy\" to `self.skip_gid_set`, and subsequent training will skip these overly simple samples, focusing on challenging data.\n\n**Expected Output:**\n- No direct return value, but updates the following class attributes:\n- `self.difficulty_dict_train`: Sample difficulty dictionary `{global_id: difficulty}`\n- `self.target_high_entropy_token_num_dict`: Target HWE token count `{difficulty: target_num}`\n- `self.alpha_entropy_dict`: Entropy coefficient dictionary `{difficulty: alpha}`\n- `self.skip_gid_set`: Skip sample set (optional)\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def update_difficulty_and_skip_gid_set(self, batch: DataProto) -> None:\n    \"\"\"\n    Update difficulty assignments and adaptive entropy coefficients based on the provided batch.\n\n    This method:\n      - Aggregates per-sample stats (accuracy, entropy, high-entropy-token count) by global_id.\n      - Assigns difficulty (easy/medium/hard) for each global_id using rolling aggregated accuracy.\n      - Updates per-difficulty target high-entropy-token counts using an EMA.\n      - Adjusts per-difficulty entropy coefficient (alpha) using a simple gradient-ascent style update\n        driven by the deviation between observed and target reasoning lengths.\n      - Optionally adds trivially-easy low-entropy examples to self.skip_gid_set to skip them later.\n\n    Expects batch to provide iterables/lists for:\n      - batch.global_id\n      - batch.accuracy\n      - batch.entropies\n      - batch.high_entropy_token_num\n\n    The function mutates:\n      - self.difficulty_dict_train (dict: gid -> difficulty)\n      - self.target_high_entropy_token_num_dict (dict: difficulty -> target_num)\n      - self.alpha_entropy_dict (dict: difficulty -> alpha)\n      - self.skip_gid_set (set of gids)\n      - self._agg_stats (internal per-gid rolling aggregation storage)\n    \"\"\"\n    # Ensure internal structures exist\n    if not hasattr(self, \"_agg_stats\"):\n        # _agg_stats maps gid -> dict with running sums and count\n        self._agg_stats = {}\n\n    if not hasattr(self, \"difficulty_dict_train\") or self.difficulty_dict_train is None:\n        self.difficulty_dict_train = {}\n\n    if not hasattr(self, \"target_high_entropy_token_num_dict\") or self.target_high_entropy_token_num_dict is None:\n        # initialize reasonable defaults\n        self.target_high_entropy_token_num_dict = {\"easy\": 1.0, \"medium\": 3.0, \"hard\": 6.0}\n\n    if not hasattr(self, \"alpha_entropy_dict\") or self.alpha_entropy_dict is None:\n        # entropy penalty coefficients; positive means penalize entropy (shorter reasoning),\n        # negative would encourage entropy (longer reasoning). Start with small positives.\n        self.alpha_entropy_dict = {\"easy\": 0.02, \"medium\": 0.02, \"hard\": 0.02}\n\n    if not hasattr(self, \"skip_gid_set\") or self.skip_gid_set is None:\n        self.skip_gid_set = set()\n\n    # Hyperparameters for updates (tunable)\n    ema_beta = getattr(self, \"difficulty_ema_beta\", 0.1)            # smoothing for target length\n    lr_alpha = getattr(self, \"alpha_lr\", 0.01)                     # learning rate for alpha updates\n    alpha_min = getattr(self, \"alpha_min\", -0.5)                   # allow encouragement (negative) up to this\n    alpha_max = getattr(self, \"alpha_max\", 1.0)                    # maximum penalty\n    low_entropy_threshold = getattr(self, \"skip_low_entropy_thresh\", 0.05)  # threshold to skip trivial samples\n    skip_entropy_required_acc_rate = getattr(self, \"skip_required_acc_rate\", 1.0)  # require all correct\n\n    # Read from batch (support tensors/lists)\n    gids = list(batch.global_id)\n    accs = list(batch.accuracy)\n    entropies = list(batch.entropies)\n    hwe_nums = list(batch.high_entropy_token_num)\n\n    # Aggregate incoming stats into _agg_stats (rolling sums)\n    for gid, acc, ent, hwe in zip(gids, accs, entropies, hwe_nums):\n        # Normalize/interpret accuracy as 0/1 if boolean-like; accept floats too\n        try:\n            correct = float(acc)\n        except Exception:\n            correct = 1.0 if acc else 0.0\n\n        try:\n            ent = float(ent)\n        except Exception:\n            ent = 0.0\n\n        try:\n            hwe = float(hwe)\n        except Exception:\n            hwe = float(hwe) if hwe is not None else 0.0\n\n        s = self._agg_stats.get(gid)\n        if s is None:\n            s = {\"sum_correct\": 0.0, \"sum_entropy\": 0.0, \"sum_hwe\": 0.0, \"count\": 0}\n            self._agg_stats[gid] = s\n        s[\"sum_correct\"] += correct\n        s[\"sum_entropy\"] += ent\n        s[\"sum_hwe\"] += hwe\n        s[\"count\"] += 1\n\n    # Temporary per-difficulty observed lists for high-entropy-token counts (current view)\n    observed_hwe_per_diff = {\"easy\": [], \"medium\": [], \"hard\": []}\n\n    # For each gid we updated, compute statistics and assign difficulty\n    for gid in set(gids):\n        s = self._agg_stats[gid]\n        count = max(1, s[\"count\"])\n        acc_rate = s[\"sum_correct\"] / count\n        avg_entropy = s[\"sum_entropy\"] / count\n        avg_hwe = s[\"sum_hwe\"] / count\n\n        # Determine difficulty from accuracy thresholds\n        if acc_rate >= 2.0 / 3.0:\n            difficulty = \"easy\"\n        elif acc_rate >= 1.0 / 3.0:\n            difficulty = \"medium\"\n        else:\n            difficulty = \"hard\"\n\n        # Save difficulty\n        self.difficulty_dict_train[gid] = difficulty\n\n        # Collect observed high-entropy token counts for this difficulty\n        observed_hwe_per_diff[difficulty].append(avg_hwe)\n\n        # Optionally skip trivial samples:\n        # If the sample is always correct so far (acc_rate >= required) and entropy very low,\n        # we consider skipping it in subsequent training to focus on harder examples.\n        if acc_rate >= skip_entropy_required_acc_rate and avg_entropy <= low_entropy_threshold:\n            self.skip_gid_set.add(gid)\n\n    # Update per-difficulty target high-entropy-token counts (EMA) and adjust alphas\n    for difficulty in (\"easy\", \"medium\", \"hard\"):\n        obs_list = observed_hwe_per_diff.get(difficulty, [])\n        if len(obs_list) == 0:\n            # Nothing new to update for this difficulty\n            continue\n\n        observed_mean = sum(obs_list) / len(obs_list)\n\n        # Current target\n        old_target = self.target_high_entropy_token_num_dict.get(difficulty, observed_mean)\n        # EMA update\n        new_target = (1.0 - ema_beta) * old_target + ema_beta * observed_mean\n        self.target_high_entropy_token_num_dict[difficulty] = new_target\n\n        # Adjust alpha based on deviation (observed - target)\n        # If observed > target for easy/medium -> increase penalty (alpha up)\n        # If observed < target for hard -> encourage (reduce alpha)\n        old_alpha = self.alpha_entropy_dict.get(difficulty, 0.02)\n        deviation = observed_mean - new_target  # note: new_target is already moved toward observed_mean; small deviation\n\n        # Simpler, more stable update uses observed_mean vs old_target\n        deviation_vs_old = observed_mean - old_target\n\n        # Update rule: alpha += lr * deviation_vs_old, but different sign expectations for difficulty types\n        if difficulty in (\"easy\", \"medium\"):\n            # if observed > old_target -> reasoning longer than desired -> increase penalty (alpha up)\n            alpha_update = lr_alpha * deviation_vs_old\n        else:  # hard\n            # if observed < old_target -> reasoning shorter than desired -> encourage (reduce alpha)\n            # We invert sign so that negative deviation (observed < old_target) reduces alpha\n            alpha_update = lr_alpha * deviation_vs_old\n\n        new_alpha = old_alpha + alpha_update\n\n        # Clamp alpha to reasonable bounds\n        new_alpha = max(alpha_min, min(alpha_max, new_alpha))\n        self.alpha_entropy_dict[difficulty] = new_alpha\n\n    # No return; in-place updates performed."], "usage": {"prompt_tokens": 655, "completion_tokens": 2800, "total_tokens": 3455, "per_completion": [{"prompt_tokens": 655, "completion_tokens": 2800, "total_tokens": 3455}]}}
{"function_name": "TokenLevelAdaptiveKLController.update", "overview": "Implements update logic for token-level adaptive KL controller, dynamically adjusting KL coefficients using dual ascent mechanism based on deviation between actual KL divergence and target for each difficulty bucket.", "function_signature": "def update(self, current_kls: Dict[str, float], n_steps: Dict[str, int]):", "input_parameters": "- `current_kls`: Average KL divergence for each difficulty bucket at current step, dictionary format `{difficulty: kl_value}`\n- Example: `{\"easy\": 0.06, \"medium\": 0.12, \"hard\": 0.18}`\n- `n_steps`: Sample count for each difficulty bucket (used for weighting, may be simplified in actual implementation)", "detailed_description": "Token-level adaptive KL control allows using different KL budgets for different difficulty tasks, automatically adjusting KL divergence coefficients for each difficulty bucket through dual ascent mechanism. When actual KL divergence exceeds target value, increase penalty coefficient; when below target value, decrease coefficient.", "output": "- No direct return value, but updates class attributes:\n- `self.kl_coefs`: Updated KL coefficient dictionary `{difficulty: beta}`\n- `self.lambdas`: Updated Lagrange multiplier dictionary `{difficulty: lambda}`", "func_implementation": "", "test_code_path": "code/test_code/test_kl_controller.py", "implementation_location": "code/verl/trainer/core_algos.py:line 77-86", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `TokenLevelAdaptiveKLController.update`.\n\n**Function Overview:**\nImplements update logic for token-level adaptive KL controller, dynamically adjusting KL coefficients using dual ascent mechanism based on deviation between actual KL divergence and target for each difficulty bucket.\n\n**Function Signature:**\n```python\ndef update(self, current_kls: Dict[str, float], n_steps: Dict[str, int]):\n```\n\n**Input Parameters:**\n- `current_kls`: Average KL divergence for each difficulty bucket at current step, dictionary format `{difficulty: kl_value}`\n- Example: `{\"easy\": 0.06, \"medium\": 0.12, \"hard\": 0.18}`\n- `n_steps`: Sample count for each difficulty bucket (used for weighting, may be simplified in actual implementation)\n\n**Detailed Description:**\nToken-level adaptive KL control allows using different KL budgets for different difficulty tasks, automatically adjusting KL divergence coefficients for each difficulty bucket through dual ascent mechanism. When actual KL divergence exceeds target value, increase penalty coefficient; when below target value, decrease coefficient.\n\n**Expected Output:**\n- No direct return value, but updates class attributes:\n- `self.kl_coefs`: Updated KL coefficient dictionary `{difficulty: beta}`\n- `self.lambdas`: Updated Lagrange multiplier dictionary `{difficulty: lambda}`\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def update(self, current_kls: Dict[str, float], n_steps: Dict[str, int]):\n    \"\"\"\n    Update token-level KL coefficients and Lagrange multipliers per difficulty bucket.\n\n    Args:\n        current_kls: dict mapping difficulty -> observed average KL at current step\n        n_steps: dict mapping difficulty -> number of samples for that difficulty (used to weight updates)\n    Side effects:\n        Updates self.lambdas and self.kl_coefs in-place.\n    Notes:\n        - Uses a dual-ascent style update:\n            lambda_new = max(0, lambda_old + lambda_lr * weight * (current_kl - target_kl))\n          and then sets kl_coef to a clamped version of the lambda (or initialized coefficient).\n        - Many hyperparameters are taken from attributes on self if present, otherwise reasonable defaults are used.\n    \"\"\"\n    import math\n\n    # Hyperparameters (can be set on the controller instance)\n    lambda_lr = getattr(self, \"lambda_lr\", 0.5)               # learning rate for lambda updates\n    kl_coef_min = getattr(self, \"min_kl_coef\", 1e-6)          # minimum kl coef allowed\n    kl_coef_max = getattr(self, \"max_kl_coef\", 1e6)           # maximum kl coef allowed\n    kl_coef_init = getattr(self, \"kl_coef_init\", 1.0)         # initial kl coef when not present\n    lambda_init = getattr(self, \"lambda_init\", 1.0)           # initial lambda when not present\n    use_n_steps_weighting = getattr(self, \"use_n_steps_weighting\", True)\n\n    # Compute normalization for weighting (so updates are proportional to sample counts)\n    total_n = float(sum(n_steps.values())) if (use_n_steps_weighting and len(n_steps) > 0) else 0.0\n\n    # Ensure dicts exist\n    if not hasattr(self, \"lambdas\") or self.lambdas is None:\n        self.lambdas = {}\n    if not hasattr(self, \"kl_coefs\") or self.kl_coefs is None:\n        self.kl_coefs = {}\n\n    for diff, observed_kl in current_kls.items():\n        # Get target KL for this difficulty (fallback to a single target or default)\n        if hasattr(self, \"target_kl\"):\n            # target_kl might be a dict or a scalar\n            if isinstance(self.target_kl, dict):\n                target = self.target_kl.get(diff, getattr(self, \"target_kl_default\", 0.1))\n            else:\n                target = float(self.target_kl)\n        else:\n            target = getattr(self, \"target_kl_default\", 0.1)\n\n        # Get counts and compute weighting\n        n = float(n_steps.get(diff, 1))\n        if use_n_steps_weighting and total_n > 0.0:\n            weight = n / total_n\n        else:\n            weight = 1.0\n\n        # Current stored values (initializing if necessary)\n        lambda_old = float(self.lambdas.get(diff, lambda_init))\n        kl_coef_old = float(self.kl_coefs.get(diff, kl_coef_init))\n\n        # Dual ascent update on lambda (Lagrange multiplier)\n        # If observed_kl > target => error positive => increase lambda (increase penalty)\n        error = observed_kl - target\n        lambda_update = lambda_lr * weight * error\n        lambda_new = lambda_old + lambda_update\n        # Clamp lambda to be non-negative\n        if lambda_new < 0.0:\n            lambda_new = 0.0\n\n        # Update stored lambda\n        self.lambdas[diff] = lambda_new\n\n        # Map lambda to a KL coefficient. Two simple and stable choices:\n        #  - directly use lambda as the coefficient (clamped)\n        #  - or adapt multiplicatively from previous coefficient\n        # Here we use the simpler and commonly used approach: set kl_coef = clamp(lambda)\n        # but fall back to previous coefficient if lambda is very small to avoid collapse.\n        kl_coef_new = lambda_new\n        # enforce bounds\n        if kl_coef_new < kl_coef_min:\n            kl_coef_new = kl_coef_min\n        if kl_coef_new > kl_coef_max:\n            kl_coef_new = kl_coef_max\n\n        # If lambda is exactly zero (no penalty), keep a tiny positive coef to avoid degenerate zero\n        if kl_coef_new == 0.0:\n            kl_coef_new = kl_coef_min\n\n        self.kl_coefs[diff] = float(kl_coef_new)\n\n        # Optional: very small safety smoothing could be applied here if desired (not performed by default)\n\n    # No return; updates are in-place on self.kl_coefs and self.lambdas."], "usage": {"prompt_tokens": 455, "completion_tokens": 1897, "total_tokens": 2352, "per_completion": [{"prompt_tokens": 455, "completion_tokens": 1897, "total_tokens": 2352}]}}
{"function_name": "apply_kl_penalty", "overview": "Applies KL divergence penalty to token-level rewards and updates KL controller.", "function_signature": "def apply_kl_penalty(data: DataProto, kl_ctrl: KLController, kl_penalty=\"kl\"):", "input_parameters": "- `data`: DataProto object containing:\n- `token_level_scores`: Original token-level rewards [batch_size, response_length]\n- `old_log_probs`: Actor old policy log probabilities [batch_size, response_length]\n- `ref_log_probs`: Ref policy log probabilities [batch_size, response_length]\n- `response_mask`: Response mask [batch_size, response_length]\n- `kl_ctrl`: KL controller (FixedKLController or AdaptiveKLController)\n- `kl_penalty`: KL penalty type (\"kl\", \"abs\", \"mse\", \"low_var_kl\", \"full\")", "detailed_description": "This function implements computation and application of token-level KL penalty. It computes token-level KL divergence based on `kl_penalty` type, then subtracts KL penalty term from original rewards to obtain token-level rewards, and computes sequence-level KL divergence and batch-level KL divergence.\nThe function also needs to implement KL controller update.", "output": "- `data`: Updated DataProto where `data.batch[\"token_level_rewards\"]` has KL penalty applied\n- `metrics`: Metrics dictionary `{\"critic/kl\": current_kl, \"critic/kl_coef\": kl_ctrl.kl_coef}`", "func_implementation": "", "test_code_path": "code/test_code/test_apply_kl_penalty.py", "implementation_location": "code/verl/trainer/ray_trainer.py:line 107-124", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `apply_kl_penalty`.\n\n**Function Overview:**\nApplies KL divergence penalty to token-level rewards and updates KL controller.\n\n**Function Signature:**\n```python\ndef apply_kl_penalty(data: DataProto, kl_ctrl: KLController, kl_penalty=\"kl\"):\n```\n\n**Input Parameters:**\n- `data`: DataProto object containing:\n- `token_level_scores`: Original token-level rewards [batch_size, response_length]\n- `old_log_probs`: Actor old policy log probabilities [batch_size, response_length]\n- `ref_log_probs`: Ref policy log probabilities [batch_size, response_length]\n- `response_mask`: Response mask [batch_size, response_length]\n- `kl_ctrl`: KL controller (FixedKLController or AdaptiveKLController)\n- `kl_penalty`: KL penalty type (\"kl\", \"abs\", \"mse\", \"low_var_kl\", \"full\")\n\n**Detailed Description:**\nThis function implements computation and application of token-level KL penalty. It computes token-level KL divergence based on `kl_penalty` type, then subtracts KL penalty term from original rewards to obtain token-level rewards, and computes sequence-level KL divergence and batch-level KL divergence.\nThe function also needs to implement KL controller update.\n\n**Expected Output:**\n- `data`: Updated DataProto where `data.batch[\"token_level_rewards\"]` has KL penalty applied\n- `metrics`: Metrics dictionary `{\"critic/kl\": current_kl, \"critic/kl_coef\": kl_ctrl.kl_coef}`\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["import torch\n\ndef apply_kl_penalty(data: \"DataProto\", kl_ctrl, kl_penalty=\"kl\"):\n    \"\"\"\n    Apply KL divergence penalty to token-level rewards and update KL controller.\n\n    Args:\n        data (DataProto): contains batch tensors:\n            - \"token_level_scores\": original token-level rewards [B, L]\n            - \"old_log_probs\": old policy log-probs for chosen tokens [B, L]\n            - \"ref_log_probs\": reference policy log-probs for chosen tokens [B, L]\n            - \"response_mask\": mask for response tokens [B, L] (0/1 or bool)\n        kl_ctrl (KLController): KL controller with attributes `.kl_coef` and `.update(current_kl)`\n        kl_penalty (str): one of {\"kl\", \"abs\", \"mse\", \"low_var_kl\", \"full\"}\n\n    Returns:\n        data (DataProto): same object with data.batch[\"token_level_rewards\"] updated\n        metrics (dict): {\"critic/kl\": current_kl (float), \"critic/kl_coef\": kl_ctrl.kl_coef}\n    \"\"\"\n    batch = data.batch\n\n    # Extract tensors\n    token_level_scores = batch[\"token_level_scores\"]            # [B, L]\n    old_log_probs = batch[\"old_log_probs\"]                      # [B, L]\n    ref_log_probs = batch[\"ref_log_probs\"]                      # [B, L]\n    response_mask = batch[\"response_mask\"]                      # [B, L]\n\n    # Make sure mask is float\n    if response_mask.dtype == torch.bool:\n        mask = response_mask.to(dtype=token_level_scores.dtype)\n    else:\n        mask = response_mask.to(dtype=token_level_scores.dtype)\n\n    # Difference in log-probs (element-wise)\n    diff = old_log_probs - ref_log_probs  # [B, L]\n\n    # Compute token-level KL-like penalty according to kl_penalty\n    if kl_penalty == \"kl\":\n        # Standard per-token KL estimate: log pi_old - log pi_ref\n        token_kl = diff.detach()\n    elif kl_penalty == \"abs\":\n        token_kl = torch.abs(diff).detach()\n    elif kl_penalty == \"mse\":\n        token_kl = diff.pow(2).detach()\n    elif kl_penalty == \"low_var_kl\":\n        # Importance-weighted style to reduce variance:\n        # weight by probability of chosen token under old policy\n        # Note: exp(old_log_probs) is the prob of the chosen token.\n        token_kl = (torch.exp(old_log_probs.detach()) * diff).detach()\n    elif kl_penalty == \"full\":\n        # Fallback to same as \"kl\" when full distribution KL is not available\n        token_kl = diff.detach()\n    else:\n        raise ValueError(f\"Unknown kl_penalty: {kl_penalty}\")\n\n    # Zero-out masked-out positions\n    token_kl = token_kl * mask\n\n    # Apply KL penalty to token-level rewards\n    kl_coef = float(kl_ctrl.kl_coef)  # ensure numeric scalar\n    token_level_rewards = token_level_scores - kl_coef * token_kl\n\n    # Ensure masked-out tokens have zero reward (or keep original behavior)\n    # Here we zero them to be consistent\n    token_level_rewards = token_level_rewards * mask\n\n    # Save updated token-level rewards back into data.batch\n    batch[\"token_level_rewards\"] = token_level_rewards\n\n    # Compute sequence-level KL: average KL per token for each sequence\n    # Avoid divide-by-zero\n    seq_token_counts = mask.sum(dim=1)  # [B]\n    denom = seq_token_counts.clamp_min(1.0)\n    seq_kl = token_kl.sum(dim=1) / denom  # [B]\n\n    # Batch-level current KL (scalar)\n    current_kl = float(seq_kl.mean().item())\n\n    # Update KL controller\n    # Many controllers accept a float or torch scalar\n    try:\n        kl_ctrl.update(current_kl)\n    except Exception:\n        # fallback: try passing torch.tensor\n        kl_ctrl.update(torch.tensor(current_kl, dtype=token_level_scores.dtype, device=token_level_scores.device))\n\n    metrics = {\n        \"critic/kl\": current_kl,\n        \"critic/kl_coef\": float(kl_ctrl.kl_coef)\n    }\n\n    return data, metrics"], "usage": {"prompt_tokens": 499, "completion_tokens": 2052, "total_tokens": 2551, "per_completion": [{"prompt_tokens": 499, "completion_tokens": 2052, "total_tokens": 2551}]}}
