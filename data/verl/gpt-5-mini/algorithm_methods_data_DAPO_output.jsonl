{"function_name": "agg_loss", "overview": "Aggregates loss matrix to scalar, supporting multiple aggregation modes. DAPO recommends using `token-mean` mode, making each token's contribution to loss equal, avoiding long sequences dominating gradients.", "function_signature": "def agg_loss(loss_mat: torch.Tensor, loss_mask: torch.Tensor, loss_agg_mode: str):", "input_parameters": "- `loss_mat`: Loss matrix with shape `(batch_size, response_length)`\n- `loss_mask`: Mask tensor with shape `(batch_size, response_length)` (response part is 1, padding is 0)\n- `loss_agg_mode`: String specifying aggregation mode\n- `\"token-mean\"`: Average over all tokens (**DAPO recommended**)\n- `\"seq-mean-token-sum\"`: Sum tokens within sequences first, then average across sequences\n- `\"seq-mean-token-mean\"`: Average tokens within sequences first, then average across sequences\n- `\"seq-mean-token-sum-norm\"`: Sum sequences then normalize by sequence length", "detailed_description": "The core function is to aggregate a 2D loss matrix into a single scalar value, supporting the following four aggregation strategies: token-mean, seq-mean-token-sum, seq-mean-token-mean, seq-mean-token-sum-norm.", "output": "- Scalar loss value for backpropagation", "func_implementation": "", "test_code_path": "code/test_code/test_agg_loss.py", "implementation_location": "code/verl/trainer/ppo/core_algos.py:line 704-737", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `agg_loss`.\n\n**Function Overview:**\nAggregates loss matrix to scalar, supporting multiple aggregation modes. DAPO recommends using `token-mean` mode, making each token's contribution to loss equal, avoiding long sequences dominating gradients.\n\n**Function Signature:**\n```python\ndef agg_loss(loss_mat: torch.Tensor, loss_mask: torch.Tensor, loss_agg_mode: str):\n```\n\n**Input Parameters:**\n- `loss_mat`: Loss matrix with shape `(batch_size, response_length)`\n- `loss_mask`: Mask tensor with shape `(batch_size, response_length)` (response part is 1, padding is 0)\n- `loss_agg_mode`: String specifying aggregation mode\n- `\"token-mean\"`: Average over all tokens (**DAPO recommended**)\n- `\"seq-mean-token-sum\"`: Sum tokens within sequences first, then average across sequences\n- `\"seq-mean-token-mean\"`: Average tokens within sequences first, then average across sequences\n- `\"seq-mean-token-sum-norm\"`: Sum sequences then normalize by sequence length\n\n**Detailed Description:**\nThe core function is to aggregate a 2D loss matrix into a single scalar value, supporting the following four aggregation strategies: token-mean, seq-mean-token-sum, seq-mean-token-mean, seq-mean-token-sum-norm.\n\n**Expected Output:**\n- Scalar loss value for backpropagation\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["import torch\n\ndef agg_loss(loss_mat: torch.Tensor, loss_mask: torch.Tensor, loss_agg_mode: str):\n    \"\"\"\n    Aggregate a 2D loss matrix into a scalar according to the specified mode.\n\n    Args:\n        loss_mat (torch.Tensor): shape (batch_size, response_length)\n        loss_mask (torch.Tensor): shape (batch_size, response_length), 1 for token, 0 for padding\n        loss_agg_mode (str): one of\n            - \"token-mean\": average over all tokens (DAPO recommended)\n            - \"seq-mean-token-sum\": sum tokens within sequences, then average sequences\n            - \"seq-mean-token-mean\": average tokens within sequences, then average sequences\n            - \"seq-mean-token-sum-norm\": sum tokens within sequences, then normalize by average sequence length\n    Returns:\n        torch.Tensor: scalar loss (0-dim tensor) suitable for backprop\n    \"\"\"\n    # Validate mode\n    modes = {\n        \"token-mean\",\n        \"seq-mean-token-sum\",\n        \"seq-mean-token-mean\",\n        \"seq-mean-token-sum-norm\",\n    }\n    if loss_agg_mode not in modes:\n        raise ValueError(f\"Unsupported loss_agg_mode: {loss_agg_mode}\")\n\n    # Ensure mask has same dtype as loss_mat for multiplication\n    mask = loss_mask.to(loss_mat.dtype)\n\n    # Per-sequence token counts and total tokens\n    per_seq_counts = mask.sum(dim=1)                # shape: (batch,)\n    total_tokens = per_seq_counts.sum()             # scalar\n\n    # Per-sequence sum of losses (masked)\n    per_seq_sums = (loss_mat * mask).sum(dim=1)     # shape: (batch,)\n\n    if loss_agg_mode == \"token-mean\":\n        # Average over all tokens (each token contributes equally)\n        denom = total_tokens.clamp_min(1.0)\n        loss = per_seq_sums.sum() / denom\n\n    elif loss_agg_mode == \"seq-mean-token-sum\":\n        # Sum tokens in each sequence, then average across sequences\n        # This gives longer sequences larger influence\n        loss = per_seq_sums.mean()\n\n    elif loss_agg_mode == \"seq-mean-token-mean\":\n        # Average tokens within each sequence, then average sequences\n        # For sequences with 0 tokens, treat their mean as 0 (avoid NaN by clamping)\n        denom = per_seq_counts.clamp_min(1.0)\n        per_seq_means = per_seq_sums / denom\n        loss = per_seq_means.mean()\n\n    elif loss_agg_mode == \"seq-mean-token-sum-norm\":\n        # Sum tokens within sequences then normalize by average sequence length.\n        # This equals token-mean (sum_all / total_tokens) when average length is used.\n        avg_seq_len = per_seq_counts.mean().clamp_min(1.0)\n        loss = per_seq_sums.mean() / avg_seq_len\n\n    else:\n        # Defensive: should not reach here due to earlier check\n        raise RuntimeError(f\"Unhandled loss_agg_mode: {loss_agg_mode}\")\n\n    # Ensure returned value is a scalar tensor\n    if not torch.is_tensor(loss):\n        loss = torch.tensor(loss, dtype=loss_mat.dtype, device=loss_mat.device)\n\n    return loss"], "usage": {"prompt_tokens": 482, "completion_tokens": 2260, "total_tokens": 2742, "per_completion": [{"prompt_tokens": 482, "completion_tokens": 2260, "total_tokens": 2742}]}}
{"function_name": "compute_policy_loss_vanilla", "overview": "Computes PPO policy gradient loss, supporting decoupled clipping (DAPO's core innovation). Through `clip_ratio_low` and `clip_ratio_high` parameters, allows asymmetric clipping for positive and negative advantages, implementing the \"Clip-Higher\" mechanism.", "function_signature": "def compute_policy_loss_vanilla(\nold_log_prob: torch.Tensor,\nlog_prob: torch.Tensor,\nadvantages: torch.Tensor,\nresponse_mask: torch.Tensor,\nloss_agg_mode: str = \"token-mean\",\nconfig: Optional[DictConfig | AlgoConfig] = None,\nrollout_log_probs: torch.Tensor | None = None,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:", "input_parameters": "- `old_log_prob`: Old policy log probabilities with shape `(batch_size, response_length)`\n- `log_prob`: Current policy log probabilities with shape `(batch_size, response_length)`\n- `advantages`: Advantage estimates with shape `(batch_size, response_length)`\n- `response_mask`: Response mask with shape `(batch_size, response_length)`\n- `loss_agg_mode`: Loss aggregation mode (default is token-mean)\n- `config`: Actor configuration object, containing:\n- `clip_ratio`: Default clipping parameter (backward compatibility)\n- `clip_ratio_low`: Lower bound clipping parameter (**DAPO new**)\n- `clip_ratio_high`: Upper bound clipping parameter (**DAPO new**)\n- `clip_ratio_c`: Dual clipping lower bound (handles extreme negative advantages, default 3.0)\n- `rollout_log_probs`: Optional, Rollout policy log probabilities (for off-policy training)", "detailed_description": "The function implements enhanced PPO policy loss computation, including standard PPO clipped loss calculation, dual-clip PPO, and truncated importance sampling (optional), ultimately calling related functions to implement loss aggregation.\nStandard PPO clipping mechanism supports asymmetric clipping, requiring computation of unclipped original policy gradient loss and asymmetrically clipped loss, taking the maximum of both as pessimistic update strategy. When advantage value is negative, apply dual clipping to more conservatively reduce the probability of that action. If truncated importance sampling is enabled and rollout policy log probabilities are provided, the function computes the importance ratio of old policy relative to rollout policy, clips it to the configured upper limit, then multiplies it into the policy gradient loss.", "output": "Returns four scalar tensors:\n- `pg_loss`: Policy gradient loss\n- `pg_clipfrac`: Clipping fraction (proportion of clipped tokens, for monitoring training dynamics)\n- `ppo_kl`: Approximate KL divergence\n- `pg_clipfrac_lower`: Lower bound clipping fraction (proportion of tokens where dual clipping takes effect)", "func_implementation": "", "test_code_path": "code/test_code/test_compute_policy_loss_vanilla.py", "implementation_location": "code/verl/trainer/ppo/core_algos.py:line 816-902", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `compute_policy_loss_vanilla`.\n\n**Function Overview:**\nComputes PPO policy gradient loss, supporting decoupled clipping (DAPO's core innovation). Through `clip_ratio_low` and `clip_ratio_high` parameters, allows asymmetric clipping for positive and negative advantages, implementing the \"Clip-Higher\" mechanism.\n\n**Function Signature:**\n```python\ndef compute_policy_loss_vanilla(\nold_log_prob: torch.Tensor,\nlog_prob: torch.Tensor,\nadvantages: torch.Tensor,\nresponse_mask: torch.Tensor,\nloss_agg_mode: str = \"token-mean\",\nconfig: Optional[DictConfig | AlgoConfig] = None,\nrollout_log_probs: torch.Tensor | None = None,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n```\n\n**Input Parameters:**\n- `old_log_prob`: Old policy log probabilities with shape `(batch_size, response_length)`\n- `log_prob`: Current policy log probabilities with shape `(batch_size, response_length)`\n- `advantages`: Advantage estimates with shape `(batch_size, response_length)`\n- `response_mask`: Response mask with shape `(batch_size, response_length)`\n- `loss_agg_mode`: Loss aggregation mode (default is token-mean)\n- `config`: Actor configuration object, containing:\n- `clip_ratio`: Default clipping parameter (backward compatibility)\n- `clip_ratio_low`: Lower bound clipping parameter (**DAPO new**)\n- `clip_ratio_high`: Upper bound clipping parameter (**DAPO new**)\n- `clip_ratio_c`: Dual clipping lower bound (handles extreme negative advantages, default 3.0)\n- `rollout_log_probs`: Optional, Rollout policy log probabilities (for off-policy training)\n\n**Detailed Description:**\nThe function implements enhanced PPO policy loss computation, including standard PPO clipped loss calculation, dual-clip PPO, and truncated importance sampling (optional), ultimately calling related functions to implement loss aggregation.\nStandard PPO clipping mechanism supports asymmetric clipping, requiring computation of unclipped original policy gradient loss and asymmetrically clipped loss, taking the maximum of both as pessimistic update strategy. When advantage value is negative, apply dual clipping to more conservatively reduce the probability of that action. If truncated importance sampling is enabled and rollout policy log probabilities are provided, the function computes the importance ratio of old policy relative to rollout policy, clips it to the configured upper limit, then multiplies it into the policy gradient loss.\n\n**Expected Output:**\nReturns four scalar tensors:\n- `pg_loss`: Policy gradient loss\n- `pg_clipfrac`: Clipping fraction (proportion of clipped tokens, for monitoring training dynamics)\n- `ppo_kl`: Approximate KL divergence\n- `pg_clipfrac_lower`: Lower bound clipping fraction (proportion of tokens where dual clipping takes effect)\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["```python\nimport torch\nfrom typing import Optional, Tuple, Union\nfrom omegaconf import DictConfig\n\n# AlgoConfig is referenced in signature; if available in your codebase, import it.\n# Here we just allow it as a typing hint via Union.\nAlgoConfig = object\n\n\ndef compute_policy_loss_vanilla(\n    old_log_prob: torch.Tensor,\n    log_prob: torch.Tensor,\n    advantages: torch.Tensor,\n    response_mask: torch.Tensor,\n    loss_agg_mode: str = \"token-mean\",\n    config: Optional[Union[DictConfig, AlgoConfig]] = None,\n    rollout_log_probs: torch.Tensor | None = None,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Compute PPO-style policy gradient loss with support for asymmetric clipping\n    (clip_ratio_low, clip_ratio_high), dual clipping (clip_ratio_c), and optional\n    truncated importance sampling using rollout_log_probs.\n\n    Returns:\n      - pg_loss: scalar tensor (aggregated according to loss_agg_mode)\n      - pg_clipfrac: scalar tensor - fraction of (response) tokens clipped by the\n                     asymmetrical clipping bounds\n      - ppo_kl: scalar tensor - approximate KL = mean(old_log_prob - log_prob)\n      - pg_clipfrac_lower: scalar tensor - fraction of tokens where dual clipping\n                            (adv * clip_ratio_c bound) took effect\n    \"\"\"\n\n    # Basic checks\n    assert old_log_prob.shape == log_prob.shape == advantages.shape == response_mask.shape, \\\n        \"old_log_prob, log_prob, advantages, response_mask must have the same shape\"\n\n    # device/dtype\n    device = log_prob.device\n    dtype = log_prob.dtype\n\n    # Read clipping config with sensible fallbacks\n    # If config is None, fall back to default PPO clip 0.2 and dual clip 3.0\n    def _cfg_get(attr, default):\n        if config is None:\n            return default\n        return getattr(config, attr, default)\n\n    # Backward-compatible symmetric clip ratio\n    clip_ratio_fallback = float(_cfg_get(\"clip_ratio\", 0.2))\n\n    # Asymmetric clip ratios (DAPO extension)\n    clip_ratio_low = float(_cfg_get(\"clip_ratio_low\", clip_ratio_fallback))\n    clip_ratio_high = float(_cfg_get(\"clip_ratio_high\", clip_ratio_fallback))\n\n    # Dual clipping constant for negative advantages\n    clip_ratio_c = float(_cfg_get(\"clip_ratio_c\", 3.0))\n\n    # Truncated importance sampling flags/upper bound (optional)\n    # We try several likely config names for toggling truncation\n    truncation_enabled = bool(\n        _cfg_get(\"truncate_rho\", False)\n        or _cfg_get(\"truncate_is\", False)\n        or _cfg_get(\"truncated_importance_sampling\", False)\n        or _cfg_get(\"truncate_rho_hat\", False)\n    )\n    truncation_upper = float(_cfg_get(\"truncate_rho_upper\", clip_ratio_fallback))\n\n    # Prepare mask (ensure float tensor)\n    mask = response_mask.to(dtype=dtype, device=device)\n    mask_bool = mask.bool()\n\n    # Prevent divide by zero for mask sums\n    mask_sum = mask.sum()\n    if mask_sum <= 0:\n        # No tokens to compute loss on; return zeros with correct device/dtype\n        zero = torch.tensor(0.0, device=device, dtype=dtype)\n        return zero, zero, zero, zero\n\n    # Compute importance ratio between current and old policy\n    # ratio = pi_theta / pi_old = exp(log_prob - old_log_prob)\n    ratio = torch.exp(log_prob - old_log_prob)\n\n    # Compute clipped ratio with asymmetric bounds:\n    lower = 1.0 - clip_ratio_low\n    upper = 1.0 + clip_ratio_high\n    clipped_ratio = torch.clamp(ratio, min=lower, max=upper)\n\n    # Optionally compute truncated IS factor from rollout policy (if enabled)\n    # rho = pi_old / pi_rollout = exp(old_log_prob - rollout_log_probs)\n    # truncated_rho = min(rho, truncation_upper)\n    if (rollout_log_probs is not None) and truncation_enabled:\n        rho = torch.exp(old_log_prob - rollout_log_probs)\n        truncated_rho = torch.clamp(rho, max=truncation_upper)\n        # expand/truncate shape should already broadcast correctly; ensure device/dtype\n        truncated_rho = truncated_rho.to(device=device, dtype=dtype)\n    else:\n        truncated_rho = None\n\n    # Surrogates: s1 = ratio * adv, s2 = clipped_ratio * adv\n    s1 = ratio * advantages\n    s2 = clipped_ratio * advantages\n\n    # If truncated importance sampling enabled, incorporate truncated_rho into s1\n    if truncated_rho is not None:\n        s1 = s1 * truncated_rho\n\n    # For bookkeeping of clipping fractions:\n    # A token is considered \"clipped\" if ratio outside [lower, upper]\n    clip_mask = ((ratio > upper) | (ratio < lower)) & mask_bool\n    # clipfrac = num clipped tokens / num tokens\n    pg_clipfrac = clip_mask.sum().to(dtype=dtype) / mask_sum.to(dtype=dtype)\n\n    # Choose elementwise surrogate according to advantage sign:\n    # for adv >= 0 -> take min(s1, s2)\n    # for adv <  0 -> take max(s1, s2)\n    adv_pos = (advantages >= 0).to(dtype=dtype)\n    adv_neg = (advantages < 0).to(dtype=dtype)\n\n    # elementwise selections\n    s_min = torch.min(s1, s2)\n    s_max = torch.max(s1, s2)\n    surrogate = s_min * adv_pos + s_max * adv_neg  # elementwise pick\n\n    # Keep a copy before dual clipping decision to compute whether dual clip would act\n    surrogate_before_dual = surrogate.clone()\n\n    # Apply dual clipping for negative advantages:\n    # surrogate = max(surrogate, advantages * clip_ratio_c) for adv < 0\n    if clip_ratio_c is not None:\n        adv_bound = advantages * clip_ratio_c\n        # Only apply where advantages < 0\n        # For adv >=0, adv_bound does not apply\n        surrogate = torch.where(advantages < 0, torch.maximum(surrogate, adv_bound), surrogate)\n\n    # Compute fraction where dual clipping took effect:\n    # i.e., where advantage < 0 AND surrogate_before_dual < advantages * clip_ratio_c\n    if clip_ratio_c is not None:\n        lower_clip_mask = ((advantages < 0) & (surrogate_before_dual < (advantages * clip_ratio_c))) & mask_bool\n        pg_clipfrac_lower = lower_clip_mask.sum().to(dtype=dtype) / mask_sum.to(dtype=dtype)\n    else:\n        pg_clipfrac_lower = torch.tensor(0.0, device=device, dtype=dtype)\n\n    # Policy gradient loss per token is negative surrogate (we maximize surrogate)\n    per_token_loss = -surrogate\n\n    # Mask out non-response tokens\n    per_token_loss = per_token_loss * mask\n\n    # Aggregate loss according to loss_agg_mode\n    loss_agg_mode = loss_agg_mode.lower() if loss_agg_mode is not None else \"token-mean\"\n    if loss_agg_mode in (\"token-mean\", \"token_mean\", \"tokenmean\"):\n        pg_loss = per_token_loss.sum() / mask_sum\n    elif loss_agg_mode in (\"batch-mean\", \"batch_mean\", \"batchmean\"):\n        # compute mean per batch item, then mean across batch (respecting mask)\n        # assume first dim is batch\n        batch_size = per_token_loss.shape[0]\n        # per example sum / per example token counts (avoid divide by zero)\n        per_example_counts = mask.sum(dim=1).clamp(min=1.0)\n        per_example_loss = per_token_loss.sum(dim=1) / per_example_counts\n        pg_loss = per_example_loss.mean()\n    elif loss_agg_mode in (\"sum\",):\n        pg_loss = per_token_loss.sum()\n    else:\n        # default fallback to token-mean\n        pg_loss = per_token_loss.sum() / mask_sum\n\n    # Approximate KL: mean(old_log_prob - log_prob) over masked tokens\n    approx_kl = ((old_log_prob - log_prob) * mask).sum() / mask_sum\n\n    # Ensure returned tensors are scalar (0-dim)\n    pg_loss = pg_loss.reshape(()).to(device=device, dtype=dtype)\n    pg_clipfrac = pg_clipfrac.reshape(()).to(device=device, dtype=dtype)\n    approx_kl = approx_kl.reshape(()).to(device=device, dtype=dtype)\n    pg_clipfrac_lower = pg_clipfrac_lower.reshape(()).to(device=device, dtype=dtype)\n\n    return pg_loss, pg_clipfrac, approx_kl, pg_clipfrac_lower\n```"], "usage": {"prompt_tokens": 753, "completion_tokens": 5363, "total_tokens": 6116, "per_completion": [{"prompt_tokens": 753, "completion_tokens": 5363, "total_tokens": 6116}]}}
{"function_name": "DAPORewardManager.__call__", "overview": "DAPO's reward manager, responsible for computing task rewards and applying overlong penalty. This is one of the key components distinguishing DAPO from other methods, preventing the model from generating meaningless lengthy outputs through a linear penalty mechanism.", "function_signature": "def __call__(self, data: DataProto, return_dict: bool = False):", "input_parameters": "- `data`: DataProto object containing:\n- `batch[\"prompts\"]`: Prompt token IDs with shape `(bs, prompt_len)`\n- `batch[\"responses\"]`: Response token IDs with shape `(bs, response_len)`\n- `batch[\"attention_mask\"]`: Attention mask with shape `(bs, total_len)`\n- `non_tensor_batch[\"reward_model\"][\"ground_truth\"]`: Ground truth answers\n- `non_tensor_batch[\"data_source\"]`: Data source (e.g., \"gsm8k\", \"math\")\n- `return_dict`: Boolean, whether to return dictionary format (including additional information like accuracy, overlong penalty, etc.)", "detailed_description": "The function is responsible for computing reward scores for generated responses. First checks if there are pre-computed rewards; if so, returns directly; otherwise processes each response individually, decoding token sequences into text and computing task rewards based on ground truth answers (needs to call related functional functions). Computed scores are placed at the last valid token position in the response sequence. If length control is enabled, the function applies additional negative reward penalty for overlong responses (linear penalty mechanism). Throughout the process, detailed information for some samples is output for debugging verification, ultimately returning complete reward tensor or dictionary containing additional evaluation metrics.", "output": "- If `return_dict=False`: Returns sparse reward tensor with shape `(batch_size, response_length)` (only last valid token position is non-zero)\n- If `return_dict=True`: Returns dictionary containing:\n- `\"reward_tensor\"`: Reward tensor as above\n- `\"reward_extra_info\"`: Dictionary containing `\"acc\"` (accuracy list), `\"overlong_reward\"` (overlong penalty list), `\"overlong\"` (boolean list indicating if overlong), etc.", "func_implementation": "", "test_code_path": "code/test_code/test_dapo_reward_manager.py", "implementation_location": "code/verl/workers/reward_manager/dapo.py:line 53-149", "prompt": [{"role": "system", "content": "You are a Python programmer developing with the verl framework. verl (Volcano Engine Reinforcement Learning) is a flexible, efficient and production-ready RL training library for large language models (LLMs). It supports various RL algorithms like PPO and GRPO, integrates with mainstream frameworks including FSDP, Megatron-LM, vLLM, and SGLang, features state-of-the-art throughput performance and flexible device mapping capabilities, and scales to hundreds of GPUs and hundreds of billion parameter models."}, {"role": "user", "content": "Please implement a function named `DAPORewardManager.__call__`.\n\n**Function Overview:**\nDAPO's reward manager, responsible for computing task rewards and applying overlong penalty. This is one of the key components distinguishing DAPO from other methods, preventing the model from generating meaningless lengthy outputs through a linear penalty mechanism.\n\n**Function Signature:**\n```python\ndef __call__(self, data: DataProto, return_dict: bool = False):\n```\n\n**Input Parameters:**\n- `data`: DataProto object containing:\n- `batch[\"prompts\"]`: Prompt token IDs with shape `(bs, prompt_len)`\n- `batch[\"responses\"]`: Response token IDs with shape `(bs, response_len)`\n- `batch[\"attention_mask\"]`: Attention mask with shape `(bs, total_len)`\n- `non_tensor_batch[\"reward_model\"][\"ground_truth\"]`: Ground truth answers\n- `non_tensor_batch[\"data_source\"]`: Data source (e.g., \"gsm8k\", \"math\")\n- `return_dict`: Boolean, whether to return dictionary format (including additional information like accuracy, overlong penalty, etc.)\n\n**Detailed Description:**\nThe function is responsible for computing reward scores for generated responses. First checks if there are pre-computed rewards; if so, returns directly; otherwise processes each response individually, decoding token sequences into text and computing task rewards based on ground truth answers (needs to call related functional functions). Computed scores are placed at the last valid token position in the response sequence. If length control is enabled, the function applies additional negative reward penalty for overlong responses (linear penalty mechanism). Throughout the process, detailed information for some samples is output for debugging verification, ultimately returning complete reward tensor or dictionary containing additional evaluation metrics.\n\n**Expected Output:**\n- If `return_dict=False`: Returns sparse reward tensor with shape `(batch_size, response_length)` (only last valid token position is non-zero)\n- If `return_dict=True`: Returns dictionary containing:\n- `\"reward_tensor\"`: Reward tensor as above\n- `\"reward_extra_info\"`: Dictionary containing `\"acc\"` (accuracy list), `\"overlong_reward\"` (overlong penalty list), `\"overlong\"` (boolean list indicating if overlong), etc.\n\nPlease implement this function based on the above information, ensuring that:\n1. The function signature matches the one provided above\n2. The implementation follows the logic described in the detailed description\n3. All input parameters are properly handled\n4. The return value matches the expected output format\n5. The code style is clear with necessary comments\n\nPlease provide the Python function implementation code directly."}], "completions": ["def __call__(self, data: \"DataProto\", return_dict: bool = False):\n    \"\"\"\n    Compute DAPO task rewards and optional overlong penalty.\n\n    - Expects:\n        batch[\"prompts\"]: (bs, prompt_len) tensor of token ids\n        batch[\"responses\"]: (bs, response_len) tensor of token ids\n        batch[\"attention_mask\"]: (bs, total_len) tensor (prompt + response)\n        non_tensor_batch[\"reward_model\"][\"ground_truth\"]: list of ground truth strings (len bs or 1)\n        non_tensor_batch[\"data_source\"]: data source string or list (not strictly used here)\n\n    - Behavior:\n        * If pre-computed rewards exist in non_tensor_batch under key \"precomputed_reward_tensor\",\n          return that directly.\n        * Otherwise compute a task reward per example (placed at the final valid response token\n          position). Task reward is 1.0 for a correct answer, 0.0 otherwise. Matching logic:\n            - if both predicted and ground-truth contain a numeric value, compare numerically\n              (float) with small tolerance;\n            - otherwise compare normalized text (lowercased, punctuation stripped).\n        * If length control is enabled on the object (self.length_control == True), and a\n          self.max_response_len exists, apply linear negative penalty:\n              penalty = - self.overlong_penalty_coef * (resp_len - self.max_response_len)\n          only if resp_len > self.max_response_len. Penalty is added to the task reward at the\n          same final token position (so final reward may be negative).\n        * Returns either a reward tensor of shape (bs, response_len) (sparse, only last pos non-zero)\n          or a dict containing the tensor and extra info.\n\n    Note: this implementation is defensive: it attempts to use self.tokenizer.decode if available,\n    otherwise falls back to joining token ids as a string.\n    \"\"\"\n    import re\n    import string\n    import math\n    import torch\n\n    # Helper to safely get batch and non-tensor batch\n    try:\n        batch = data.batch\n        non_tensor = data.non_tensor_batch\n    except Exception:\n        batch = data[\"batch\"]\n        non_tensor = data[\"non_tensor_batch\"]\n\n    # quick check for precomputed rewards\n    precomputed = None\n    if isinstance(non_tensor, dict):\n        precomputed = non_tensor.get(\"precomputed_reward_tensor\", None)\n    if precomputed is not None:\n        # assume already in correct shape; return directly\n        return precomputed if not return_dict else {\"reward_tensor\": precomputed, \"reward_extra_info\": {}}\n\n    prompts = batch[\"prompts\"]\n    responses = batch[\"responses\"]\n    attention_mask = batch[\"attention_mask\"]\n\n    # support tensors or numpy arrays\n    if isinstance(prompts, torch.Tensor):\n        device = prompts.device\n    else:\n        device = torch.device(\"cpu\")\n\n    bs, resp_len = responses.shape\n    _, prompt_len = prompts.shape\n\n    # Build output reward tensor (float)\n    reward_tensor = torch.zeros((bs, resp_len), dtype=torch.float32, device=device)\n\n    # fetch ground truth list\n    gm = non_tensor.get(\"reward_model\", {}) if isinstance(non_tensor, dict) else {}\n    ground_truths = gm.get(\"ground_truth\", None)\n    if ground_truths is None:\n        # fallback: top-level key\n        ground_truths = non_tensor.get(\"ground_truth\", None) if isinstance(non_tensor, dict) else None\n\n    # Normalize ground_truths into a list of strings\n    if ground_truths is None:\n        ground_truths = [\"\"] * bs\n    elif isinstance(ground_truths, str):\n        ground_truths = [ground_truths] * bs\n    elif isinstance(ground_truths, (list, tuple)):\n        if len(ground_truths) == 1 and bs > 1:\n            ground_truths = list(ground_truths) * bs\n        else:\n            ground_truths = list(ground_truths)\n    else:\n        # try to coerce\n        ground_truths = [str(ground_truths)] * bs\n\n    # data source (unused here, but could affect scoring)\n    data_source = non_tensor.get(\"data_source\", None) if isinstance(non_tensor, dict) else None\n\n    # length control parameters (use defaults if not set on self)\n    length_control = getattr(self, \"length_control\", False)\n    max_response_len = getattr(self, \"max_response_len\", resp_len)\n    overlong_penalty_coef = getattr(self, \"overlong_penalty_coef\", 0.01)\n\n    # tokenizer if present\n    tokenizer = getattr(self, \"tokenizer\", None)\n\n    # helper functions\n    num_re = re.compile(r\"[-+]?\\d*\\.\\d+|[-+]?\\d+\")\n    punct_table = str.maketrans(\"\", \"\", string.punctuation)\n\n    def normalize_text(s: str) -> str:\n        s = s.strip().lower()\n        # remove punctuation\n        s = s.translate(punct_table)\n        # collapse whitespace\n        s = re.sub(r\"\\s+\", \" \", s)\n        return s\n\n    def extract_last_number(s: str):\n        matches = num_re.findall(s)\n        if not matches:\n            return None\n        try:\n            # prefer integer when possible\n            val = float(matches[-1])\n            return val\n        except Exception:\n            return None\n\n    def is_correct(pred: str, truth: str) -> bool:\n        # try numeric match first\n        pnum = extract_last_number(pred)\n        tnum = extract_last_number(truth)\n        if (pnum is not None) and (tnum is not None):\n            # numeric comparison tolerance\n            # treat close numbers as equal\n            if math.isclose(pnum, tnum, rel_tol=1e-3, abs_tol=1e-3):\n                return True\n            # also treat integer-equality if they are very close to integers\n            if abs(round(pnum) - pnum) < 1e-6 and abs(round(tnum) - tnum) < 1e-6 and round(pnum) == round(tnum):\n                return True\n            return False\n        # fallback to normalized textual comparison\n        return normalize_text(pred) == normalize_text(truth)\n\n    # extract final valid index in response for each sample using attention mask:\n    # attention_mask covers prompt + response positions (total_len)\n    # last_valid_global = sum(attention_mask) - 1\n    # last_valid_response_pos = last_valid_global - prompt_len\n    if isinstance(attention_mask, torch.Tensor):\n        total_attention = attention_mask.sum(dim=1).long()\n    else:\n        # assume numpy array\n        import numpy as np\n        total_attention = torch.tensor((attention_mask.sum(axis=1)).astype(int), dtype=torch.long, device=device)\n\n    last_valid_global = total_attention - 1  # shape (bs,)\n    last_response_pos = last_valid_global - prompt_len  # could be negative if no response tokens\n\n    # ensure within [0, resp_len-1]\n    last_response_pos = last_response_pos.clamp(min=0, max=resp_len - 1)\n\n    # For storing extra info\n    acc_list = []\n    overlong_flags = []\n    overlong_rewards = []\n    task_rewards = []\n\n    debug_to_show = min(3, bs)\n\n    for i in range(bs):\n        # compute how many response tokens are valid for decoding: up to last_response_pos[i] inclusive\n        last_pos = int(last_response_pos[i].item()) if isinstance(last_response_pos, torch.Tensor) else int(last_response_pos[i])\n        # If attention_mask says there is no response (total_attention <= prompt_len), we treat as zero-length response\n        if (isinstance(total_attention, torch.Tensor) and total_attention[i].item() <= prompt_len) or (not isinstance(total_attention, torch.Tensor) and total_attention[i] <= prompt_len):\n            resp_token_count = 0\n        else:\n            resp_token_count = last_pos + 1\n\n        # obtain token ids for this response (only valid tokens)\n        if isinstance(responses, torch.Tensor):\n            token_ids = responses[i][:resp_token_count].tolist() if resp_token_count > 0 else []\n        else:\n            # assume numpy array / list\n            token_ids = list(responses[i][:resp_token_count]) if resp_token_count > 0 else []\n\n        # decode predicted text\n        if tokenizer is not None:\n            try:\n                pred_text = tokenizer.decode(token_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n            except TypeError:\n                # some tokenizers expect plain list\n                pred_text = tokenizer.decode(token_ids, skip_special_tokens=True)\n            except Exception:\n                pred_text = \" \".join(map(str, token_ids))\n        else:\n            # fallback: join ids (not ideal but safe)\n            pred_text = \" \".join(map(str, token_ids))\n\n        gt_text = ground_truths[i] if i < len(ground_truths) else \"\"\n        # compute correctness\n        correct = is_correct(pred_text, gt_text)\n        task_reward = 1.0 if correct else 0.0\n        task_rewards.append(task_reward)\n        acc_list.append(1 if correct else 0)\n\n        # compute overlong penalty if enabled\n        overlong_flag = False\n        overlong_penalty = 0.0\n        if length_control:\n            # here resp_token_count is the generated response length\n            if resp_token_count > max_response_len:\n                overlong_flag = True\n                overlong_penalty = -float(overlong_penalty_coef) * float(resp_token_count - max_response_len)\n        overlong_flags.append(overlong_flag)\n        overlong_rewards.append(overlong_penalty)\n\n        # final reward is task reward + overlong_penalty\n        final_reward = float(task_reward) + float(overlong_penalty)\n\n        # place final_reward at last valid token position in reward_tensor\n        reward_tensor[i, last_pos] = torch.tensor(final_reward, dtype=torch.float32, device=device)\n\n        # debug printing for a few samples\n        if i < debug_to_show:\n            print(f\"[DAPORewardManager] Sample {i}:\")\n            print(f\"  pred_text: '{pred_text}'\")\n            print(f\"  ground_truth: '{gt_text}'\")\n            print(f\"  resp_token_count: {resp_token_count}, last_pos_in_resp: {last_pos}\")\n            print(f\"  task_reward: {task_reward}, overlong_flag: {overlong_flag}, overlong_penalty: {overlong_penalty}, final_reward: {final_reward}\")\n\n    reward_extra_info = {\n        \"acc\": acc_list,\n        \"task_reward\": task_rewards,\n        \"overlong\": overlong_flags,\n        \"overlong_reward\": overlong_rewards,\n    }\n\n    if return_dict:\n        return {\"reward_tensor\": reward_tensor, \"reward_extra_info\": reward_extra_info}\n    return reward_tensor"], "usage": {"prompt_tokens": 638, "completion_tokens": 4054, "total_tokens": 4692, "per_completion": [{"prompt_tokens": 638, "completion_tokens": 4054, "total_tokens": 4692}]}}
